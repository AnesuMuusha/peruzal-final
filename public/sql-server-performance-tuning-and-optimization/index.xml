<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>SQL Server Performance and Tuning</title>
    <link>http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/index.xml</link>
    <description>Recent content on SQL Server Performance and Tuning</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Copyright 2017 Peruzal</copyright>
    <lastBuildDate>Sat, 18 Mar 2017 10:23:42 +0200</lastBuildDate>
    <atom:link href="http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Performance Tuning Overview</title>
      <link>http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/performance-tuning-overview/</link>
      <pubDate>Sat, 18 Mar 2017 10:23:42 +0200</pubDate>
      
      <guid>http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/performance-tuning-overview/</guid>
      <description>

&lt;h2 id=&#34;factors-impacting-performance&#34;&gt;Factors Impacting Performance&lt;/h2&gt;

&lt;p&gt;There are several factors that impact the application performance, although in this guide we will be more concerned with the factors that are related to SQL Server specifically. Here are some factors that impact performance and scalability :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Application Architecture&lt;/li&gt;
&lt;li&gt;Application Design&lt;/li&gt;
&lt;li&gt;Transactions and Isolation Levels&lt;/li&gt;
&lt;li&gt;Transact-SQL Code&lt;/li&gt;
&lt;li&gt;Hardware Resources&lt;/li&gt;
&lt;li&gt;SQL Server Configuration&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;sql-server-configuration&#34;&gt;SQL Server Configuration&lt;/h3&gt;

&lt;p&gt;To view the sql server configuration options you can use the stored procedure, &lt;code&gt;sp_configure&lt;/code&gt;. You can show configurations options as follows :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;exec sp_configure
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The above will not show advanced configuration options, you can toggle the display of advanced configurations by running the following :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;EXEC sp_configure &#39;show advanced options&#39;, 1
GO
RECONFIGURE
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;the-performance-tuning-process&#34;&gt;The Performance-Tuning Process&lt;/h2&gt;

&lt;p&gt;Performance tuning needs to be a proactive process not reactive. A general good strategy might entail the following :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Creating a baseline for your workload&lt;/li&gt;
&lt;li&gt;Monitoring your workload&lt;/li&gt;
&lt;li&gt;Detecting, isolating, and troubleshooting performance problems&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You should ask yourself the following general questions before embarking on the performance tuning process :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Is any other resource-intensive application running on the same server?&lt;/li&gt;
&lt;li&gt;Is the capacity of the hardware subsystem capable of withstanding the maximum workload?&lt;/li&gt;
&lt;li&gt;Is SQL Server configured properly?&lt;/li&gt;
&lt;li&gt;Does the shared environment, whether VM or platform, have adequate resources, or am I dealing with a configuration issue there or even resource contention from outside forces?&lt;/li&gt;
&lt;li&gt;Is the database connection between SQL Server and the database application efficient?&lt;/li&gt;
&lt;li&gt;Does the database design support the fastest data retrieval (and modification for an updatable database)?&lt;/li&gt;
&lt;li&gt;Is the user workload, consisting of SQL queries, optimized to reduce the load on SQL Server?&lt;/li&gt;
&lt;li&gt;What processes are causing the system to slow down as reflected in the measurement of various wait states, performance counters, and dynamic management objects?&lt;/li&gt;
&lt;li&gt;Does the workload support the required level of concurrency?&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;creating-a-baseline&#34;&gt;Creating a Baseline&lt;/h3&gt;

&lt;p&gt;A baseline is what you will use to check deviations in performance within your sql server environment. Some environments might even require multiple baselines depending on the application requirements e.g a baseline for OLTP(Online Transaction Processing) workloads and another baseline for OLAP(Online Analytical Processing) workloads.&lt;/p&gt;

&lt;p&gt;Here are some commonly used tools for creating a SQL Server Performance Baseline :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Performance Monitor&lt;/li&gt;
&lt;li&gt;SQL Server Profiler(Deprecated since 2012, Use Extended Events instead)&lt;/li&gt;
&lt;li&gt;Extended Events&lt;/li&gt;
&lt;li&gt;Database Engine Tuning Advisor (DTA)&lt;/li&gt;
&lt;li&gt;DBCC Commands&lt;/li&gt;
&lt;li&gt;Dynamic Management Views (DMV) and Functions (DMF)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;monitoring-the-workload&#34;&gt;Monitoring the Workload&lt;/h3&gt;

&lt;p&gt;The baseline is not useful if its not monitored. A plan should be put in place to monitor the baseline. Any significant deviation from baseline represents a change that needs to be understood and analyzed for its impact on the performance of your workload&lt;/p&gt;

&lt;h3 id=&#34;detecting-isolating-and-troubleshooting-common-performance-problems&#34;&gt;Detecting, Isolating, and Troubleshooting Common Performance Problems&lt;/h3&gt;

&lt;p&gt;Performance problems usually manifest themselves as a bottleneck in a subsystem within SQL Server and these can be :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;CPU&lt;/li&gt;
&lt;li&gt;Memory&lt;/li&gt;
&lt;li&gt;I/O&lt;/li&gt;
&lt;li&gt;tempdb&lt;/li&gt;
&lt;li&gt;Blocking&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;cpu-bottlenecks&#34;&gt;CPU Bottlenecks&lt;/h2&gt;

&lt;p&gt;When a user submits a query that query is either a batch or a request. Each request or batch is executed by a worker process, which is a logical thread in SQL Server, and is mapped directly to an OS thread or a fiber. A worker process can be either in three states :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;RUNNING&lt;/strong&gt; - The worker is currently executing on the CPU&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RUNNABLE&lt;/strong&gt; - The worker is currently waiting for its turn on the CPU&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SUSPENDED&lt;/strong&gt; - The worker is waiting on a resource, for example, a lock or an I/O&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If the SQL Server instance have a large number of workers in the RUNNABLE state this could be a sign of a CPU bottleneck and if more workers are spending their time in the SUSPENDED state they could be a blocking issue, which might be related to I/O or another resource.&lt;/p&gt;

&lt;h3 id=&#34;detecting-cpu-bottlenecks&#34;&gt;Detecting CPU Bottlenecks&lt;/h3&gt;

&lt;p&gt;The following tools can help detect CPU bottleneck :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Performance Monitor&lt;/li&gt;
&lt;li&gt;DMVs&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;performance-monitor-counters&#34;&gt;Performance Monitor Counters&lt;/h3&gt;

&lt;p&gt;We can use the the following Performance counters to detect CPU pressure :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Processor:% Processor Time&lt;/strong&gt; - A consistent value greater than 80 percent for 15 to 20 minutes indicates that you have CPU bottleneck.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;System:Processor Queue&lt;/strong&gt; -  A sustained value of 2 or higher typically indicates CPU pressure&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Process:%Processor Time&lt;/strong&gt; - If SQL Server is the only application on the machine then you can monitor this value and check the amount of CPU time used by SQL Server.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;using-dmvs&#34;&gt;Using DMVs&lt;/h3&gt;

&lt;p&gt;We can check the number of worker processes for each scheduler that are in the RUNNABLE state using the &lt;code&gt;sys.dm_os_schedulers&lt;/code&gt; and `` as follows :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;select s.scheduler_id, count(*) [workers waiting for cpu] from sys.dm_os_workers w
join sys.dm_os_schedulers s
on w.scheduler_address = s.scheduler_address
where s.scheduler_id &amp;lt; 255 and w.state = N&#39;RUNNABLE&#39;
group by s.scheduler_id 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;or we can find out the amount of time spent by workers in the RUNNABLE state as follows :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT SUM(signal_wait_time_ms)
FROM sys.dm_os_wait_stats
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;signal_wait_time&lt;/code&gt; represents the difference in time when the worker process actually started running from the RUNNABLE state.&lt;/p&gt;

&lt;div class=&#34;admonition note&#34;&gt;
&lt;p class=&#34;admonition-title&#34;&gt;Note&lt;/p&gt;
&lt;p&gt;The above DMVs contains values since SQL Server was started. You will need to use a baseline to find significant deviations in order to detect CPU pressure.&lt;/p&gt;
&lt;/div&gt;

&lt;h3 id=&#34;isolating-and-troubleshooting-of-cpu-bottlenecks&#34;&gt;Isolating and troubleshooting of CPU bottlenecks&lt;/h3&gt;

&lt;p&gt;Once you are convinced the pressure is cause by CPU bottleneck, you can diagnose the causes. The following are some causes of CPU pressure :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;An inefficient query plan&lt;/li&gt;
&lt;li&gt;Excessive compilation and recompilation&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;an-inefficient-query-plan&#34;&gt;An inefficient query plan&lt;/h3&gt;

&lt;p&gt;We need to find the queries taking the most CPU time first and comapre them to the baseline. We can use the &lt;code&gt;sys.dm_exec_query_stats&lt;/code&gt;. This DMV returns aggregate performance statistics on cached query plans in SQL Server. We can find the top queries with high CPU per execution with the following :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;USE AdventureWorks2012;
GO  
SELECT TOP 10 query_stats.query_hash AS &amp;quot;Query Hash&amp;quot;,   
    SUM(query_stats.total_worker_time) / SUM(query_stats.execution_count) AS &amp;quot;Avg CPU Time&amp;quot;,  
    MIN(query_stats.statement_text) AS &amp;quot;Statement Text&amp;quot;  
FROM   
    (SELECT QS.*,   
    SUBSTRING(ST.text, (QS.statement_start_offset/2) + 1,  
    ((CASE statement_end_offset   
        WHEN -1 THEN DATALENGTH(ST.text)  
        ELSE QS.statement_end_offset END   
            - QS.statement_start_offset)/2) + 1) AS statement_text  
     FROM sys.dm_exec_query_stats AS QS  
     CROSS APPLY sys.dm_exec_sql_text(QS.sql_handle) as ST) as query_stats  
GROUP BY query_stats.query_hash  
ORDER BY 2 DESC;  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;or a variation of the above :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT TOP 10
   total_worker_time/execution_count AS avg_cpu_cost, plan_handle,
   execution_count,
   (SELECT SUBSTRING(text, statement_start_offset/2 + 1,
      (CASE WHEN statement_end_offset = -1
         THEN LEN(CONVERT(nvarchar(max), text)) * 2
         ELSE statement_end_offset
      END - statement_start_offset)/2)
   FROM sys.dm_exec_sql_text(sql_handle)) AS query_text
FROM sys.dm_exec_query_stats
ORDER BY [avg_cpu_cost] DESC
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We might miss frequently executed queries by using the above, a slight modification is to find frequently executed queries as follows :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT TOP 10 total_worker_time, plan_handle,execution_count,
   (SELECT SUBSTRING(text, statement_start_offset/2 + 1,
      (CASE WHEN statement_end_offset = -1
         THEN LEN(CONVERT(nvarchar(max),text)) * 2
         ELSE statement_end_offset
      END - statement_start_offset)/2)
   FROM sys.dm_exec_sql_text(sql_handle)) AS query_text
FROM sys.dm_exec_query_stats
ORDER BY execution_count DESC
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The following example returns row count aggregate information (total rows, minimum rows, maximum rows and last rows) for queries :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT qs.execution_count,  
    SUBSTRING(qt.text,qs.statement_start_offset/2 +1,   
                 (CASE WHEN qs.statement_end_offset = -1   
                       THEN LEN(CONVERT(nvarchar(max), qt.text)) * 2   
                       ELSE qs.statement_end_offset end -  
                            qs.statement_start_offset  
                 )/2  
             ) AS query_text,   
     qt.dbid, dbname= DB_NAME (qt.dbid), qt.objectid,   
     qs.total_rows, qs.last_rows, qs.min_rows, qs.max_rows  
FROM sys.dm_exec_query_stats AS qs   
CROSS APPLY sys.dm_exec_sql_text(qs.sql_handle) AS qt   
WHERE qt.text like &#39;%SELECT%&#39;   
ORDER BY qs.execution_count DESC;  
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&#34;admonition note&#34;&gt;
&lt;p class=&#34;admonition-title&#34;&gt;Note&lt;/p&gt;
&lt;p&gt;The above DMVs only contains aggregate performance information for queries that have the plans cached. Some expensive queries plans might not be cached and will be missed. But if you are polling for this information frequently you will be able to catch them.&lt;/p&gt;
&lt;/div&gt;

&lt;h3 id=&#34;excessive-compilation-and-recompilation&#34;&gt;Excessive compilation and recompilation&lt;/h3&gt;

&lt;p&gt;SQL Server compiles the query before executing it and usually caches the plan. For an expensive query the cost of re-compiling can be reducing by re-using cached query plans. Compiling is a CPU intensive task and excessive re-compilation will result in CPU pressure. There many reasons SQL Server might recompile a query, some of them are :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Schema Change&lt;/strong&gt; - f the metadata of the referenced objects is changed, it causes a recompile. So if you have a batch that mixes DDL and DML, it will force a recompile&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SET Options&lt;/strong&gt; - There are some set options which will cause a recompile, if changed. Some of these options are &lt;em&gt;ANSI_NULLS&lt;/em&gt;, &lt;em&gt;ANSI_PADDINGS&lt;/em&gt;, &lt;em&gt;ANSI_NULL&lt;/em&gt;, and &lt;em&gt;ARITHABORT&lt;/em&gt;. If you change these options inside a batch, it will force a recompile every time&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Updated Statistics&lt;/strong&gt; - Any significant changes in the statistical information will force a recompile&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Recompile Query Hint&lt;/strong&gt; - If you have a stored procedure with recompile option, it will get recompiled on every execution&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;detecting-compilation-and-re-compilation&#34;&gt;Detecting Compilation and Re-compilation&lt;/h3&gt;

&lt;p&gt;We can use Performance Monitor and DMVs to monitors compilations and recompilations.&lt;/p&gt;

&lt;h3 id=&#34;performance-counters&#34;&gt;Performance Counters&lt;/h3&gt;

&lt;p&gt;We can look at the following Performance Counters :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;SQLServer: SQL Statistics: Batch Requests/Sec&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;SQLServer: SQL Statistics: SQL Compilations/Sec&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;SQLServer: SQL Statistics: SQL Recompilations/Sec&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&#34;admonition note&#34;&gt;
&lt;p class=&#34;admonition-title&#34;&gt;Note&lt;/p&gt;
&lt;p&gt;It is important to look at compile/recompile numbers in relation to number batch requests per second&lt;/p&gt;
&lt;/div&gt;

&lt;h3 id=&#34;using-dmvs-1&#34;&gt;Using DMVs&lt;/h3&gt;

&lt;p&gt;We can check how much time the SQL Server optimizer is spending optimizing the query plan using the `` DMV as follows :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT * FROM sys.dm_exec_query_optimizer_info
WHERE counter in (&#39;optimizations&#39;, &#39;elapsed time&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can use the following query to find the top 10 queries with the most recompiles as follows :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT TOP 10 plan_generation_num, execution_count,
   (SELECT SUBSTRING(text, statement_start_offset/2 + 1,
      (CASE WHEN statement_end_offset = -1
         THEN LEN(CONVERT(nvarchar(max),text)) * 2
         ELSE statement_end_offset
      END - statement_start_offset)/2)
   FROM sys.dm_exec_sql_text(sql_handle)) AS query_text
FROM sys.dm_exec_query_stats
WHERE plan_generation_num &amp;gt;1
ORDER BY plan_generation_num DESC
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Recompilations might mean SQL Server is under memory pressure and can not keep the cached plans in memory. We can look at the memory using &lt;em&gt;DBCC&lt;/em&gt; as follows :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;DBCC MEMORYSTATUS
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Look under the Procedure Cache tab results. &lt;em&gt;TotalPages&lt;/em&gt; represent stolen buffer pool pages used to store optimized plans. Here is the sample output :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;Procedure Cache                          Value
---------------------------------------- -----------
TotalProcs                               0
TotalPages                               211
InUsePages                               0

(3 row(s) affected)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;memory-bottlenecks&#34;&gt;Memory Bottlenecks&lt;/h2&gt;

&lt;p&gt;All data read from tables needs to be in memory first before SQL Server can work with it. The first time SQL Server reads a page from the file, this is called a physical read and when the page is read from memory its called a logical read. SQL Server keeps the pages in memory structure called the buffer pool. Its uses the LRU(Least Recently Used) algorithm to keep the pages in memory.&lt;/p&gt;

&lt;p&gt;SQL Server also uses memory for its internal structures, e.g. for user connections, locks, query cache and other internal structures.&lt;/p&gt;

&lt;p&gt;SQL Server uses both physical RAM and virtual memory. A page file is used for the virtual memory. When a process is not using a part of memory, the data is &lt;em&gt;paged&lt;/em&gt; to disk and then &lt;em&gt;paged in&lt;/em&gt; again when the process needs to use them. This process is quite slow. Paging can also be the result of not having enough RAM on the machine.&lt;/p&gt;

&lt;h3 id=&#34;show-available-memory&#34;&gt;Show Available Memory&lt;/h3&gt;

&lt;p&gt;We can use &lt;em&gt;DBCC&lt;/em&gt; to show the available memory and paging memory as follows :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;DBCC MEMORYSTATUS
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here is a sample output :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;Process/System Counts                    Value
---------------------------------------- --------------------
Available Physical Memory                3500609536
Available Virtual Memory                 140714606923776
Available Paging File                    2413473792
Working Set                              168247296
Percent of Committed Memory in WS        13
Page Faults                              1821744
System physical memory high              1
System physical memory low               0
Process physical memory low              0
Process virtual memory low               0

(10 row(s) affected)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;detecting-memory-pressure&#34;&gt;Detecting Memory Pressure&lt;/h3&gt;

&lt;p&gt;Using Performance Monitor we can look at the following counters :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Memory: Available Bytes&lt;/strong&gt; - This represents the amount of physical memory, in bytes, available to processes running on the computer.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SQLServer:Buffer Manager: Buffer Cache Hit Ratio&lt;/strong&gt; - This represents the percentage of pages that were found in the buffer pool without having to incur a read from disk. For most production workloads this value should be in the high 90s&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SQLServer:Buffer Manager: Page Life Expectancy&lt;/strong&gt; - This represents the number of seconds a page will stay in the buffer pool without references. A lower value indicates that the buffer pool is under memory pressure.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SQLServer:Buffer Manager: Checkpoint Pages/Sec&lt;/strong&gt; - This represents number of pages flushed by checkpoint or other operations that require all dirty pages to be flushed. This indicates increased buffer pool activity of your workload&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SQLServer:Buffer Manager: Lazywrites/Sec&lt;/strong&gt; - This represents number of buffers written by buffer manager’s lazy writer. This indicates increased buffer pool activity of your workload.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The majority of the committed memory is used by the buffer pool on a SQL Server instance can be seen from the output of &lt;code&gt;DBCC MEMORYSTATUS&lt;/code&gt;. Here is the sample output :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;Buffer Pool                              Value
---------------------------------------- -----------
Database                                 6005
Simulated                                1104
Target                                   16203776
Dirty                                    162
In IO                                    0
Latched                                  0
Page Life Expectancy                     105067

(7 row(s) affected)
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Committed&lt;/strong&gt; - This value shows the total buffers that are committed. Buffers that are committed have physical memory associated with them. The Committed value is the current size of the buffer pool.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Target&lt;/strong&gt; - This value shows the target size of the buffer pool. It is computed periodically by SQL Server as the number of 8-KB pages it can commit without causing paging. SQL Server lowers its value in response to memory low notification from the Windows operating system. A decrease in the number of target pages on a normally loaded server may indicate response to an external physical memory pressure.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;isolation-and-troubleshooting-of-memory-pressure&#34;&gt;Isolation and Troubleshooting of Memory Pressure&lt;/h3&gt;

&lt;p&gt;Use the Performance counters to detect external memory pressure. The following are some counters to look at :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Process:Working Set&lt;/strong&gt; counter for each process&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Memory:Cache Bytes&lt;/strong&gt; counter for system working set&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Memory:Pool Nonpaged&lt;/strong&gt; Bytes counter for size of unpaged pool&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Memory:Available Bytes&lt;/strong&gt; (equivalent of the Available value in Task Manager)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We can use the following DMV to find the amount of memory currently being used by the buffer pool :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT
   SUM(virtual_memory_committed_kb + shared_memory_committed_kb
      + awe_allocated_kb) AS [Used by BPool, Kb]
FROM sys.dm_os_memory_clerks
WHERE type = &#39;MEMORYCLERK_SQLBUFFERPOOL&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;i-o-bottlenecks&#34;&gt;I/O Bottlenecks&lt;/h2&gt;

&lt;p&gt;The IO subsystem is an important part of SQL Server. SQL Server needs to read data pages from the file before it can work with. Sometimes IO bottlenecks are the result os a different subsystem, e.g memory pressure.&lt;/p&gt;

&lt;h3 id=&#34;detection-of-i-o-bottlenecks&#34;&gt;Detection of I/O Bottlenecks&lt;/h3&gt;

&lt;p&gt;We can use Performance Monitor and look at counters for I/O. Here are some counters to look at :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;PhysicalDisk Object: Avg. Disk Queue Length&lt;/strong&gt; - Disk Queue Length represents the average number of physical Read and Write requests that were queued on the selected physical disk during the sampling period. If your I/O system is overloaded, more Read/Write operations will be waiting. If your disk-queue length frequently exceeds a value of two per physical disk during peak usage of SQL Server, then you might have an I/O bottleneck&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;PhysicalDisk Object: Avg. Disk Sec/Read or Avg&lt;/strong&gt; - is the average time, in seconds, of a read or write of data from/to the disk. Some general guidelines follow:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Less than 10 ms is very good&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Between 10 and 20 ms is okay&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Between 20 and 50 ms is slow, needs attention&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Greater than 50 ms is considered a serious I/O bottleneck&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;PhysicalDisk: Disk Reads/Sec or Disk Writes/Sec&lt;/strong&gt; - is the rate of read or write operations on the disk. You need to make sure that this number is less than 85 percent of the disk capacity. The disk access time increases exponentially beyond 85 percent capacity.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&#34;admonition note&#34;&gt;
&lt;p class=&#34;admonition-title&#34;&gt;Note&lt;/p&gt;
&lt;p&gt;The Performance counter shows the I/O information at the disk level not file level. If you are mixing the database files and the logs files on the same disk you miss valuable information.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;We can use the DMV to find the I/O information at the file level as follows :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT database_id, file_id, io_stall_read_ms, io_stall_write_ms 
FROM sys.dm_io_virtual_file_stats(null, null)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can also check for the latch waits. SQL Server waits for &lt;em&gt;PAGEIOLATCH_EX&lt;/em&gt; or &lt;em&gt;PAGEIOLATCH_SH&lt;/em&gt; if the page is not in the buffer pool.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT
   wait_type,
   waiting_tasks_count,
   wait_time_ms,
   signal_wait_time_ms,
   (wait_time_ms - signal_wait_time_ms) &#39;io time waiting&#39;
FROM sys.dm_os_wait_stats
WHERE wait_type IN (&#39;PAGEIOLATCH_SH&#39;, &#39;PAGEIOLATCH_EX&#39;)
ORDER BY wait_type
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The long waits of this type indicate a problem with the disk subsystem. The column &lt;em&gt;wait_time_ms&lt;/em&gt; includes the time a worker spends in SUSPENDED state and in RUNNABLE state while the column &lt;em&gt;signal_wait_time_ms&lt;/em&gt; represents the time a worker spends in RUNNABLE state. So the difference of the two (wait_time_ms – signal_wait_time_ms), actually represents the time spent waiting for I/O to complete.&lt;/p&gt;

&lt;h3 id=&#34;isolation-and-troubleshooting-of-i-o-bottlenecks&#34;&gt;Isolation and Troubleshooting of I/O Bottlenecks&lt;/h3&gt;

&lt;p&gt;To troubleshoot I/O bottlenecks you will need to look into
- physical memory available to SQL Server and,
- the queries with the highest I/O.&lt;/p&gt;

&lt;p&gt;For troubleshooting memory, you can use the techniques already discussed.&lt;/p&gt;

&lt;h3 id=&#34;using-dmvs-to-troublehsoot-i-o&#34;&gt;Using DMVs to Troublehsoot I/O&lt;/h3&gt;

&lt;p&gt;We can use the following DMV to find the top 10 queries with the highest I/O per execution as follows :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT TOP 10
   (total_logical_reads/execution_count) AS avg_logical_reads,
   (total_logical_writes/execution_count) AS avg_logical_writes,
   (total_physical_reads/execution_count) AS avg_phys_reads,
   execution_count,
   (SELECT SUBSTRING(text, statement_start_offset/2 + 1,
      (CASE WHEN statement_end_offset = -1
         THEN LEN(CONVERT(nvarchar(MAX),text)) * 2
         ELSE statement_end_offset
         END - statement_start_offset)/2)
      FROM sys.dm_exec_sql_text(sql_handle)) AS query_text,
   plan_handle
FROM sys.dm_exec_query_stats
ORDER BY (total_logical_reads + total_logical_writes) DESC
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;tempdb-bottlenecks&#34;&gt;Tempdb Bottlenecks&lt;/h3&gt;

&lt;p&gt;The SQL Server &lt;code&gt;tempdb&lt;/code&gt; is a shared resource within the SQL Server instance, and all databases uses the same tempdb. The tempdb can be a bottleneck and with the new SQL Server it can be configured with its own set of memory, data and log files and CPU configurations.&lt;/p&gt;

&lt;h2 id=&#34;blocking&#34;&gt;Blocking&lt;/h2&gt;

&lt;p&gt;When a user submits a request/batch, the SQL Server assigns a worker and schedules it on the CPU to execute it. If the number of incoming requests far exceeds the capacity of CPU(s) to process it, the end user may perceive that the request is blocked or running slow. Poorly configured hardware configurations can cause blocking in SQL Server.&lt;/p&gt;

&lt;h3 id=&#34;detection-of-blocking&#34;&gt;Detection of blocking&lt;/h3&gt;

&lt;p&gt;We can use the &lt;code&gt;sys.dm_os_wait_stats&lt;/code&gt; DMV to detect blocking. The following query would list the top 10 waits encountered :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT TOP 10
   wait_type,
   waiting_tasks_count AS tasks,
   wait_time_ms,
   max_wait_time_ms AS max_wait,
   signal_wait_time_ms AS signal
FROM sys.dm_os_wait_stats
ORDER BY wait_time_ms DESC
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can use the DBCC command to clear the wait stats as follows :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;DBCC SQLPERF(&amp;quot;sys.dm_os_wait_stats&amp;quot;,CLEAR);  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Using Performance Monitor,we can also monitor the following counters :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;SQLServer:Locks: Average Wait Time (ms)&lt;/strong&gt; - represents the average wait time (milliseconds) for each lock request that resulted in a wait.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SQLServer:Locks: Lock Requests/Sec&lt;/strong&gt; - represents the number of new locks and lock conversions requested from the lock manager&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SQLServer:Locks: Lock Wait Time (ms)&lt;/strong&gt; - represents total wait time (milliseconds) for locks in the last second.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SQLServer:Locks: Lock Waits/Sec&lt;/strong&gt; - represents number of lock requests that could not be satisfied immediately and required the caller to wait before being granted the lock&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SQLServer:Locks: Number of Deadlocks/Sec&lt;/strong&gt; - represents the number of lock requests that resulted in a deadlock&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SQLServer:General Statistics: Processes Blocked&lt;/strong&gt; -  represents the number of currently blocked processes&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SQLServer:Access Methods: Table Lock Escalations/Sec&lt;/strong&gt; - represents the number of times locks were escalated to table-level granularity&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;isolating-and-troubleshooting-blocking-problems&#34;&gt;Isolating and Troubleshooting Blocking Problems&lt;/h3&gt;

&lt;p&gt;Blocking is normal is an application but excessive blocking in undesirable. Here are some guidelines to reduce blocking :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Shorten the duration of the transaction and run it at a lower isolation level&lt;/li&gt;
&lt;li&gt;Minimize the data that needs to be accessed by the transaction&lt;/li&gt;
&lt;li&gt;When doing DML operations on objects, try designing your application in such a way that you access objects in the same order&lt;/li&gt;
&lt;li&gt;If you are doing DML operations on a large number of rows, break it into smaller transactions to prevent lock escalation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We can use the &lt;code&gt;sys.dm_tran_locks&lt;/code&gt; to find out all the locks that are being held and their status as follows :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT resource_type, resource_associated_entity_id,  
    request_status, request_mode,request_session_id,  
    resource_description   
    FROM sys.dm_tran_locks  
    WHERE resource_database_id = db_id()  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and we can find blocking information as follows :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT   
        t1.resource_type,  
        t1.resource_database_id,  
        t1.resource_associated_entity_id,  
        t1.request_mode,  
        t1.request_session_id,  
        t2.blocking_session_id  
    FROM sys.dm_tran_locks as t1  
    INNER JOIN sys.dm_os_waiting_tasks as t2  
        ON t1.lock_owner_address = t2.resource_address;  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The following query returns object information by using &lt;em&gt;resource_associated_entity_id&lt;/em&gt; from the previous query. This query must be executed while you are connected to the database that contains the object.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT object_name(object_id), *  
    FROM sys.partitions  
    WHERE hobt_id=&amp;lt;resource_associated_entity_id&amp;gt;  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can even get more information by combining other DMVs as follows :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT
   t1.resource_type,
   &#39;database&#39; = DB_NAME(resource_database_id),
   &#39;blk object&#39; = t1.resource_associated_entity_id,
   t1.request_mode,
   t1.request_session_id,
   t2.blocking_session_id,
   t2.wait_duration_ms,
   (SELECT SUBSTRING(text, t3.statement_start_offset/2 + 1,
   (CASE WHEN t3.statement_end_offset = -1
      THEN LEN(CONVERT(nvarchar(max),text)) * 2
      ELSE t3.statement_end_offset
   END - t3.statement_start_offset)/2)
FROM sys.dm_exec_sql_text(sql_handle)) AS query_text,
   t2.resource_description
FROM
   sys.dm_tran_locks AS t1,
   sys.dm_os_waiting_tasks AS t2,
   sys.dm_exec_requests AS t3
WHERE
   t1.lock_owner_address = t2.resource_address AND
   t1.request_request_id = t3.request_id AND
   t2.session_id = t3.session_id
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can find out more information on blocking on indexes using the &lt;em&gt;sys.dm_db_index_operational_stats&lt;/em&gt;. The DMV function provides comprehensive index usage statistics including blocking experienced while accessing that index.&lt;/p&gt;

&lt;p&gt;For example to find the lock counts on the indexes on the Person.Person table we can use the following query :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT s.index_id,
   i.name,
   range_scan_count,
   row_lock_count,
   page_lock_count
FROM sys.dm_db_index_operational_stats(DB_ID(),
   OBJECT_ID(&#39;Person.Person&#39;), NULL, NULL) s
JOIN sys.indexes i
ON s.index_id = i.index_id and i.object_id =  OBJECT_ID(&#39;Person.Person&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;performance-vs-price&#34;&gt;Performance vs. Price&lt;/h2&gt;

&lt;p&gt;When optimizing performance you should remember the Pareto Principle. In most cases, you will see 80% performance improvement by only fixing 20% of the problems with SQL Server. Here are general considerations :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Use a baseline to measure performance against&lt;/li&gt;
&lt;li&gt;Count the cost of improving performance&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;performance-baseline&#34;&gt;Performance Baseline&lt;/h2&gt;

&lt;p&gt;Performance tuning should be a proactive process. A baseline should be created at the beginning of the process and used to measure against any deviations in performance.&lt;/p&gt;

&lt;h2 id=&#34;where-to-focus-efforts&#34;&gt;Where to Focus Efforts&lt;/h2&gt;

&lt;p&gt;You should focus your effort on the SQL queries and stored procedures executed on the database more as compared to focusing your efforts on the hardware running SQL Server. You will gain a considerable amount of performance in the data access layer than you will from the hardware.&lt;/p&gt;

&lt;p&gt;T-SQL Code and indexing were the top SQL Server root cause of poor performance from a survey conducted by Paul Radal.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://www.sqlskills.com/blogs/paul/wp-content/uploads/2011/4/mostrecentperf.jpg&#34; alt=&#34;SQL Server Performance Survey Result&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;sql-server-performance-killers&#34;&gt;SQL Server Performance Killers&lt;/h2&gt;

&lt;p&gt;Here some issues that might degrade SQL Server Performance :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Insufficient indexing&lt;/li&gt;
&lt;li&gt;Inaccurate statistics&lt;/li&gt;
&lt;li&gt;Improper query design&lt;/li&gt;
&lt;li&gt;Poorly generated execution plans&lt;/li&gt;
&lt;li&gt;Excessive blocking and deadlocks&lt;/li&gt;
&lt;li&gt;Non-set-based operations, usually T-SQL cursors&lt;/li&gt;
&lt;li&gt;Inappropriate database design&lt;/li&gt;
&lt;li&gt;Excessive fragmentation&lt;/li&gt;
&lt;li&gt;Nonreusable execution plans&lt;/li&gt;
&lt;li&gt;Frequent recompilation of queries&lt;/li&gt;
&lt;li&gt;Improper use of cursors&lt;/li&gt;
&lt;li&gt;Improper configuration of the database transaction log&lt;/li&gt;
&lt;li&gt;Excessive use or improper configuration of tempdb&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.sqlskills.com/blogs/paul/survey-results-common-causes-of-performance-problems/&#34;&gt;http://www.sqlskills.com/blogs/paul/survey-results-common-causes-of-performance-problems/&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://docs.microsoft.com/en-us/sql/relational-databases/system-dynamic-management-views/sys-dm-exec-query-stats-transact-sql&#34;&gt;https://docs.microsoft.com/en-us/sql/relational-databases/system-dynamic-management-views/sys-dm-exec-query-stats-transact-sql&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://support.microsoft.com/en-za/help/907877/how-to-use-the-dbcc-memorystatus-command-to-monitor-memory-usage-on-sql-server-2005&#34;&gt;https://support.microsoft.com/en-za/help/907877/how-to-use-the-dbcc-memorystatus-command-to-monitor-memory-usage-on-sql-server-2005&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://social.technet.microsoft.com/wiki/contents/articles/22316.sql-server-memory-and-troubleshooting.aspx#How_to_test_that_your_SQL_server_is_facing_memory_crunch&#34;&gt;https://social.technet.microsoft.com/wiki/contents/articles/22316.sql-server-memory-and-troubleshooting.aspx#How_to_test_that_your_SQL_server_is_facing_memory_crunch&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://technet.microsoft.com/en-us/library/aa337525(v=sql.105).aspx&#34;&gt;https://technet.microsoft.com/en-us/library/aa337525(v=sql.105).aspx&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://docs.microsoft.com/en-us/sql/relational-databases/system-dynamic-management-views/sys-dm-tran-locks-transact-sql&#34;&gt;https://docs.microsoft.com/en-us/sql/relational-databases/system-dynamic-management-views/sys-dm-tran-locks-transact-sql&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>SQL Performance Analysis</title>
      <link>http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/sql-performance-analysis/</link>
      <pubDate>Sat, 18 Mar 2017 14:51:09 +0200</pubDate>
      
      <guid>http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/sql-performance-analysis/</guid>
      <description>

&lt;h2 id=&#34;performance-monitor-tool&#34;&gt;Performance Monitor Tool&lt;/h2&gt;

&lt;p&gt;Performance Monitor tracks resource behavior by capturing performance data generated by hardware and software components of the system, such as a processor, a process, a thread, and so on.&lt;/p&gt;

&lt;p&gt;To start Perfmon do the following :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Click &lt;strong&gt;Start&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Type &lt;strong&gt;run&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Type &lt;strong&gt;perfmon&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/perfmon.png&#34; alt=&#34;PerfMon&#34; /&gt;&lt;/p&gt;

&lt;p&gt;There can be multiple instances of a component and totals for all the components, e.g for the Processor component&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Performance object: Processor&lt;/li&gt;
&lt;li&gt;Counter: % Processor Time&lt;/li&gt;
&lt;li&gt;Instance: _Total&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can use the graphical component to gather counters but the recommended way is to save the data collection set to a file and analyze off the production server.&lt;/p&gt;

&lt;h2 id=&#34;dynamic-management-views&#34;&gt;Dynamic Management Views&lt;/h2&gt;

&lt;p&gt;Dynamic management views and functions return server state information that can be used to monitor the health of a server instance, diagnose problems, and tune performance.&lt;/p&gt;

&lt;p&gt;There are two types of dynamic management views and functions:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Server-scoped dynamic management views and functions&lt;/strong&gt; -These require VIEW SERVER STATE permission on the server.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Database-scoped dynamic management views and functions&lt;/strong&gt; - These require VIEW DATABASE STATE permission on the database.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All dynamic management views and functions exist in the sys schema and follow this naming convention &lt;code&gt;sys.dm_&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Here are some execution dynamic management functions and views we will use in this guide :&lt;/p&gt;

&lt;h3 id=&#34;execution-related-dmvs&#34;&gt;Execution Related DMVs&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;sys.dm_exec_query_plan&lt;/li&gt;
&lt;li&gt;sys.dm_exec_query_stats&lt;/li&gt;
&lt;li&gt;sys.dm_exec_sql_text&lt;/li&gt;
&lt;li&gt;sys.dm_exec_cached_plans&lt;/li&gt;
&lt;li&gt;sys.dm_exec_procedure_stats&lt;/li&gt;
&lt;li&gt;sys.dm_exec_query_optimizer_info&lt;/li&gt;
&lt;li&gt;sys.dm_exec_requests&lt;/li&gt;
&lt;li&gt;sys.dm_exec_text_query_plan&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;i-o-related-dmvs&#34;&gt;I/O Related DMVs&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;sys.dm_io_virtual_file_stats&lt;/li&gt;
&lt;li&gt;sys.dm_io_pending_io_requests&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;transaction-related-dmvs&#34;&gt;Transaction Related DMVs&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;sys.dm_tran_database_transaction&lt;/li&gt;
&lt;li&gt;sys.dm_tran_session_transactions&lt;/li&gt;
&lt;li&gt;sys.dm_tran_active_transactions&lt;/li&gt;
&lt;li&gt;sys.dm_tran_current_transaction&lt;/li&gt;
&lt;li&gt;sys.dm_tran_locks&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;sql-server-operating-system-related-dynamic-management-views&#34;&gt;SQL Server Operating System Related Dynamic Management Views&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;sys.dm_os_buffer_descriptors&lt;/li&gt;
&lt;li&gt;sys.dm_os_host_info&lt;/li&gt;
&lt;li&gt;sys.dm_os_memory_brokers&lt;/li&gt;
&lt;li&gt;sys.dm_os_memory_cache_counters&lt;/li&gt;
&lt;li&gt;sys.dm_os_memory_cache_entries&lt;/li&gt;
&lt;li&gt;sys.dm_os_memory_clerks&lt;/li&gt;
&lt;li&gt;sys.dm_os_memory_nodes&lt;/li&gt;
&lt;li&gt;sys.dm_os_performance_counters&lt;/li&gt;
&lt;li&gt;sys.dm_os_process_memory&lt;/li&gt;
&lt;li&gt;sys.dm_os_schedulers&lt;/li&gt;
&lt;li&gt;sys.dm_os_sys_info&lt;/li&gt;
&lt;li&gt;sys.dm_os_sys_memory&lt;/li&gt;
&lt;li&gt;sys.dm_os_tasks&lt;/li&gt;
&lt;li&gt;sys.dm_os_threads&lt;/li&gt;
&lt;li&gt;sys.dm_os_volume_stats&lt;/li&gt;
&lt;li&gt;sys.dm_os_wait_stats&lt;/li&gt;
&lt;li&gt;sys.dm_os_waiting_tasks&lt;/li&gt;
&lt;li&gt;sys.dm_os_windows_info&lt;/li&gt;
&lt;li&gt;sys.dm_os_workers&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;object-related-dynamic-management-views-and-functions&#34;&gt;Object Related Dynamic Management Views and Functions&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;sys.dm_db_stats_properties&lt;/li&gt;
&lt;li&gt;sys.dm_db_stats_histogram&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;sys-dm-exec-requests-and-sys-dm-exec-sessions&#34;&gt;sys.dm_exec_requests and sys.dm_exec_sessions&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;sys.dm_exec_requests&lt;/code&gt; DMV is used to show resources currently used by an executing request&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sys.dm_exec_sessions&lt;/code&gt; shows the accumulated resources used on a session.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To see the difference between the two DMVs let run the following :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt; -- copy and be ready to run the following code on that window:
DBCC FREEPROCCACHE
DBCC DROPCLEANBUFFERS
GO
SELECT * FROM Production.Product p1 CROSS JOIN
Production.Product p2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and in a new query window run the following and replace the SPID :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- copy the following code to a second window
SELECT cpu_time, reads, total_elapsed_time, logical_reads, row_count
FROM sys.dm_exec_requests
WHERE session_id = 56
GO
SELECT cpu_time, reads, total_elapsed_time, logical_reads, row_count
FROM sys.dm_exec_sessions
WHERE session_id = 56
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;sys-dm-exec-query-stats&#34;&gt;sys.dm_exec_query_stats&lt;/h3&gt;

&lt;p&gt;Provides aggregated performance information for cached query plans. Using this information, you can avoid running server trace since the same information is available using this DMV.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s see how the DMV works by running the following queries :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- create the following stored procedure with three simple queries:
CREATE PROC test
AS
SELECT * FROM Sales.SalesOrderDetail WHERE SalesOrderID = 60677
SELECT * FROM Person.Address WHERE AddressID = 21
SELECT * FROM HumanResources.Employee WHERE BusinessEntityID = 229
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Clean the buffer pool and the procedure cache and execute the stored procedure and notice the results from the DMV using the following :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- note that the code uses the sys.dm_exec_sql_text DMF
DBCC FREEPROCCACHE
DBCC DROPCLEANBUFFERS
GO
EXEC test
GO
SELECT execution_count, total_worker_time, total_physical_reads, total_logical_reads, text, sql_handle, plan_handle 
FROM sys.dm_exec_query_stats
CROSS APPLY sys.dm_exec_sql_text(sql_handle)
WHERE objectid = OBJECT_ID(&#39;dbo.test&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you run several times you will notice the execution count value going up if the cached query plan is reused. The batch have the same &lt;strong&gt;sql_handle&lt;/strong&gt; and &lt;strong&gt;query_handle&lt;/strong&gt; and we can use those to retrieve the sql text within the batch and the query plan.&lt;/p&gt;

&lt;div class=&#34;admonition note&#34;&gt;
&lt;p class=&#34;admonition-title&#34;&gt;Cached Plans&lt;/p&gt;
&lt;p&gt;The DMV only displays the resource usage information for cached query plans. Due to memory pressure some query plans will not be cached, so you should be careful of missing expensive queries that might be running on the server but do not have their query plans cached. Instead you can use the &lt;code&gt;sys.dm_exec_query_requests&lt;/code&gt; to currently executing queries.&lt;/p&gt;
&lt;/div&gt;

&lt;h3 id=&#34;retrieving-the-actual-query-with-the-batch&#34;&gt;Retrieving the Actual Query with the Batch&lt;/h3&gt;

&lt;p&gt;Using the &lt;code&gt;statement_start_offset&lt;/code&gt; and &lt;code&gt;statement_end_offset&lt;/code&gt; we can retrieve the actual query within the batch. Both columns are the number of bytes where the query text begins and ends. Zero is the start of the batch and -1 the end of the batch.&lt;/p&gt;

&lt;p&gt;The text data is stored as Unicode so we need to divide by 2 to get the length of the actual strings. We can use the SUBSTRING and DATALENGTH functions to get the desired result as follows :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- we can easily extend our previous query to inspect the plan cache to use statement_start_offset
-- and statement_end_offset and get something like the following code:
DBCC FREEPROCCACHE
DBCC DROPCLEANBUFFERS
GO
EXEC test
GO
SELECT SUBSTRING(text, (statement_start_offset/2) + 1,
((CASE statement_end_offset
WHEN -1
THEN DATALENGTH(text)
ELSE
statement_end_offset
END
- statement_start_offset)/2) + 1) AS statement_text, *
FROM sys.dm_exec_query_stats
CROSS APPLY sys.dm_exec_sql_text(sql_handle)
WHERE objectid = OBJECT_ID(&#39;dbo.test&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and we can manually test this be replacing the actual values as follows :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- to test the concept for a particular query
SELECT SUBSTRING(text, 44 / 2 + 1, (168 - 44) / 2 + 1) FROM sys.dm_exec_sql_text(
0x03000500996DB224E0B27201B7A1000001000000000000000000000000000000000000000000000000000000)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;retrieving-the-sql-text-and-plan-with-sql-handle-and-plan-handle&#34;&gt;Retrieving the sql text and plan with sql_handle and plan_handle&lt;/h3&gt;

&lt;p&gt;We can use the &lt;code&gt;sql_handle&lt;/code&gt; and &lt;code&gt;plan_handle&lt;/code&gt; to retrieve the sql text and plans respectively as follows :&lt;/p&gt;

&lt;p&gt;&amp;ndash; using the example before
SELECT * from sys.dm_exec_sql_text(
0x03000500996DB224E0B27201B7A1000001000000000000000000000000000000000000000000000000000000)&lt;/p&gt;

&lt;p&gt;The sql_handle hash is guaranteed to be unique for every batch in the system. The text of the batch is stored in the SQL Manager Cache or SQLMGR, which you can inspect by running the following query:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT * FROM sys.dm_os_memory_objects WHERE type = ‘MEMOBJ_SQLMGR’
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Because a sql_handle has a 1:N relationship with a plan_handle (that is, there can be more than one generated executed plan for a particular query), the text of the batch will remain on the SQLMGR cache store until the last of the generated plans is evicted from the plan cache.&lt;/p&gt;

&lt;p&gt;The plan_handle is guaranteed to be unique for each batch in the system and we can retrieve the query plan as follows :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- here is an example:
SELECT * FROM sys.dm_exec_query_plan(
0x05000500996DB224B0C9B8F80100000001000000000000000000000000000000000000000000000000000000)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Cached execution plans are stored in the SQLCP and OBJCP cache stores: object plans, including stored procedures, triggers, and functions, are stored in the OBJCP cache stores, whereas plans for ad hoc, autoparameterized, and prepared queries are stored in the SQLCP cache store.&lt;/p&gt;

&lt;h2 id=&#34;finding-similar-query-plans-and-text-with-query-hash-and-plan-hash&#34;&gt;Finding similar query plans and text with query_hash and plan_hash&lt;/h2&gt;

&lt;p&gt;When a query is auto-parameterised, the query plan and hash will be the same even if different plans are generated for the query. But this is not always the case, let&amp;rsquo;s look at an example :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;DBCC FREEPROCCACHE
DBCC DROPCLEANBUFFERS
GO
SELECT * FROM Person.Address
WHERE AddressID = 12
GO
SELECT * FROM Person.Address
WHERE AddressID = 37
GO
SELECT plan_handle, plan_generation_num, execution_count FROM sys.dm_exec_query_stats
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and the result shows the following :&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/query-handle.png&#34; alt=&#34;Query Handle&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You will notice both queries have the same plan handle and the plan was re-used to execute the second query. The query hashes are different but the plan handle are the same. The optimizer parameterised the query. This is good since the same query plan doesnt need to be re-generated again.&lt;/p&gt;

&lt;p&gt;Now lets run and different query and use a different column as follows :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;- however, we can see a different behavior with the following query:
DBCC FREEPROCCACHE
DBCC DROPCLEANBUFFERS
GO
SELECT * FROM Person.Address
WHERE StateProvinceID = 79
GO
SELECT * FROM Person.Address
WHERE StateProvinceID = 59
GO
SELECT * FROM sys.dm_exec_query_stats
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now notice this time that the plan handle is different since the optimizer generated separated query plans. The reason is because the StateProvinceID could return 0 , 1 or more rows and its not safe to parameterize them.&lt;/p&gt;

&lt;p&gt;To aggregate the query and plan handle qe can use the &lt;code&gt;query_hash&lt;/code&gt; and &lt;code&gt;query_plan_hash&lt;/code&gt; values as they will be the same. the query_hash is computed from the tree of logical operators, so the text does not have to be exactly the same. Different queries will produce the same query_hash.&lt;/p&gt;

&lt;h3 id=&#34;finding-expensive-queries&#34;&gt;Finding Expensive Queries&lt;/h3&gt;

&lt;p&gt;Lets use the &lt;code&gt;sys.dm_exec_query_stats&lt;/code&gt; to find expensive queries with cached plans. The query uses the &lt;code&gt;query_hash&lt;/code&gt; to group all related queries regardless of whether they are parameterized or not as follows :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- note that the query is grouping on the query_hash value to aggregate similar queries
SELECT TOP 20 query_stats.query_hash,
SUM(query_stats.total_worker_time) / SUM(query_stats.execution_count)
AS avg_cpu_time,
MIN(query_stats.statement_text) AS statement_text
FROM
(SELECT qs.*,
SUBSTRING(st.text, (qs.statement_start_offset/2) + 1,
((CASE statement_end_offset
WHEN -1 THEN DATALENGTH(ST.text)
ELSE qs.statement_end_offset END
- qs.statement_start_offset)/2) + 1) AS statement_text
FROM sys.dm_exec_query_stats qs
CROSS APPLY sys.dm_exec_sql_text(qs.sql_handle) AS st) AS query_stats
GROUP BY query_stats.query_hash
ORDER BY avg_cpu_time DESC
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can trim the query down and only focus on the query plans and the batch as follows :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- notice that there is no need to use the statement_start_offset and statement_end_offset columns 
-- to separate the particular queries and that this time we are grouping on the query_plan_hash value.
SELECT TOP 20 query_plan_hash,
SUM(total_worker_time) / SUM(execution_count) AS avg_cpu_time,
MIN(plan_handle) AS plan_handle, MIN(text) AS query_text
FROM sys.dm_exec_query_stats qs
CROSS APPLY sys.dm_exec_sql_text(qs.plan_handle) AS st
GROUP BY query_plan_hash
ORDER BY avg_cpu_time DESC
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and we can find the most expensive queries currently executing as follows :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- finally, we could also apply the same concept to find the most expensive queries currently executing
SELECT TOP 20 SUBSTRING(st.text, (er.statement_start_offset/2) + 1,
((CASE statement_end_offset
WHEN -1
THEN DATALENGTH(st.text)
ELSE
er.statement_end_offset
END
- er.statement_start_offset)/2) + 1) AS statement_text
, *
FROM sys.dm_exec_requests er
CROSS APPLY sys.dm_exec_sql_text(er.sql_handle) st
ORDER BY total_elapsed_time DESC
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;hardware-resource-bottlenecks&#34;&gt;Hardware Resource Bottlenecks&lt;/h2&gt;

&lt;p&gt;Typically, SQL Server database performance is affected by stress on the following hardware resources:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Memory&lt;/li&gt;
&lt;li&gt;Disk I/O&lt;/li&gt;
&lt;li&gt;Processor&lt;/li&gt;
&lt;li&gt;Network&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Stress beyond the capacity of a hardware resource forms a bottleneck. To address the overall performance of a system, you need to identify these bottlenecks because they form the limit on overall system performance.&lt;/p&gt;

&lt;h2 id=&#34;memory-bottleneck-analysis&#34;&gt;Memory Bottleneck Analysis&lt;/h2&gt;

&lt;p&gt;Memory can be a problematic bottleneck because a bottleneck in memory will manifest on other resources, too. When the SQL Server process is low on memory, the &lt;em&gt;lazy writer&lt;/em&gt; process works extensively by writing pages from memory to disk causing high contention on I/O and also increasing CPU cycles.&lt;/p&gt;

&lt;h3 id=&#34;sql-server-memory-management&#34;&gt;SQL Server Memory Management&lt;/h3&gt;

&lt;p&gt;The biggest consumer of SQL Server memory is the &lt;em&gt;buffer pool&lt;/em&gt;. SQL Server first reads the data from the physical file into the buffer pool. When there&amp;rsquo;s limited physical memory the pages from the buffer pool are flushed to dish.&lt;/p&gt;

&lt;p&gt;The default SQL Server configuration is to dynamically manage memory. SQL Server will use the amount it needs and occasionally release memory to the OS.&lt;/p&gt;

&lt;p&gt;We can configure the memory using SQL Server Management Studio :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Right-Click the &lt;strong&gt;Server&lt;/strong&gt; node in SQL Server Management Studio&lt;/li&gt;
&lt;li&gt;Click &lt;strong&gt;Properties&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Choose the &lt;strong&gt;Memory&lt;/strong&gt; Tab&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/sql-server-memory-configuration.png&#34; alt=&#34;SQL Server Memory Configuration&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We can also use the &lt;code&gt;sys.configurations&lt;/code&gt; to show the configured values for the minimum and maximum amount of memory as follows :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT name, value, minimum, maximum, value_in_use, description, is_dynamic 
FROM sys.configurations
WHERE name IN (&#39;max server memory (MB)&#39;,&#39;min server memory (MB)&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;or we can use the &lt;code&gt;sp_configure&lt;/code&gt; stored procedure as follows :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;--Enable showing advanced options
EXEC sp_configure &#39;show advanced options&#39;, 1;
GO
--Activate the new configuration
RECONFIGURE;
GO
EXEC sp_configure  &#39;min server memory&#39;;
EXEC sp_configure  &#39;max server memory&#39;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can also change the values using &lt;code&gt;sp_configure&lt;/code&gt; but this is not recommended, e.g. to set the maximum memory to 10Gig and min server memory to 5Gig as follows :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;USE master;
EXEC sp_configure  &#39;show advanced option&#39;,   1;
RECONFIGURE;
exec sp_configure  &#39;min server memory (MB)&#39;,  5120;
exec sp_configure  &#39;max server memory (MB)&#39;,  10240;
RECONFIGURE WITH OVERRIDE;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;performance-monitor-counters-for-memory-pressure&#34;&gt;Performance Monitor Counters for Memory Pressure&lt;/h3&gt;

&lt;p&gt;Here are Performance Monitors counters to monitor to detect memory pressure :&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/memory-perf-counters.jpg&#34; alt=&#34;Performance Monitor Memory Counters&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;buffer-cache-hit-ratio&#34;&gt;Buffer Cache Hit Ratio&lt;/h3&gt;

&lt;p&gt;This number of pages served from the buffer pool without having to read the data from the physical file. This value should be very high, in the 99 range for OLTP systems.&lt;/p&gt;

&lt;h3 id=&#34;page-life-expectancy&#34;&gt;Page Life Expectancy&lt;/h3&gt;

&lt;p&gt;This counter determines how long a page will stay in the buffer pool. On NUMA systems, the counter value is an average. To see specific measures, you’ll need to use the &lt;em&gt;Buffer Node:Page Life Expectancy&lt;/em&gt; counter.&lt;/p&gt;

&lt;h3 id=&#34;checkpoint-pages-sec&#34;&gt;Checkpoint Pages/Sec&lt;/h3&gt;

&lt;p&gt;The number of pages written to disk during checkpoint. This value should be very low around 30 and below.&lt;/p&gt;

&lt;h3 id=&#34;lazy-writes-sec&#34;&gt;Lazy Writes/Sec&lt;/h3&gt;

&lt;p&gt;The lazy writer process writes buffer pages to the disk, this values represent the number of pages written per second. The value should be consistently less than 20.&lt;/p&gt;

&lt;h3 id=&#34;monitoring-memory-dmvs&#34;&gt;Monitoring Memory DMVs&lt;/h3&gt;

&lt;p&gt;The following DMVs can be used to monitor memory pressure with SQL Server :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;sys.dm_os_memory_brokers&lt;/strong&gt; - Allocations that are internal to SQL Server use the SQL Server memory manager. Tracking the difference between process memory counters from sys.dm_os_process_memory and internal counters can indicate memory use from external components in the SQL Server memory space.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;sys.dm_os_memory_clerks&lt;/strong&gt; - Returns the set of all memory clerks that are currently active in the instance of SQL Server.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&#34;admonition note&#34;&gt;
&lt;p class=&#34;admonition-title&#34;&gt;Note&lt;/p&gt;
&lt;p&gt;The SQL Server memory manager consists of a three-layer hierarchy. At the bottom of the hierarchy are memory nodes. The middle level consists of memory clerks, memory caches, and memory pools. The top layer consists of memory objects. These objects are generally used to allocate memory in an instance of SQL Server.&lt;/p&gt;

&lt;p&gt;Memory nodes provide the interface and the implementation for low-level allocators. Inside SQL Server, only memory clerks have access to memory nodes. Memory clerks access memory node interfaces to allocate memory. Memory nodes also track the memory allocated by using the clerk for diagnostics. Every component that allocates a significant amount of memory must create its own memory clerk and allocate all its memory by using the clerk interfaces. Frequently, components create their corresponding clerks at the time SQL Server is started.&lt;/p&gt;
&lt;/div&gt;

&lt;h2 id=&#34;memory-bottleneck-resolutions&#34;&gt;Memory Bottleneck Resolutions&lt;/h2&gt;

&lt;p&gt;A few of the common resolutions for memory bottlenecks are as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Optimizing application workload&lt;/li&gt;
&lt;li&gt;Allocating more memory to SQL Server&lt;/li&gt;
&lt;li&gt;Moving in-memory tables back to standard storage&lt;/li&gt;
&lt;li&gt;Increasing system memory&lt;/li&gt;
&lt;li&gt;Changing from a 32-bit to a 64-bit processor&lt;/li&gt;
&lt;li&gt;Enabling 3GB of process space&lt;/li&gt;
&lt;li&gt;Compressing data&lt;/li&gt;
&lt;li&gt;Addressing fragmentation&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;disk-bottleneck-analysis&#34;&gt;Disk Bottleneck Analysis&lt;/h2&gt;

&lt;p&gt;The disk subsystem is the slowest component within when troubleshooting SQL Server performance.&lt;/p&gt;

&lt;p&gt;The optimal number of files depends on workload and the underlying hardware. As a rule of thumb, create four data files if the server has up to 16 logical CPUs, keeping a 1/8th ratio between files and CPUs afterward.&lt;/p&gt;

&lt;h3 id=&#34;file-auto-growth&#34;&gt;File Auto-growth&lt;/h3&gt;

&lt;p&gt;Set the same initial size and auto-growth parameters, with grow size being defined in megabytes rather than by percentage for all files in a same filegroup. This helps the proportional fill algorithm balance write activities evenly across data files.&lt;/p&gt;

&lt;p&gt;SQL Server 2016 introduces two options— &lt;strong&gt;AUTOGROW_SINGLE_FILE&lt;/strong&gt; and &lt;strong&gt;AUTOGROW_ALL_FILES—&lt;/strong&gt;which control auto-growth events on a per-filegroup level. With AUTOGROW_SINGLE_FILE, which is the default option, SQL Server 2016 grows the single file in the filegroup when needed. With AUTOGROW_ALL_FILES, SQL Server grows all files in the filegroup whenever one of the files is out of space.&lt;/p&gt;

&lt;h3 id=&#34;enable-instant-file-initialization&#34;&gt;Enable Instant File Initialization&lt;/h3&gt;

&lt;p&gt;SQL Server doesn’t have a setting or checkbox to enable IFI.&lt;/p&gt;

&lt;p&gt;Instead, it detects whether or not the service account it’s running under has the Perform Volume Maintenance Tasks permission in the Windows Security Policy. You can find and edit this policy by running secpol.msc (Local Security Policy) in Windows. Then:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Expand the Local Policies Folder&lt;/li&gt;
&lt;li&gt;Click on User Rights Assignment&lt;/li&gt;
&lt;li&gt;Go down to the “Perform Volume Maintenance Tasks” option and double click it&lt;/li&gt;
&lt;li&gt;Add your SQL Server Service account, and click OK out of the dialog.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/secpol.png&#34; alt=&#34;Enable Instant File Initialization&#34; /&gt;&lt;/p&gt;

&lt;p&gt;IFI setting can also be enabled during the installation of SQL Server by checking the following :&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/instant-file-initialization-during-install.jpg&#34; alt=&#34;Enable Instant File Initialization During Install&#34; /&gt;&lt;/p&gt;

&lt;div class=&#34;admonition note&#34;&gt;
&lt;p class=&#34;admonition-title&#34;&gt;Restart SQL After Giving Permission&lt;/p&gt;
&lt;p&gt;SQL Server checks to see if Instant File Initialization is enabled on startup. You need to restart the SQL Server service after you give the corresponding permission to the SQL Server startup account.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;In order to check if Instant File Initialization is enabled, you can use the code below :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;dbcc traceon(3004,3605,-1)
go
create database Dummy
go
exec sp_readerrorlog
go
drop database Dummy
go
dbcc traceoff(3004,3605,-1)
go
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This code sets two trace flags that force SQL Server to put additional information into the error log, create a small database, and read the content of the error log file.
If &lt;strong&gt;Instant File Initialization&lt;/strong&gt; is not enabled, SQL Server will zero out both the .mdf file and .mdf files.&lt;/p&gt;

&lt;h3 id=&#34;data-pages-and-data-rows&#34;&gt;Data Pages and Data Rows&lt;/h3&gt;

&lt;p&gt;The space in the database is divided into logical 8KB pages. These pages are continuously numbered starting with zero, and they can be referenced by specifying a file ID and page number.&lt;/p&gt;

&lt;p&gt;SQL Server does not let you create the table when this is not the case. For example, the code in Listing 1-8 produces an error.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;create table dbo.BadTable
(
    Col1 char(4000),
    Col2 char(4060)
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can reduce the size of the data row by creating tables in a manner in which variable-length columns, which usually store null values, are defined as the last ones in the &lt;strong&gt;CREATE TABLE&lt;/strong&gt; statement. This is the only case in which the order of columns in the &lt;strong&gt;CREATE TABLE&lt;/strong&gt; statement matters.&lt;/p&gt;

&lt;h3 id=&#34;large-objects-storage&#34;&gt;Large Objects Storage&lt;/h3&gt;

&lt;p&gt;Even though the fixed-length data and the internal attributes of a row must fit into a single page, SQL Server can store the variable-length data on different data pages. There are two different ways to store the data, depending on the data type and length.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;ROW-OVERFLOW STORAGE&lt;/li&gt;
&lt;li&gt;LOB STORAGE&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;row-overflow-storage&#34;&gt;ROW-OVERFLOW STORAGE&lt;/h3&gt;

&lt;p&gt;SQL Server stores variable-length column data that does not exceed 8,000 bytes on special pages called row-overflow pages.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;create table dbo.RowOverflow
(
    ID int not null,
    Col1 varchar(8000) null,
    Col2 varchar(8000) null
);
insert into dbo.RowOverflow(ID, Col1, Col2) values (1,replicate(&#39;a&#39;,8000),replicate(&#39;b&#39;,8000));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you see, SQL Server creates the table and inserts the data row without any errors, even though the data-row size exceeds 8,060 bytes. The data is store in the row-overflow pages.&lt;/p&gt;

&lt;h3 id=&#34;lob-storage&#34;&gt;LOB STORAGE&lt;/h3&gt;

&lt;p&gt;For the &lt;em&gt;text&lt;/em&gt;, &lt;strong&gt;ntext&lt;/strong&gt;, or &lt;em&gt;image&lt;/em&gt; columns, SQL Server stores the data off-row by default. It uses another kind of pages called a LOB data pages.&lt;/p&gt;

&lt;p&gt;You can control this behavior to a degree by using the &lt;code&gt;text in row&lt;/code&gt; table option. For example,&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;exec sp_table_option dbo.MyTable, &#39;text in row&#39;, 200
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/lob.jpg&#34; alt=&#34;LOB&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As with row-overflow data, there is a pointer to another piece of information called the LOB root structure, which contains a set of the pointers to other data pages and rows. When LOB data is less than 32 KB and can fit into five data pages, the LOB root structure contains the pointers to the actual chunks of LOB data. Otherwise, the LOB tree starts to include additional intermediate levels of pointers, similar to the index B-Tree&lt;/p&gt;

&lt;p&gt;Lets store some data :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;create table dbo.TextData
(
    ID int not null,
    Col1 text null
);
insert into dbo.TextData(ID, Col1) values (1, replicate(convert(varchar(max),&#39;a&#39;),16000));
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&#34;admonition warning&#34;&gt;
&lt;p class=&#34;admonition-title&#34;&gt;Deprecated Types&lt;/p&gt;
&lt;p&gt;&lt;code&gt;text&lt;/code&gt;, &lt;code&gt;ntext&lt;/code&gt;, and &lt;code&gt;image&lt;/code&gt; data types are deprecated, and they will be removed in future versions of SQL Server. Use &lt;code&gt;varchar(max)&lt;/code&gt;, &lt;code&gt;nvarchar(max)&lt;/code&gt;, and &lt;code&gt;varbinary(max)&lt;/code&gt; columns instead.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;When a page does not have enough free space to accommodate a row, SQL Server allocates a new page and places the row.&lt;/p&gt;

&lt;h3 id=&#34;select-and-i-o&#34;&gt;SELECT * and I/O&lt;/h3&gt;

&lt;p&gt;There are plenty of reasons why selecting all columns from a table with the SELECT * operator is not a good idea. It is recommended that you avoid such a pattern and instead explicitly specify the list of columns needed by the client application. This is especially important with row-overflow and LOB storage, when one row can have data stored in multiple data pages. SQL Server needs to read all of those pages, which can significantly decrease the performance of queries.&lt;/p&gt;

&lt;p&gt;As an example, let’s assume that we have table dbo.Employees, with one column storing employee pictures. The following code creates the table and populates it with some data.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;create table dbo.Employees
(
    EmployeeId int not null,
    Name varchar(128) not null,
    Picture varbinary(max) null
);

with N1(C) as (select 0 union all select 0) -- 2 rows
    ,N2(C) as (select 0 from N1 as T1 cross join N1 as T2) -- 4 rows
    ,N3(C) as (select 0 from N2 as T1 cross join N2 as T2) -- 16 rows
    ,N4(C) as (select 0 from N3 as T1 cross join N3 as T2) -- 256 rows
    ,N5(C) as (select 0 from N4 as T1 cross join N2 as T2) -- 1,024 rows
    ,IDs(ID) 
    as (
        select row_number() over (order by (select null)) from N5
    )
insert into dbo.Employees(EmployeeId, Name, Picture)
select ID, 
	&#39;Employee &#39; + convert(varchar(5),ID), 
	convert(varbinary(max),
	replicate(convert(varchar(max),&#39;a&#39;),120000)) 
from Ids;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Compare the output from the two queries :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SET STATISTICS IO ON;
SET STATISTICS TIME ON;
select * from dbo.Employees;
SET STATISTICS IO OFF;
SET STATISTICS TIME OFF;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and the output from the query :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;(1024 row(s) affected)
Table &#39;Employees&#39;. Scan count 1, logical reads 7, physical reads 0, read-ahead reads 0, lob logical reads 90888, lob physical reads 0, lob read-ahead reads 0.
 SQL Server Execution Times:
   CPU time = 125 ms,  elapsed time = 3098 ms.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Take note of the 90888 number of &lt;code&gt;lob&lt;/code&gt; reads.&lt;/p&gt;

&lt;p&gt;and the query without choosing all columns :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SET STATISTICS IO ON;
SET STATISTICS TIME ON;
select EmployeeId, Name from dbo.Employees;
SET STATISTICS IO OFF;
SET STATISTICS TIME OFF;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and the following output :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;(1024 row(s) affected)
Table &#39;Employees&#39;. Scan count 1, logical reads 7, physical reads 0, read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob read-ahead reads 0.
 SQL Server Execution Times:
   CPU time = 0 ms,  elapsed time = 144 ms.
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;extents-and-allocation-map-pages&#34;&gt;Extents and Allocation Map Pages&lt;/h3&gt;

&lt;p&gt;QL Server logically groups eight pages into 64 KB units called extents. There are two types of extents available:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;mixed extents&lt;/strong&gt; store data that belongs to different objects&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;uniform extents&lt;/strong&gt; store the data for the same object.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;By default, when a new object is created, SQL Server stores the first eight object pages in mixed extents. After that, all subsequent space allocation for that object is done with uniform extents.&lt;/p&gt;

&lt;p&gt;SQL Server uses a special kind of pages, called allocation maps, to track extent and page usage in a file.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;GAM&lt;/strong&gt; - Global Allocation Map&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SGAM&lt;/strong&gt; - Shared Global Allocation Map&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IAM&lt;/strong&gt; - Index Allocation Map&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;data-modifications&#34;&gt;Data Modifications&lt;/h3&gt;

&lt;p&gt;SQL Server does not read or modify data rows directly on the disk. Every time you access data, SQL Server reads it into memory.&lt;/p&gt;

&lt;p&gt;Initial state&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/data-modification-before.jpg&#34; alt=&#34;Initial Stage&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Data modification - modifying data&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/data-modification-second-stage.jpg&#34; alt=&#34;Data Mofification&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Data modifiation - checkpoint&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/data-modification-modification.jpg&#34; alt=&#34;Data modification - checkpoint&#34; /&gt;&lt;/p&gt;

&lt;p&gt;There is another SQL Server process called &lt;strong&gt;lazy writer&lt;/strong&gt; that can save dirty pages on disk. As the opposite to checkpoint, which saves dirty data pages by keeping them in the buffer pool, lazy writer processes the least recently used data pages (SQL Server tracks buffer pool page usage internally), releasing them from memory. It releases both dirty and clean pages, saving dirty data pages on disk during the process. As you can guess, lazy writer runs in case of memory pressure or when SQL Server needs to bring more data pages to the buffer pool.&lt;/p&gt;

&lt;h3 id=&#34;disk-performance-counters&#34;&gt;Disk Performance Counters&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/disk-counters.jpg&#34; alt=&#34;Disk Performance Monitor Counters&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;disk-bottleneck-resolutions&#34;&gt;Disk Bottleneck Resolutions&lt;/h2&gt;

&lt;p&gt;A few of the common disk bottleneck resolutions are as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Optimizing application workload&lt;/li&gt;
&lt;li&gt;Using a faster I/O path&lt;/li&gt;
&lt;li&gt;Using a RAID array&lt;/li&gt;
&lt;li&gt;Using a SAN system&lt;/li&gt;
&lt;li&gt;Using Solid State Drives&lt;/li&gt;
&lt;li&gt;Aligning disks properly&lt;/li&gt;
&lt;li&gt;Using a battery-backed controller cache&lt;/li&gt;
&lt;li&gt;Adding system memory&lt;/li&gt;
&lt;li&gt;Creating multiple files and filegroups&lt;/li&gt;
&lt;li&gt;Moving the log files to a separate physical drive&lt;/li&gt;
&lt;li&gt;Using partitioned tables&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;processor-bottleneck-analysis&#34;&gt;Processor Bottleneck Analysis&lt;/h2&gt;

&lt;h3 id=&#34;performance-monitor-cpu-counters&#34;&gt;Performance Monitor CPU Counters&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/cpu-counters.jpg&#34; alt=&#34;Performance Monitor CPU Counters&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;processor-bottleneck-resolutions&#34;&gt;Processor Bottleneck Resolutions&lt;/h2&gt;

&lt;p&gt;You can use the following DMVs to monitor CPU pressure :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;sys.dm_os_workers&lt;/li&gt;
&lt;li&gt;sys.dm_os_schedulers&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A few of the common processor bottleneck resolutions are as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Optimizing application workload&lt;/li&gt;
&lt;li&gt;Eliminating or reducing excessive compiles/recompiles&lt;/li&gt;
&lt;li&gt;Using more or faster processors&lt;/li&gt;
&lt;li&gt;Not running unnecessary software&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;network-bottleneck-analysis&#34;&gt;Network Bottleneck Analysis&lt;/h2&gt;

&lt;p&gt;There are few issues related to networking issues on a production server but you should pay attention and monitor the performance of your network.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/network-counters.jpg&#34; alt=&#34;Network Counters&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;network-bottleneck-resolutions&#34;&gt;Network Bottleneck Resolutions&lt;/h2&gt;

&lt;p&gt;A few of the common network bottleneck resolutions are as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Optimizing application workload&lt;/li&gt;
&lt;li&gt;Adding network adapters&lt;/li&gt;
&lt;li&gt;Moderating and avoiding interruptions&lt;/li&gt;
&lt;li&gt;Let’s consider these resolutions in more detail&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;sql-server-overall-performance&#34;&gt;SQL Server Overall Performance&lt;/h2&gt;

&lt;p&gt;To analyze the overall performance of a SQL Server instance, besides examining hardware resource utilization, you should also examine some general aspects of SQL Server itself. You can use the performance counters presented below :&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Object(Instance[,InstanceN])&lt;/th&gt;
&lt;th&gt;Counter&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;SQLServer:Access&lt;/td&gt;
&lt;td&gt;Methods FreeSpace Scans/secFull Scans/secTable Lock Escalations/secWorktables Created/sec&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;SQLServer:Latches&lt;/td&gt;
&lt;td&gt;Total Latch Wait Time (ms)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;SQLServer:Locks(_Total)&lt;/td&gt;
&lt;td&gt;Lock Timeouts/secLock Wait Time (ms)Number of Deadlocks/sec&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;SQLServer:SQL Statistics&lt;/td&gt;
&lt;td&gt;Batch Requests/secSQL Re-Compilations/sec&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;SQLServer:General Statistics&lt;/td&gt;
&lt;td&gt;Processes BlockedUser ConnectionsTemp Tables Creation RateTemp Tables for Destruction&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;creating-a-baseline&#34;&gt;Creating a Baseline&lt;/h2&gt;

&lt;p&gt;You can use the following steps to create a baseline :&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Create a reusable list of performance counters.&lt;/li&gt;
&lt;li&gt;Create a counter log using your list of performance counters.&lt;/li&gt;
&lt;li&gt;Minimize Performance Monitor overhead.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>SQL Query Performance Analysis</title>
      <link>http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/sql-query-performance-analysis/</link>
      <pubDate>Sat, 18 Mar 2017 14:51:52 +0200</pubDate>
      
      <guid>http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/sql-query-performance-analysis/</guid>
      <description>

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#extended-events&#34;&gt;Extended Events&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#benefits-of-sql-server-extended-events&#34;&gt;Benefits of SQL Server Extended Events&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#sql-server-management-studio-extended-events-integration&#34;&gt;SQL Server Management Studio Extended Events Integration&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#extended-events-concepts&#34;&gt;Extended Events Concepts&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#package&#34;&gt;Package&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#package-contents&#34;&gt;Package Contents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#events&#34;&gt;Events&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#event-categorization&#34;&gt;Event Categorization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#channel&#34;&gt;Channel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#keyword&#34;&gt;Keyword&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#targets&#34;&gt;Targets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#actions&#34;&gt;Actions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#predicates&#34;&gt;Predicates&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#types&#34;&gt;Types&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#maps&#34;&gt;Maps&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#demo-creating-a-session&#34;&gt;Demo Creating a Session&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#extended-events-automation&#34;&gt;Extended Events Automation&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#scripting-the-session&#34;&gt;Scripting the Session&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#testing-the-session-with-data&#34;&gt;Testing the Session with Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#displaying-results&#34;&gt;Displaying Results&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#view-target-data&#34;&gt;View Target Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#watch-live-data&#34;&gt;Watch Live Data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;extended-events&#34;&gt;Extended Events&lt;/h2&gt;

&lt;p&gt;In this module we will cover using Extended Events. SQL Server Extended Events has a highly scalable and highly configurable architecture that allows users to collect as much or as little information as is necessary to troubleshoot or identify a performance problem.&lt;/p&gt;

&lt;h2 id=&#34;benefits-of-sql-server-extended-events&#34;&gt;Benefits of SQL Server Extended Events&lt;/h2&gt;

&lt;p&gt;Extended Events allows you to do the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Graphically monitor SQL Server queries&lt;/li&gt;
&lt;li&gt;Collect query information in the background&lt;/li&gt;
&lt;li&gt;Analyze performance&lt;/li&gt;
&lt;li&gt;Diagnose problems such as deadlocks&lt;/li&gt;
&lt;li&gt;Debug a Transact-SQL (T-SQL) statement&lt;/li&gt;
&lt;li&gt;Extended Events is a light weight performance monitoring system that uses very few performance resources.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;sql-server-management-studio-extended-events-integration&#34;&gt;SQL Server Management Studio Extended Events Integration&lt;/h3&gt;

&lt;p&gt;SQL Server Management Studio provides an excellent user interface (UI) for extended events. The UI is so good that many users have no need to engage with extended events by using Transact-SQL or the dynamic management views (DMVs) that target extended events.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/extended-events-ssms-integration.png&#34; alt=&#34;Extended Events SQL Server Integration&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;extended-events-concepts&#34;&gt;Extended Events Concepts&lt;/h2&gt;

&lt;p&gt;SQL Server Extended Events (Extended Events) builds on existing concepts, such as an event or an event consumer, uses concepts from Event Tracing for Windows(ETW), and introduces new concepts.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Package&lt;/li&gt;
&lt;li&gt;Session&lt;/li&gt;
&lt;li&gt;Events&lt;/li&gt;
&lt;li&gt;Targets&lt;/li&gt;
&lt;li&gt;Predicates&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;package&#34;&gt;Package&lt;/h3&gt;

&lt;p&gt;A &lt;strong&gt;package&lt;/strong&gt; is a container for SQL Server Extended Events objects. There are three kinds of Extended Events packages, which include the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;package0&lt;/strong&gt; - Extended Events system objects. This is the default package.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;sqlserver&lt;/strong&gt; - SQL Server related objects.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;sqlos&lt;/strong&gt; - SQL Server Operating System (SQLOS) related objects.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Packages are identified by a name, a GUID, and the binary module that contains the package. We can use the &lt;code&gt;sys.dm_xe_packages&lt;/code&gt; DMV to look at more information about packages. Here is an example listing the available packages and the dll file providing the module :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT xe.name, xe.description, os.name
FROM sys.dm_xe_packages xe
JOIN sys.dm_os_loaded_modules os
ON xe.module_address = os.base_address
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;package-contents&#34;&gt;Package Contents&lt;/h3&gt;

&lt;p&gt;The following illustration shows the objects that can exist in packages, which are contained in a module. A module can be an executable or a dynamic link library.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/xepackagesobjects.gif&#34; alt=&#34;Package Contents&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;events&#34;&gt;Events&lt;/h3&gt;

&lt;p&gt;Events are monitoring points of interest in the execution path of a program, such as SQL Server. An event firing carries with it the fact that the point of interest was reached, and state information from the time the event was fired.&lt;/p&gt;

&lt;p&gt;Events can be used solely for tracing purposes or for triggering actions. These actions can either be synchronous or asynchronous.&lt;/p&gt;

&lt;div class=&#34;admonition note&#34;&gt;
&lt;p class=&#34;admonition-title&#34;&gt;Note&lt;/p&gt;
&lt;p&gt;An event does not have any knowledge of the actions that may be triggered in response to the event firing.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;All events have a versioned schema which defines their contents. This schema is composed of event columns with well defined types. An event of a specific type must always provide its data in exactly the same order that is specified in the schema. However, an event target does not have to consume all the data that is provided.&lt;/p&gt;

&lt;p&gt;We can query for available events using the following DMV, &lt;code&gt;sys.dm_xe_objects&lt;/code&gt; as follows :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT name, description
FROM sys.dm_xe_objects
WHERE object_type = &#39;event&#39;
ORDER BY name
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;event-categorization&#34;&gt;Event Categorization&lt;/h3&gt;

&lt;p&gt;Extended Events uses an event categorization model similar to Event Tracing for Windows (ETW). Two event properties are used for categorization, channel and keyword.&lt;/p&gt;

&lt;h3 id=&#34;channel&#34;&gt;Channel&lt;/h3&gt;

&lt;p&gt;A channel identifies the audience for an event. These channels are described in the following table.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Term&lt;/th&gt;
&lt;th&gt;Definition&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Admin&lt;/td&gt;
&lt;td&gt;Admin events are primarily targeted to the end users, administrators, and support. The events that are found in the admin channels indicate a problem with a well-defined solution that an administrator can act on. An example of an admin event is when an application fails to connect to a printer. These events are either well-documented or have a message associated with them that tells the reader what to do to rectify the problem.&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Operational&lt;/td&gt;
&lt;td&gt;Operational events are used for analyzing and diagnosing a problem or occurrence. They can be used to trigger tools or tasks based on the problem or occurrence. An example of an operational event is when a printer is added or removed from a system.&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Analytic&lt;/td&gt;
&lt;td&gt;Analytic events are published in high volume. They describe program operation and are typically used in performance investigations.&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Debug&lt;/td&gt;
&lt;td&gt;Debug events are used solely by developers to diagnose a problem for debugging.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Events in the Debug channel return internal implementation-specific state data. The schemas and data that the events return may change or become invalid in future versions of SQL Server. Therefore, events in the Debug channel may change or be removed in future versions of SQL Server without notice.&lt;/p&gt;

&lt;h3 id=&#34;keyword&#34;&gt;Keyword&lt;/h3&gt;

&lt;p&gt;A keyword is application specific and enables a finer-grained grouping of related events, which makes it easier for you to specify and retrieve an event that you want to use in a session. You can use the following query to obtain keyword information.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT map_value Keyword 
FROM sys.dm_xe_map_values  
WHERE name = &#39;keyword_map&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;targets&#34;&gt;Targets&lt;/h3&gt;

&lt;p&gt;SQL Server Extended Events targets are event consumers. Targets can write to a file, store event data in a memory buffer, or aggregate event data. Targets can process data synchronously or asynchronously.&lt;/p&gt;

&lt;p&gt;Extended Events provide the following targets that you can use for an Extended Events session:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Event counter&lt;/strong&gt; - Counts all specified events that occur during an Extended Events session. Use to obtain information about workload characteristics without adding the overhead of full event collection. This is a synchronous target.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Event file&lt;/strong&gt; - Use to write event session output from complete memory buffers to disk. This is an asynchronous target.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Event pairing - Many kinds of events occur in pairs, such as lock acquires and lock releases. Use to determine when a specified paired event does not occur in a matched set. This is an asynchronous target.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Event Tracing for Windows (ETW)&lt;/strong&gt; - Use to correlate SQL Server events with Windows operating system or application event data. This is a synchronous target.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Histogram&lt;/strong&gt; - Use to count the number of times that a specified event occurs, based on a specified event column or action. This is an asynchronous target.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Ring buffer&lt;/strong&gt; - Use to hold the event data in memory on a first-in first-out (FIFO) basis, or on a per-event FIFO basis. This is an asynchronous target.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;actions&#34;&gt;Actions&lt;/h3&gt;

&lt;p&gt;An action is a programmatic response or series of responses to an event. Actions are bound to an event, and each event may have a unique set of actions. Actions can:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Capture a stack dump and inspect data.&lt;/li&gt;
&lt;li&gt;Capture the query plan hash&lt;/li&gt;
&lt;li&gt;Capture the sql query hash&lt;/li&gt;
&lt;li&gt;Store state information in a local context using variable storage.&lt;/li&gt;
&lt;li&gt;Aggregate event data.&lt;/li&gt;
&lt;li&gt;Append data to event data.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;predicates&#34;&gt;Predicates&lt;/h3&gt;

&lt;p&gt;Predicates are a set of logical rules that are used to evaluate events when they are processed. This enables the Extended Events user to selectively capture event data based on specific criteria.&lt;/p&gt;

&lt;p&gt;Predicates are evaluated as full Boolean expressions, and support short circuiting at the first point where the entire expression is found to be false.&lt;/p&gt;

&lt;p&gt;Predicates can store data in a local context that can be used for creating predicates that return true once every n minutes or every n times that an event fires.&lt;/p&gt;

&lt;h3 id=&#34;types&#34;&gt;Types&lt;/h3&gt;

&lt;p&gt;Because data is a collection of bytes strung together, the length and characteristics of the byte collection are required in order to interpret the data. This information is encapsulated in the Type object. The following types are provided for package objects:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;event&lt;/li&gt;
&lt;li&gt;action&lt;/li&gt;
&lt;li&gt;target&lt;/li&gt;
&lt;li&gt;pred_source&lt;/li&gt;
&lt;li&gt;pred_compare&lt;/li&gt;
&lt;li&gt;type&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We can use the following to look at the type available :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT object_type, count(*) total
FROM sys.dm_xe_objects
GROUP BY object_type
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;maps&#34;&gt;Maps&lt;/h3&gt;

&lt;p&gt;A map table maps an internal value to a string, which enables a user to know what the value represents. Instead of only being able to obtain a numeric value, a user can get a meaningful description of the internal value. The following query shows how to obtain map values.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT map_key, map_value, name
FROM sys.dm_xe_map_values
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;demo-creating-a-session&#34;&gt;Demo Creating a Session&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Connect with &lt;strong&gt;SSMS&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;In the Object Explorer, click &lt;strong&gt;Management&lt;/strong&gt; &amp;gt; &lt;strong&gt;Extended Events&lt;/strong&gt; &amp;gt; &lt;strong&gt;New Session&lt;/strong&gt;. The &lt;strong&gt;New Session&lt;/strong&gt; dialog is preferable to the &lt;strong&gt;New Session Wizard&lt;/strong&gt;, although the two are similar to each other.&lt;/li&gt;

&lt;li&gt;&lt;p&gt;In the upper-left, click the &lt;strong&gt;General&lt;/strong&gt; page. Then type &lt;strong&gt;Demo&lt;/strong&gt;, or any name you like, into the Session name text box. Do not press the &lt;strong&gt;OK&lt;/strong&gt; button yet, that comes only at the end of the demo.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/extended-events/choose-session-name.png&#34; alt=&#34;Extended Events Session Wizard&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You can choose some predefined session here using the template, but in this section we will choose the events manually.&lt;/p&gt;

&lt;p&gt;Using the wizard we can create a session to gather performance data. Here are the available templates :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Query Batch Sampling&lt;/strong&gt; - This template will capture queries and procedure calls for 20 percent of all active sessions on the server.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Query Batch Tracking&lt;/strong&gt; - This template captures all queries and procedures for all sessions on the server.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Query Detail Sampling&lt;/strong&gt; - This template contains a set of events that will capture every statement in queries and procedures for 20 percent of all active sessions on the server.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Query Detail Tracking&lt;/strong&gt; - This template is the same as Query Batch Tracking, but for every single statement in the system as well. This generates a large amount of data.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Query Wait Statistic&lt;/strong&gt; - This template captures wait statistics for each statement of every query and procedure for 20 percent of all active sessions.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/extended-events/session-templates.png&#34; alt=&#34;Session Tempaltes&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;In the upper-left, click the &lt;strong&gt;Events&lt;/strong&gt; page, and then click the &lt;strong&gt;Select&lt;/strong&gt; button. &lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/extended-events/choose-an-event.png&#34; alt=&#34;Event Selection&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;In the &lt;strong&gt;Event library&lt;/strong&gt; area, in the drop-down list, choose &lt;strong&gt;Event names&lt;/strong&gt; only.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Into the text box, type in &lt;strong&gt;sql&lt;/strong&gt;, which filters and reduces the long list of available events by using a contains operator.&lt;/li&gt;
&lt;li&gt;Scroll and click the event named &lt;strong&gt;sql_statement_completed&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Click the right arrow button &lt;strong&gt;&amp;gt;&lt;/strong&gt; to move the event to the &lt;strong&gt;Selected events&lt;/strong&gt; box.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Staying on the Events page, click the &lt;strong&gt;Configure&lt;/strong&gt; button at the far right.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;With the left side chopped off for better display, in the following screenshot you can see the Event configuration options area.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Click the &lt;strong&gt;Filter (Predicate)&lt;/strong&gt; tab. Next, click &lt;strong&gt;Click here to add a clause&lt;/strong&gt;, for the intention of capturing all SQL SELECT statements that have a HAVING clause.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;In the Field drop-down list, and choose &lt;strong&gt;sqlserver.sql_text&lt;/strong&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;For Operator choose a &lt;strong&gt;LIKE&lt;/strong&gt; operator.&lt;/li&gt;
&lt;li&gt;For Value type in &lt;strong&gt;%SELECT%HAVING%&lt;/strong&gt; &lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/extended-events/add-a-filter.png&#34; alt=&#34;Extended Events add a Predicate&#34; /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;In the upper-left, click the &lt;strong&gt;Data Storage&lt;/strong&gt; page.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;In the &lt;strong&gt;Targets&lt;/strong&gt; area, click &lt;strong&gt;Click here to a target&lt;/strong&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;In the &lt;strong&gt;Type&lt;/strong&gt; drop-down list, choose &lt;strong&gt;event_file&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;This means the event data will be stored in a file that we can view.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;In the &lt;strong&gt;Properties&lt;/strong&gt; area, type in a full path and file name into the File name on server text box.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The file name extension must be &lt;em&gt;.xel&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Our little test will need less than 1 MB of file size. &lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/extended-events/choose-a-file-target.png&#34; alt=&#34;Choose a file target&#34; /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;In the upper-left, click the &lt;strong&gt;Advanced&lt;/strong&gt; page.
Leave the &lt;strong&gt;Maximum dispatch latency&lt;/strong&gt; at 30 seconds.
Finally, click the &lt;strong&gt;OK&lt;/strong&gt; button at the bottom. &lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/extended-events/choose-maximum-dispatch-latency.png&#34; alt=&#34;Choose Maximum Dispatch Latence&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Back in the &lt;strong&gt;Object Explorer&lt;/strong&gt;, expand &lt;strong&gt;Management&lt;/strong&gt; &amp;gt; &lt;strong&gt;Sessions&lt;/strong&gt;, and see the new node for &lt;strong&gt;Demo&lt;/strong&gt;. &lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/extended-events/object-explorer-extended-events.png&#34; alt=&#34;Extended Eevents Object Explorer Properties&#34; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;extended-events-automation&#34;&gt;Extended Events Automation&lt;/h2&gt;

&lt;p&gt;You can script the Extended Events Session from within the wizard. This script can then be applied on other server to gather the same metrics.&lt;/p&gt;

&lt;h3 id=&#34;scripting-the-session&#34;&gt;Scripting the Session&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Define a session.&lt;/li&gt;
&lt;li&gt;Right-click the &lt;strong&gt;Demo&lt;/strong&gt; session, and select &lt;strong&gt;Script Sessions As&lt;/strong&gt; &amp;gt; &lt;strong&gt;CREATE To&lt;/strong&gt; &amp;gt; &lt;strong&gt;File&lt;/strong&gt; to output straight to a file. Or, use the &lt;strong&gt;Script&lt;/strong&gt; button at the top of the &lt;strong&gt;New Session&lt;/strong&gt; window to create a T-SQL command in the Query window.&lt;/li&gt;
&lt;li&gt;These steps will generate the script that you need to create a session and output it to a file.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/extended-events/script-session.png&#34; alt=&#34;Script Session to File&#34; /&gt;&lt;/p&gt;

&lt;p&gt;To manually create this new trace, use Management Studio as follows:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Open the script file or navigate to the Query window.&lt;/li&gt;
&lt;li&gt;Modify the path and file location for the server you’re creating this session on.&lt;/li&gt;
&lt;li&gt;Execute the script.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/extended-events/session-properties-script-to-file.png&#34; alt=&#34;Script Session to a File using Session Properties&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Here is the SQL script of the session :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;CREATE EVENT SESSION [Demo] ON SERVER 
ADD EVENT sqlserver.sql_statement_completed(
    WHERE ([sqlserver].[like_i_sql_unicode_string]([sqlserver].[sql_text],N&#39;%SELECT%HAVING%&#39;)))
ADD TARGET package0.event_file(SET filename=N&#39;C:\Temp\Demo.xel&#39;,max_file_size=(10))
WITH (MAX_MEMORY=4096 KB,
		EVENT_RETENTION_MODE=ALLOW_SINGLE_EVENT_LOSS,
		MAX_DISPATCH_LATENCY=30 SECONDS,MAX_EVENT_SIZE=0 KB,
		MEMORY_PARTITION_MODE=NONE,
		TRACK_CAUSALITY=OFF,
		STARTUP_STATE=OFF)
GO
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once the session is created, you can use the following command to start it:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;ALTER EVENT SESSION [Demo]
ON SERVER
STATE = START;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;testing-the-session-with-data&#34;&gt;Testing the Session with Data&lt;/h2&gt;

&lt;p&gt;Test your event session with these simple steps:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;In the SSMS &lt;strong&gt;Object Explorer&lt;/strong&gt;, right-click &lt;strong&gt;Demo&lt;/strong&gt; session node, and then click &lt;strong&gt;Start Session&lt;/strong&gt;.&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Run the following SELECT&amp;hellip;HAVING statement a couple times.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Ideally you might change the HAVING Count value between the two runs, toggling between 2 and 3. This enables you to see the differences in the results.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;    SELECT
        c.name,
        Count(*)  AS [Count-Per-Column-Repeated-Name]
    FROM
            sys.syscolumns  AS c
        JOIN sys.sysobjects  AS o

            ON o.id = c.id
    WHERE
        o.type = &#39;V&#39;
        AND
        c.name like &#39;%event%&#39;
    GROUP BY
        c.name
    HAVING
        Count(*) &amp;gt;= 3   --2     -- Try both values during session.
    ORDER BY
        c.name;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Right-click your session node, and then click &lt;strong&gt;Stop Session&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;displaying-results&#34;&gt;Displaying Results&lt;/h2&gt;

&lt;p&gt;You can view the results in two ways :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;View target data&lt;/li&gt;
&lt;li&gt;Watch live data&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;view-target-data&#34;&gt;View Target Data&lt;/h3&gt;

&lt;p&gt;In the SSMS &lt;strong&gt;Object Explorer&lt;/strong&gt;, you can right-click the target node which is under your event session node. In the context menu you click &lt;strong&gt;View Target Data&lt;/strong&gt;. SSMS displays the data.
The display is not updated as new data is reported by the event. But you can click View Target Data again.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/extended-events/view-target-data.png&#34; alt=&#34;View Target Data&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;watch-live-data&#34;&gt;Watch Live Data&lt;/h3&gt;

&lt;p&gt;n the SSMS &lt;strong&gt;Object Explorer&lt;/strong&gt;, you can right-click your &lt;strong&gt;Demo&lt;/strong&gt; session node. In the context menu you click &lt;strong&gt;Watch Live Data&lt;/strong&gt;. SSMS displays incoming data as it continues to arrive in real time.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/extended-events/watch-live-data.png&#34; alt=&#34;Watch Live Data&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Index Analysis</title>
      <link>http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/index-analysis/</link>
      <pubDate>Sat, 18 Mar 2017 14:52:09 +0200</pubDate>
      
      <guid>http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/index-analysis/</guid>
      <description>

&lt;p&gt;One of the best ways to reduce disk I/O is to use an index. An index allows SQL Server to find data in a table without scanning the entire table.&lt;/p&gt;

&lt;p&gt;The following query scans the entire table to retrieve the data :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT TOP 10
        p.ProductID,
        p.[Name],
        p.StandardCost,
        p.[Weight],
        ROW_NUMBER() OVER (ORDER BY p.Name DESC) AS RowNumber
FROM Production.Product p
ORDER BY p.Name DESC;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The table will still be scanned if we were to add a WHERE clause on StandardCost column.&lt;/p&gt;

&lt;h3 id=&#34;index-terms&#34;&gt;Index Terms&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Heap&lt;/strong&gt; - A heap is a data structure where rows are stored without a specified order. In other words, it is a table without a clustered index.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Clustered index&lt;/strong&gt; - A table can only have one clustered index and the data is sorted on the clustered index.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Nonclustered index&lt;/strong&gt; - A B-Tree structure that contains the index key values and a pointer to the data row on the base table. It can be created on a heap on a clustered index. Each table can have up to 999 nonclustered indexes. A nonclustered index can optionally contain non-key columns when using the &lt;strong&gt;INCLUDE&lt;/strong&gt; clause, which are particularly useful when covering a query.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Unique index&lt;/strong&gt; - a unique index does not allow two rows of data to have identical key values. A table can have one or more unique indexes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Primary key&lt;/strong&gt; - A primary key is a key that uniquely identifies each record in the table and creates a unique index, which, by default, will also be a clustered index.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;creating-a-primary-key&#34;&gt;Creating a Primary Key&lt;/h3&gt;

&lt;p&gt;By default when a primary key is created, it creates a unique, clustered index. This is the behaviour when using the designer in SSMS and also the ALTER TABLE statement.&lt;/p&gt;

&lt;p&gt;If you run the following queries, the will both produce a clustered index :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- if you run the following code to create a primary key, where the CLUSTERED or NONCLUSTERED keywords are not specified
CREATE TABLE table1 (
 col1 int NOT NULL,
 col2 nchar(10) NULL,
CONSTRAINT PK_table1 PRIMARY KEY(col1)
)

-- or
CREATE TABLE table1
(
 col1 int NOT NULL,
 col2 nchar(10) NULL
)
GO
ALTER TABLE table1 ADD CONSTRAINT
PK_table1 PRIMARY KEY
(
 col1
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/primark-key-default-sql.png&#34; alt=&#34;Primary Key Clsutered Index&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;create-index-syntax&#34;&gt;Create Index Syntax&lt;/h3&gt;

&lt;p&gt;The following is the syntax to creating an index :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;CREATE [ UNIQUE ] [ CLUSTERED | NONCLUSTERED ] INDEX index_name
    ON &amp;lt;object&amp;gt; ( column [ ASC | DESC ] [ ,...n ] )
    [ INCLUDE ( column_name [ ,...n ] ) ]
    [ WHERE &amp;lt;filter_predicate&amp;gt; ]
    [ WITH ( &amp;lt;relational_index_option&amp;gt; [ ,...n ] ) ]
    [ ON { partition_scheme_name ( column_name )
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;modifying-the-index&#34;&gt;Modifying the Index&lt;/h3&gt;

&lt;p&gt;The &lt;strong&gt;ALTER INDEX&lt;/strong&gt; command is used to modify an existing table or view index (relational or XML) by disabling, rebuilding, or reorganizing the index; or by setting options on the index.&lt;/p&gt;

&lt;p&gt;The syntax is as follows :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;ALTER INDEX { index_name | ALL } ON &amp;lt;object&amp;gt;
{
      REBUILD {
            [ PARTITION = ALL ] [ WITH ( &amp;lt;rebuild_index_option&amp;gt; [ ,...n ] ) ]
          | [ PARTITION = partition_number [ WITH ( &amp;lt;single_partition_rebuild_index_option&amp;gt; ) [ ,...n ] ]
      }
    | DISABLE
    | REORGANIZE  [ PARTITION = partition_number ] [ WITH ( &amp;lt;reorganize_option&amp;gt;  ) ]
    | SET ( &amp;lt;set_index_option&amp;gt; [ ,...n ] )
    }
[ ; ]
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;dropping-an-index&#34;&gt;Dropping an Index&lt;/h3&gt;

&lt;p&gt;We can the &lt;strong&gt;DROP INDEX&lt;/strong&gt; command remove an existing index and the syntax is as follows :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;DROP INDEX [ IF EXISTS ]
{ &amp;lt;drop_relational_or_xml_or_spatial_index&amp;gt; [ ,...n ]
| &amp;lt;drop_backward_compatible_index&amp;gt; [ ,...n ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;the following will drop the index name &lt;strong&gt;IX_ProductVendor_BusinessEntityID&lt;/strong&gt; from the &lt;strong&gt;Purchasing.ProductVendor&lt;/strong&gt; table.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;DROP INDEX IX_ProductVendor_BusinessEntityID
    ON Purchasing.ProductVendor;
GO
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and we can also drop multiple indexes as follows :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;DROP INDEX
    IX_PurchaseOrderHeader_EmployeeID ON Purchasing.PurchaseOrderHeader,
    IX_Address_StateProvinceID ON Person.Address;
GO
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;dropping-a-primary-key-constraint-online&#34;&gt;Dropping a PRIMARY KEY constraint online&lt;/h3&gt;

&lt;p&gt;Indexes that are created as the result of creating &lt;strong&gt;PRIMARY KEY&lt;/strong&gt; or &lt;strong&gt;UNIQUE&lt;/strong&gt; constraints cannot be dropped by using &lt;strong&gt;DROP INDEX&lt;/strong&gt;. They are dropped using the &lt;strong&gt;ALTER TABLE DROP CONSTRAINT&lt;/strong&gt; statement.&lt;/p&gt;

&lt;p&gt;The following example deletes a clustered index with a P&lt;strong&gt;RIMARY KEY&lt;/strong&gt; constraint by dropping the constraint. The &lt;strong&gt;ProductCostHistory&lt;/strong&gt; table has no **FOREIGN KEY **constraints. If it did, those constraints would have to be removed first.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- Set ONLINE = OFF to execute this example on editions other than Enterprise Edition.
ALTER TABLE Production.TransactionHistoryArchive
DROP CONSTRAINT PK_TransactionHistoryArchive_TransactionID
WITH (ONLINE = ON);
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;creating-indexes&#34;&gt;Creating Indexes&lt;/h3&gt;

&lt;p&gt;Let’s do a quick exercise to show some of these concepts and T-SQL statements mentioned in this section. Create a new table by running the following statement:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- create a new table by running the following statement:
SELECT * INTO dbo.SalesOrderDetail
FROM Sales.SalesOrderDetail
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;view-the-indexes-for-the-table&#34;&gt;View the indexes for the Table&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- let’s use the sys.indexes catalog view to inspect the table properties:
SELECT * FROM sys.indexes
WHERE object_id = OBJECT_ID(&#39;dbo.SalesOrderDetail&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/heap.png&#34; alt=&#34;Heap&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;create-a-non-clustered-index&#34;&gt;Create a non-clustered index&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- let’s create a nonclustered index:
CREATE INDEX IX_ProductID ON dbo.SalesOrderDetail(ProductID)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/non-clustered-index.png&#34; alt=&#34;Non Clustered Index&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Nonclustered indexes can have index_id values between 2 and 250 and between 256 and 1005. The values between 251 and 255 are reserved&lt;/p&gt;

&lt;h3 id=&#34;create-a-clustered-index&#34;&gt;Create a clustered index&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- now create a clustered index:
CREATE CLUSTERED INDEX IX_SalesOrderID_SalesOrderDetailID
ON dbo.SalesOrderDetail(SalesOrderID, SalesOrderDetailID)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/clustered-index.png&#34; alt=&#34;Clustered Index&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Note that instead of a heap, now we have a clustered index and the index_id is now 1. A clustered index always has an index_id of 1. Internally, the nonclustered index has been rebuilt to now use a cluster key pointer rather than a row identifier (RID).&lt;/p&gt;

&lt;p&gt;Dropping the nonclustered index will remove the index pages entirely, leaving only the clustered index:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- dropping the nonclustered index will remove the index pages entirely, leaving only the clustered index:
DROP INDEX dbo.SalesOrderDetail.IX_ProductID
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But notice that deleting the clustered index, which is considered the entire table, does not delete the underlying data, but simply changes the table structure to be a heap:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- but notice that deleting the clustered index, which is considered the entire table
DROP INDEX dbo.SalesOrderDetail.IX_SalesOrderID_SalesOrderDetail
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/heap.png&#34; alt=&#34;Heap&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;choosing-a-clustered-index&#34;&gt;Choosing a Clustered Index&lt;/h3&gt;

&lt;p&gt;You definitely want to use a clustered index in the following cases:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;You frequently need to return data in a sorted order or query ranges of data. In this case, you would need to create the clustered index key on the column’s desired order.&lt;/li&gt;
&lt;li&gt;You frequently need to return data grouped together. In this case, you would need to create the clustered index key on the columns used by the GROUP BY clause.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;clustered-vs-nonclustered-indexes&#34;&gt;Clustered vs. Nonclustered Indexes&lt;/h3&gt;

&lt;p&gt;The main considerations in choosing between a clustered and a nonclustered index are as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Number of rows to be retrieved&lt;/li&gt;
&lt;li&gt;Data-ordering requirement&lt;/li&gt;
&lt;li&gt;Index key width&lt;/li&gt;
&lt;li&gt;Column update frequency&lt;/li&gt;
&lt;li&gt;Lookup cost&lt;/li&gt;
&lt;li&gt;Any disk hot spots&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;covering-indexes&#34;&gt;Covering Indexes&lt;/h2&gt;

&lt;p&gt;A covering index is a nonclustered index built upon all the columns required to satisfy a SQL query without going to the heap or the clustered index. If a query encounters an index and does not need to refer to the underlying structures at all, then the index can be considered a covering index.&lt;/p&gt;

&lt;p&gt;Using the following query you will notice that the column PostalCode is needed but we have to perform a lookup in order to retrieve the data not satisfied by the index :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- for example, the following query is already covered by an existing index, IX_SalesOrderHeader_CustomerID
SELECT SalesOrderID, CustomerID FROM Sales.SalesOrderHeader
WHERE CustomerID = 16448
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There is no need to access the base table at all. If we slightly change the query to also request the SalesPersonID column, this time, there is no index that covers the query :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- if we slightly change the query to also request the SalesPersonID column
SELECT SalesOrderID, CustomerID, SalesPersonID FROM Sales.SalesOrderHeader
WHERE CustomerID = 16448
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/non-covered-index.png&#34; alt=&#34;Non Covered Index&#34; /&gt;&lt;/p&gt;

&lt;p&gt;WE can use the INCLUDE keyword when creating the index in order to include the index column without adding it to the index keys as follows :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- at this point, you may decide to just update an existing index to include the required column
CREATE INDEX IX_SalesOrderHeader_CustomerID_SalesPersonID
ON Sales.SalesOrderHeader(CustomerID)
INCLUDE (SalesPersonID
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Re-run the query and notice the plan with an index seek. Drop the index :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- finally, to clean up, drop the temporarily created index:
DROP INDEX Sales.SalesOrderHeader.IX_SalesOrderHeader_CustomerID_SalesPersonID
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The INCLUDE is best used in the following cases:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;You don’t want to increase the size of the index keys, but you still want to make the index a covering index.&lt;/li&gt;
&lt;li&gt;You have a data type that cannot be an index key column but can be added to the nonclustered index through the INCLUDE command.&lt;/li&gt;
&lt;li&gt;You’ve already exceeded the maximum number of key columns for an index (although this is a problem best avoided).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;filtered-indexes&#34;&gt;Filtered Indexes&lt;/h2&gt;

&lt;p&gt;A filtered index is a nonclustered index that uses a filter, basically a WHERE clause, to create a highly selective set of keys against a column or columns that may not have good selectivity otherwise.&lt;/p&gt;

&lt;p&gt;The Sales.SalesOrderHeader table has more than 30,000 rows. Of those rows, 27,000+ have a null value in the PurchaseOrderNumber column and the SalesPersonId column. If you wanted to get a simple list of purchase order numbers, the query might look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT  soh.PurchaseOrderNumber,
        soh.OrderDate,
        soh.ShipDate,
        soh.SalesPersonID
FROM    Sales.SalesOrderHeader AS soh
WHERE   PurchaseOrderNumber LIKE &#39;PO5%&#39;
        AND soh.SalesPersonID IS NOT NULL;
;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Running the query results in, as you might expect, a clustered index scan, and the following I/O and execution time.&lt;/p&gt;

&lt;p&gt;To fix this, it is possible to create an index and include some of the columns from the query to make this a covering index.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;CREATE NONCLUSTERED INDEX IX_Test
ON Sales.SalesOrderHeader(PurchaseOrderNumber,SalesPersonID)
INCLUDE  (OrderDate,ShipDate);
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;Table &#39;SalesOrderHeader&#39;. Scan count 1, logical reads 5
CPU time = 0 ms,  elapsed time = 69 ms.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see, the covering index dropped the reads from 689 to 5 and the time from 87 ms to 69 ms. Normally, this would be enough. Assume for a moment that this query has to be called frequently. Now, every bit of speed you can wring from it will pay dividends. Knowing that so much of the data in the indexed columns is null, you can adjust the index so that it filters out the null values, which aren’t used by the index anyway, reducing the size of the tree and therefore the amount of searching required.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;CREATE NONCLUSTERED INDEX IX_Test
ON Sales.SalesOrderHeader(PurchaseOrderNumber,SalesPersonID)
INCLUDE (OrderDate,ShipDate)
WHERE PurchaseOrderNumber IS NOT NULL AND SalesPersonID IS NOT NULL
WITH  (DROP_EXISTING = ON);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The final run of the query is visible in the following result :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;Table  &#39;SalesOrderHeader&#39;.  Scan count 1,  logical reads 4
CPU time = 0 ms,    elapsed time = 55 ms.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Filtered indexes improve performance in many ways.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Improving the efficiency of queries by reducing the size of the index&lt;/li&gt;
&lt;li&gt;Reducing storage costs by making smaller indexes&lt;/li&gt;
&lt;li&gt;Cutting down on the costs of index maintenance because of the reduced size&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;index-intersections&#34;&gt;Index Intersections&lt;/h2&gt;

&lt;p&gt;If a table has multiple indexes, then SQL Server can use multiple indexes to execute a query. SQL Server can take advantage of multiple indexes, selecting small subsets of data based on each index and then performing an intersection of the two subsets (that is, returning only those rows that meet all the criteria). SQL Server can exploit multiple indexes on a table and then employ a join algorithm to obtain the index intersection between the two subsets.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;--SELECT * is intentionally used in this query
SELECT  soh.*
FROM    Sales.SalesOrderHeader AS soh
WHERE   soh.SalesPersonID = 276
        AND soh.OrderDate BETWEEN &#39;4/1/2005&#39; AND &#39;7/1/2005&#39;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There&amp;rsquo;s no index on the OrderDate column so SQL Server will perform a scan of the table. We can include the column or create a new non-clustered index on the table as follows :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;CREATE NONCLUSTERED INDEX IX_Test
ON Sales.SalesOrderHeader (OrderDate);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And when we run the query you will notice SQL Server have used both indexes and created an index intersection.&lt;/p&gt;

&lt;h2 id=&#34;index-joins&#34;&gt;Index Joins&lt;/h2&gt;

&lt;p&gt;The index join is a variation of index intersection, where the covering index technique is applied to the index intersection. If no single index covers a query but multiple indexes together can cover the query, SQL Server can use an index join to satisfy the query fully without going to the base table.&lt;/p&gt;

&lt;p&gt;Let’s look at this indexing technique at work. Make a slight modification to the query from the “Index Intersections” section like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT  soh.SalesPersonID,
        soh.OrderDate
FROM    Sales.SalesOrderHeader AS soh
WHERE   soh.SalesPersonID = 276
        AND soh.OrderDate BETWEEN &#39;4/1/2005&#39; AND &#39;7/1/2005&#39;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Since the query requires the value of the OrderDate column also, the optimizer selected the clustered index to retrieve values for all the columns referred to in the query. If an index is created on the OrderDate column like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;CREATE NONCLUSTERED INDEX IX_Test
ON Sales.SalesOrderHeader (OrderDate ASC);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and the query is rerun.&lt;/p&gt;

&lt;p&gt;The combination of the two indexes acts like a covering index reducing the reads against the table from 689 to 4 because it’s using two Index Seek operations joined together instead of a clustered index scan.&lt;/p&gt;

&lt;p&gt;Since SQL Server didnt use the index, we can use a hint as follows :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT  soh.SalesPersonID,
        soh.OrderDate
FROM    Sales.SalesOrderHeader AS soh WITH
            (INDEX (IX_Test,
                    IX_SalesOrderHeader_SalesPersonID))
WHERE   soh.OrderDate BETWEEN &#39;4/1/2002&#39; AND &#39;7/1/2002&#39;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The reads have clearly increased, and you have work tables and work files that use tempdb to store data during the processing. Most of the time, the optimizer makes good choices when it comes to indexes and execution plans.&lt;/p&gt;

&lt;h2 id=&#34;index-operations&#34;&gt;Index Operations&lt;/h2&gt;

&lt;p&gt;Equality and inequality operators can be used in a predicate, including =, &amp;lt;, &amp;gt;, &amp;lt;=, &amp;gt;=, &amp;lt;&amp;gt;, !=, !&amp;lt;, !&amp;gt;, BETWEEN, and IN for seek operations.&lt;/p&gt;

&lt;p&gt;The following predicates can be matched to an Index Seek operation if there is an index on the specified column, or a multicolumn index with that column as a leading index key:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ProductID = 771&lt;/li&gt;
&lt;li&gt;UnitPrice &amp;lt; 3.975&lt;/li&gt;
&lt;li&gt;LastName = ’Allen’&lt;/li&gt;
&lt;li&gt;LastName LIKE ’Brown%’&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following query produces an Index Seek :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT ProductID, SalesOrderID, SalesOrderDetailID
FROM Sales.SalesOrderDetail
WHERE ProductID = 771 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The SalesOrderDetail table has a multicolumn index with ProductID as the leading column.&lt;/p&gt;

&lt;p&gt;An index cannot be used to seek on some complex expressions, expressions using functions, or strings with a leading wildcard character, as in the following predicates:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ABS(ProductID) = 771&lt;/li&gt;
&lt;li&gt;UnitPrice + 1 &amp;lt; 3.975&lt;/li&gt;
&lt;li&gt;LastName LIKE ’%Allen’&lt;/li&gt;
&lt;li&gt;UPPER(LastName) = ’Allen’&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following query produces a scan because of the function in the WHERE cause :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- compare the following query to the previous example
SELECT ProductID, SalesOrderID, SalesOrderDetailID
FROM Sales.SalesOrderDetail
WHERE ABS(ProductID) = 771
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the case of a multicolumn index, SQL Server can only use the index to seek on the second column if there is an equality predicate on the first column. So SQL Server can use a multicolumn index to seek on both columns in the following cases, supposing that a multicolumn index exists on both columns in the order presented:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ProductID = 771 AND SalesOrderID &amp;gt; 34000&lt;/li&gt;
&lt;li&gt;LastName = ’Smith’ AND FirstName = ’Ian’&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;That being said, if there is no equality predicate on the first column, or if the predicate cannot be evaluated on the second column, as is the case in a complex expression, then SQL Server may still only be able to use a multicolumn index to seek on just the first column, as in the following examples:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ProductID &amp;lt; 771 AND SalesOrderID = 34000&lt;/li&gt;
&lt;li&gt;LastName &amp;gt; ’Smith’ AND FirstName = ’Ian’&lt;/li&gt;
&lt;li&gt;ProductID = 771 AND ABS(SalesOrderID) = 34000&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;However, SQL Server is not able to use a multicolumn index for an Index Seek in the following examples because it is not even able to search on the first column:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ABS(ProductID) = 771 AND SalesOrderID = 34000&lt;/li&gt;
&lt;li&gt;LastName LIKE ’%Smith’ AND FirstName = ’Ian’&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following will only seek on the ProductID column :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT ProductID, SalesOrderID, SalesOrderDetailID
FROM Sales.SalesOrderDetail
WHERE ProductID = 771 AND ABS(SalesOrderID) = 45233
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;missing-indexes&#34;&gt;Missing Indexes&lt;/h2&gt;

&lt;p&gt;SQL Server does provide a second approach that can help you find useful indexes for your existing queries. Let’s take a quick look to see how this feature works. Create the dbo.SalesOrderDetail table on the AdventureWorks2012 database by running the following statement:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- create the dbo.SalesOrderDetail table on the AdventureWorks2012 database by running the following statement:
SELECT * INTO dbo.SalesOrderDetail
FROM Sales.SalesOrderDetail
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Run this query and request a graphical or XML execution plan:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- run this query and request a graphical or XML execution plan:
SELECT * FROM dbo.SalesOrderDetail
WHERE SalesOrderID = 43670 AND SalesOrderDetailID &amp;gt; 112
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This query could benefit from an index on the &lt;strong&gt;SalesOrderID&lt;/strong&gt; and &lt;strong&gt;SalesOrderDetailID&lt;/strong&gt; columns, but no missing indexes information is shown this time. One limitation of the Missing Indexes feature that this example has revealed is that it does not work with a trivial plan optimization.&lt;/p&gt;

&lt;p&gt;In our case, we’re just going to create a nonrelated index by running the following statement:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;CREATE INDEX IX_ProductID ON dbo.SalesOrderDetail(ProductID)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;What is significant about this is that, although the index created will not be used by our previous query, the query no longer qualifies for a trivial plan. Run the query again and observe the generated by now includes information about missing indexes.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- create the recommended index, after you provide a name for it, by running the following statement:
CREATE NONCLUSTERED INDEX IX_SalesOrderID_SalesOrderDetailID
ON [dbo].[SalesOrderDetail]([SalesOrderID], [SalesOrderDetailID]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you run our previous SELECT statement again and look at the execution plan, this time you’ll see an Index Seek operator using the index you’ve just created, and both the Missing Index warning and the MissingIndex element of the XML plan are gone.&lt;/p&gt;

&lt;p&gt;Finally, remove the dbo.SalesOrderDetail table you’ve just created by running the following statement:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;DROP TABLE dbo.SalesOrderDetail
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;index-fragmentation&#34;&gt;Index Fragmentation&lt;/h2&gt;

&lt;p&gt;Fragmentation happens when the logical order of pages in an index does not match the physical order in the data file. Because fragmentation can affect the performance of some queries, you need to monitor the fragmentation level of your indexes and, if required, perform reorganize or rebuild operations on them.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;It is also worth clarifying that fragmentation may affect only queries performing scans or range scans; queries performing index seeks may not be affected at all.&lt;/li&gt;
&lt;li&gt;The query optimizer does not consider fragmentation either, so the plans it produces will be the same whether you have high fragmentation or no fragmentation at all.&lt;/li&gt;
&lt;li&gt;You can use the &lt;strong&gt;sys.dm_db_index_physical_stats&lt;/strong&gt; DMF to analyze the fragmentation level of your indexes&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following example will return fragmentation information for the Sales.SalesOrderDetail of the AdventureWorks2012 database:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- the following example will return fragmentation information for the Sales.SalesOrderDetail table
SELECT a.index_id, name, avg_fragmentation_in_percent, fragment_count,
avg_fragment_size_in_pages
FROM sys.dm_db_index_physical_stats (DB_ID(&#39;AdventureWorks2012&#39;),
OBJECT_ID(&#39;Sales.SalesOrderDetail&#39;), NULL, NULL, NULL) AS a
JOIN sys.indexes AS b ON a.object_id = b.object_id AND a.index_id = b.index_id
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To rebuild all the indexes on the SalesOrderDetail table, use the following statement:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;ALTER INDEX ALL ON Sales.SalesOrderDetail REBUILD
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In case you need to reorganize the index, which is not the case here, you can use a command like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;ALTER INDEX ALL ON Sales.SalesOrderDetail REORGANIZE
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;rebuilding-indexes&#34;&gt;Rebuilding Indexes&lt;/h2&gt;

&lt;p&gt;Rebuilding an index drops and re-creates the index. This removes fragmentation, reclaims disk space by compacting the pages based on the specified or existing fill factor setting, and reorders the index rows in contiguous pages. When ALL is specified, all indexes on the table are dropped and rebuilt in a single transaction. FOREIGN KEY constraints do not have to be dropped in advance. When indexes with 128 extents or more are rebuilt, the Database Engine defers the actual page deallocations, and their associated locks, until after the transaction commits.&lt;/p&gt;

&lt;p&gt;Rebuilding or reorganizing small indexes often does not reduce fragmentation. The pages of small indexes are sometimes stored on mixed extents. Mixed extents are shared by up to eight objects, so the fragmentation in a small index might not be reduced after reorganizing or rebuilding it.&lt;/p&gt;

&lt;h3 id=&#34;rebuilding-an-index&#34;&gt;Rebuilding an index&lt;/h3&gt;

&lt;p&gt;The following example rebuilds a single index on the Employee table in the AdventureWorks2012 database.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;ALTER INDEX PK_Employee_EmployeeID ON HumanResources.Employee REBUILD;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;rebuilding-all-indexes-on-a-table-and-specifying-options&#34;&gt;Rebuilding all indexes on a table and specifying options&lt;/h3&gt;

&lt;p&gt;The following example specifies the keyword ALL. This rebuilds all indexes associated with the table Production.Product in the AdventureWorks2012 database. Three options are specified.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;ALTER INDEX ALL ON Production.Product
REBUILD WITH (FILLFACTOR = 80, SORT_IN_TEMPDB = ON, STATISTICS_NORECOMPUTE = ON);
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;setting-options-on-an-index&#34;&gt;Setting options on an index&lt;/h3&gt;

&lt;p&gt;The following example sets several options on the index AK_SalesOrderHeader_SalesOrderNumber in the AdventureWorks2012 database.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;ALTER INDEX AK_SalesOrderHeader_SalesOrderNumber ON
    Sales.SalesOrderHeader
SET (
    STATISTICS_NORECOMPUTE = ON,
    IGNORE_DUP_KEY = ON,
    ALLOW_PAGE_LOCKS = ON
    ) ;
GO
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;disabling-an-index&#34;&gt;Disabling an index&lt;/h3&gt;

&lt;p&gt;The following example disables a nonclustered index on the Employee table in the AdventureWorks2012 database.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;ALTER INDEX IX_Employee_ManagerID ON HumanResources.Employee DISABLE;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;reorganizing-indexes&#34;&gt;Reorganizing Indexes&lt;/h2&gt;

&lt;p&gt;Reorganizing an index uses minimal system resources. It defragments the leaf level of clustered and nonclustered indexes on tables and views by physically reordering the leaf-level pages to match the logical, left to right, order of the leaf nodes. Reorganizing also compacts the index pages. Compaction is based on the existing fill factor value.&lt;/p&gt;

&lt;h2 id=&#34;unused-indexes&#34;&gt;Unused Indexes&lt;/h2&gt;

&lt;p&gt;The &lt;strong&gt;sys.dm_db_index_usage_stats&lt;/strong&gt; DMV can be used to learn about the operations performed by your indexes. It is especially helpful in discovering indexes that are not used by any query, or are only minimally used.&lt;/p&gt;

&lt;p&gt;As an example, run the following code to create a new table with a nonclustered index:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;- as an example, run the following code to create a new table with a nonclustered index:
SELECT * INTO dbo.SalesOrderDetail
FROM Sales.SalesOrderDetail
CREATE NONCLUSTERED INDEX IX_ProductID ON dbo.SalesOrderDetail(ProductID)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When you run the following query, it will initially contain only one record, which was created because of table access performed when the IX_ProductID index was created:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;- when you run the following query, it will initially contain only one record
SELECT DB_NAME(database_id) AS database_name,
OBJECT_NAME(s.object_id) AS object_name, i.name, s.*
FROM sys.dm_db_index_usage_stats s JOIN sys.indexes i
ON s.object_id = i.object_id AND s.index_id = i.index_id
AND OBJECT_ID(&#39;dbo.SalesOrderDetail&#39;) = s.object_id
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However, the values that we will be inspecting in this exercise—user_seeks, user_scans, user_lookups, and user_updates—are all set to 0. Now run the following query, let’s say, three times:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT * FROM dbo.SalesOrderDetail
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This query is using a Table Scan operator, so, if you rerun our previous query using the sys.dm_db_index_usage_stats DMV, it will show the value 3 on the user_scans column.&lt;/p&gt;

&lt;p&gt;Run the next query, which uses an Index Seek, twice. After the query is executed, a new record will be added for the nonclustered index, and the user_seeks counter will show a value of 2.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- run the next query, which uses an Index Seek, twice
SELECT ProductID FROM dbo.SalesOrderDetail
WHERE ProductID = 773
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, run the following query four times, and it will use both Index Seek and RID Lookup operators. Because the user_seeks for the nonclustered index had a value of 2, it will be updated to 6, and the user_lookups value for the heap will be updated to 4.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- now, run the following query four times
SELECT * FROM dbo.SalesOrderDetail
WHERE ProductID = 773
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, run the following query once:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- finally, run the following query once:
UPDATE dbo.SalesOrderDetail
SET ProductID = 666
WHERE ProductID = 927
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that the UPDATE statement is doing an Index Seek and a Table Update, so user_seek will be updated for the index, and user_updates will be updated once for both the nonclustered index and the heap.&lt;/p&gt;

&lt;p&gt;Finally, drop the table you just created:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;DROP TABLE dbo.SalesOrderDetail
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;non-clustered-indexes-clustered-vs-non-clustered-indexes-advanced-indexing-techniques&#34;&gt;Non-clustered Indexes Clustered vs. Non-clustered Indexes Advanced Indexing Techniques&lt;/h3&gt;

&lt;p&gt;A nonclustered index does not affect the order of the data in the table pages because the leaf pages of a nonclustered index and the data pages of the table are separate.&lt;/p&gt;

&lt;p&gt;If all the columns required by the query are available in the index itself, then access to the data page is not required. This is known as a covering index.&lt;/p&gt;

&lt;h3 id=&#34;when-not-to-use-a-nonclustered-index&#34;&gt;When Not to Use a Nonclustered Index&lt;/h3&gt;

&lt;p&gt;Nonclustered indexes are not suitable for queries that retrieve a large number of rows. Such queries are better served with a clustered index, which doesn’t require a separate lookup to retrieve a data row.&lt;/p&gt;

&lt;h3 id=&#34;when-to-use-a-nonclustered-index&#34;&gt;When to Use a Nonclustered Index&lt;/h3&gt;

&lt;p&gt;A nonclustered index is most useful when all you want to do is retrieve a small number of rows and columns from a large table. As the number of columns to be retrieved increases, the ability to have a covering index decreases.&lt;/p&gt;

&lt;h3 id=&#34;relationship-with-nonclustered-indexes&#34;&gt;Relationship with Nonclustered Indexes&lt;/h3&gt;

&lt;p&gt;An index row of a nonclustered index contains a pointer to the corresponding data row of the table. This pointer is called a row locator. The value of the row locator depends on whether the data pages are stored in a heap or on a clustered index. For a nonclustered index, the row locator is a pointer to the row identifier (RID) for the data row in a heap. For a table with a clustered index, the row locator is the clustered index key value.&lt;/p&gt;

&lt;h2 id=&#34;indexed-views&#34;&gt;Indexed Views&lt;/h2&gt;

&lt;p&gt;A database view can be materialized on the disk by creating a unique clustered index on the view. Such a view is referred to as an indexed view or a materialized view. After a unique clustered index is created on the view, the view’s result set is materialized immediately and persisted in physical storage in the database, saving the overhead of performing costly operations during query execution. After the view is materialized, multiple nonclustered indexes can be created on the indexed view. Effectively, this turns a view (again, just a query) into a real table with defined storage.&lt;/p&gt;

&lt;h3 id=&#34;benefit&#34;&gt;Benefit&lt;/h3&gt;

&lt;p&gt;You can use an indexed view to increase the performance of a query in the following ways:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Aggregations can be precomputed and stored in the indexed view to minimize expensive computations during query execution.&lt;/li&gt;
&lt;li&gt;Tables can be prejoined, and the resulting data set can be materialized.&lt;/li&gt;
&lt;li&gt;Combinations of joins or aggregations can be materialized&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;overhead&#34;&gt;Overhead&lt;/h3&gt;

&lt;p&gt;Indexed views can produce major overhead on an OLTP database. Some of the overheads of indexed views are as follows:&lt;/p&gt;

&lt;p&gt;Any change in the base tables has to be reflected in the indexed view by executing the view’s SELECT statement.
Any changes to a base table on which an indexed view is defined may initiate one or more changes in the nonclustered indexes of the indexed view. The clustered index will also have to be changed if the clustering key is updated.
The indexed view adds to the ongoing maintenance overhead of the database.
Additional storage is required in the database.&lt;/p&gt;

&lt;h3 id=&#34;usage-scenarios&#34;&gt;Usage Scenarios&lt;/h3&gt;

&lt;p&gt;Reporting systems benefit the most from indexed views. OLTP systems with frequent writes may not be able to take advantage of the indexed views because of the increased maintenance cost associated with updating both the view and the underlying base tables. The net performance improvement provided by an indexed view is the difference between the total query execution savings offered by the view and the cost of storing and maintaining the view.&lt;/p&gt;

&lt;h2 id=&#34;index-compression&#34;&gt;Index Compression&lt;/h2&gt;

&lt;p&gt;Compressing an index means getting more key information onto a single page. This can lead to significant performance improvements because fewer pages and fewer index levels are needed to store the index. There will be overhead in the CPU as the key values in the index are compressed and decompressed, so this may not be a solution for all indexes. Memory benefits also because the compressed pages are stored in memory in a compressed state.&lt;/p&gt;

&lt;p&gt;By default, an index will be not be compressed. You have to explicitly call for the index to be compressed when you create the index. There are two types of compression: row- and page-level compression.&lt;/p&gt;

&lt;p&gt;e.g we can compress an index as follows :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;CREATE NONCLUSTERED INDEX IX_Comp_Test
ON Person.Address (City,PostalCode)
WITH (DATA_COMPRESSION = ROW);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and with page compression as follows :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;CREATE NONCLUSTERED INDEX IX_Comp_Page_Test
ON Person.Address  (City,PostalCode)
WITH (DATA_COMPRESSION = PAGE);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can check the compressed pages in an index as follows :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT  i.Name,
        i.type_desc,
        s.page_count,
        s.record_count,
        s.index_level,
        compressed_page_count
FROM    sys.indexes i
        JOIN sys.dm_db_index_physical_stats(DB_ID(N&#39;AdventureWorks2012&#39;),
                                            OBJECT_ID(N&#39;Person.Address&#39;),NULL,
                                            NULL,&#39;DETAILED&#39;) AS s
        ON i.index_id = s.index_id
WHERE   i.OBJECT_ID = OBJECT_ID(N&#39;Person.Address&#39;);
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;columnstore-indexes&#34;&gt;Columnstore Indexes&lt;/h2&gt;

&lt;p&gt;Introduced in SQL Server 2012, the columnstore index is used to index information by columns rather than by rows. This is especially useful when working within data warehousing systems where large amounts of data have to be aggregated and accessed quickly.&lt;/p&gt;

&lt;h3 id=&#34;index-overhead&#34;&gt;Index Overhead&lt;/h3&gt;

&lt;p&gt;The performance benefit of indexes does come at a cost. Tables with indexes require more storage and memory space for the index pages in addition to the data pages of the table. Data manipulation queries (INSERT, UPDATE, and DELETE statements, or the CUD part of Create, Read, Update, Delete [CRUD]) can take longer, and more processing time is required to maintain the indexes of constantly changing tables. When you have to deal with the existing system, you should ensure that the performance benefits of an index outweigh the extra cost in processing resources&lt;/p&gt;

&lt;h2 id=&#34;index-design-recommendations&#34;&gt;Index Design Recommendations&lt;/h2&gt;

&lt;p&gt;The main recommendations for index design are as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Examine the WHERE clause and JOIN criteria columns.&lt;/li&gt;
&lt;li&gt;Use narrow indexes.&lt;/li&gt;
&lt;li&gt;Examine column uniqueness.&lt;/li&gt;
&lt;li&gt;Examine the column data type.&lt;/li&gt;
&lt;li&gt;Consider column order.&lt;/li&gt;
&lt;li&gt;Consider the type of index (clustered versus nonclustered)s&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/en-us/sql/t-sql/statements/alter-index-transact-sql&#34;&gt;https://docs.microsoft.com/en-us/sql/t-sql/statements/alter-index-transact-sql&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/en-us/sql/t-sql/statements/create-index-transact-sql&#34;&gt;https://docs.microsoft.com/en-us/sql/t-sql/statements/create-index-transact-sql&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/en-us/sql/t-sql/statements/drop-index-transact-sql&#34;&gt;https://docs.microsoft.com/en-us/sql/t-sql/statements/drop-index-transact-sql&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Datase Tuning Advisor</title>
      <link>http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/database-tuning-advisor/</link>
      <pubDate>Sat, 18 Mar 2017 14:52:22 +0200</pubDate>
      
      <guid>http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/database-tuning-advisor/</guid>
      <description>

&lt;h2 id=&#34;database-engine-tuning-advisor-mechanisms&#34;&gt;Database Engine Tuning Advisor Mechanisms&lt;/h2&gt;

&lt;p&gt;This tool can help identify an optimal set of indexes and statistics for a given workload without requiring an expert understanding of the database schema, workload, or SQL Server internals.&lt;/p&gt;

&lt;h3 id=&#34;running-dta&#34;&gt;Running DTA&lt;/h3&gt;

&lt;p&gt;You can run the DTA from the command line or within SQL Server Management Studio.&lt;/p&gt;

&lt;h3 id=&#34;running-from-the-command-line&#34;&gt;Running from the command line&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Click &lt;strong&gt;Start&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Type &lt;strong&gt;run&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Type &lt;strong&gt;cmd&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;From the cmd you now access the &lt;strong&gt;dta&lt;/strong&gt; command line tool&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;running-from-sql-server-management-studio&#34;&gt;Running from SQL Server Management Studio&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Click &lt;strong&gt;Tools&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Choose &lt;strong&gt;Database Tuning Advisor&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;You can also tune a specific query from SMSS by right-clicking and choosing &lt;strong&gt;Database Tuning Advisor&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/database-tuning-advisor.png&#34; alt=&#34;Database Tuning Advisor&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;database-engine-tuning-advisor-examples&#34;&gt;Database Engine Tuning Advisor Examples&lt;/h2&gt;

&lt;p&gt;Tuning a Query&lt;/p&gt;

&lt;p&gt;You can use the Database Engine Tuning Advisor to recommend indexes for a complete database by using a workload that fairly represents all SQL activities. You can also use it to recommend indexes for a set of problematic queries.&lt;/p&gt;

&lt;p&gt;To learn how you can use the Database Engine Tuning Advisor to get index recommendations on a set of problematic queries, say you have a simple query that is called rather frequently. Because of the frequency, you want a quick turnaround for some tuning. This is the query:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT  soh.DueDate,
        soh.CustomerID,
        soh.Status
FROM    Sales.SalesOrderHeader AS soh
WHERE   soh.DueDate BETWEEN &#39;1/1/2008&#39; AND &#39;2/1/2008&#39;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To analyze the query, right-click it in the query window and select Analyze Query in the Database Engine Tuning Advisor.&lt;/p&gt;

&lt;h2 id=&#34;tuning-a-workload-using-the-plan-cache&#34;&gt;Tuning a Workload Using the Plan Cache&lt;/h2&gt;

&lt;p&gt;You can also specify the plan cache as a workload to tune.  In this case, the DTA will select the top 1,000 events from the plan cache based on total elapsed time of the query (that is, based on the total_elapsed_time column of the &lt;strong&gt;sys.dm_exec_query_stats&lt;/strong&gt; DMV)&lt;/p&gt;

&lt;p&gt;Let’s try an example, and to make it easy to see the results, let’s clear the plan cache and run only one query in Management Studio:s&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt; -- let’s clear the plan cache and run only one query in Management Studio:
DBCC FREEPROCCACHE
GO
SELECT SalesOrderID, OrderQty, ProductID
FROM dbo.SalesOrderDetail
WHERE CarrierTrackingNumber = &#39;D609-4F2A-9B&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After the query is executed, most likely, it will be kept in the plan cache. Open a new DTA session. In the Workload option, select Plan Cache and specify AdventureWorks2012 as both the database to tune and the database for workload analysis. Click the Start Analysis button. After the analysis is completed, you can select the Recommendations tab and select Index Recommendations, which will include the following recommendations (which you can see by looking at the Definition column):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;-- after the analysis is completed, you can select the Recommendations tab
CREATE NONCLUSTERED INDEX [_dta_index_SalesOrderDetail_5_807673925__K3_1_4_5]
ON [dbo].[SalesOrderDetail]
(
[CarrierTrackingNumber] ASC
)
INCLUDE ([SalesOrderID],
[OrderQty],
[ProductID]) WITH (SORT_IN_TEMPDB = OFF, DROP_EXISTING = OFF, ONLINE = OFF)
ON [PRIMARY]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- finally, drop the table you just created by running the following statement:
DROP TABLE dbo.SalesOrderDetail
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;offload-of-tuning-overhead-to-test-server&#34;&gt;Offload of Tuning Overhead to Test Server&lt;/h2&gt;

&lt;p&gt;One of the most interesting and perhaps less known features of the DTA is that you can use it with a test server to tune the workload of a production server.&lt;/p&gt;

&lt;p&gt;The DTA can gather the database metadata and statistics from the production server and use it to create a similar database, with no data, on a different server. This database is called a shell database.&lt;/p&gt;

&lt;h2 id=&#34;database-engine-tuning-advisor-limitations&#34;&gt;Database Engine Tuning Advisor Limitations&lt;/h2&gt;

&lt;p&gt;The Database Engine Tuning Advisor recommendations are based on the input workload. If the input workload is not a true representation of the actual workload, then the recommended indexes may sometimes have a negative effect on some queries that are missing in the workload.&lt;/p&gt;

&lt;p&gt;But most importantly, in many cases, the Database Engine Tuning Advisor may not recognize possible tuning opportunities. It has a sophisticated testing engine, but in some scenarios, its capabilities are limited&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Boomark Lookup Analysis</title>
      <link>http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/bookmark-lookup-analysis/</link>
      <pubDate>Sat, 18 Mar 2017 14:52:36 +0200</pubDate>
      
      <guid>http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/bookmark-lookup-analysis/</guid>
      <description>

&lt;h2 id=&#34;purpose-of-bookmark-lookups&#34;&gt;Purpose of Bookmark Lookups&lt;/h2&gt;

&lt;p&gt;A major overhead associated with nonclustered indexes is the cost of excessive lookups, formerly known as bookmark lookups, which are a mechanism to navigate from a nonclustered index row to the corresponding data row in the clustered index or the heap.&lt;/p&gt;

&lt;p&gt;When a SQL query requests information through a query, the optimizer can use a nonclustered index, if available, on the columns in the WHERE or JOIN clause to retrieve the data. If the query refers to columns that are not part of the nonclustered index being used to retrieve the data, then navigation is required from the index row to the corresponding data row in the table to access these remaining columns.&lt;/p&gt;

&lt;p&gt;For example, in the following SELECT statement, if the nonclustered index used by the optimizer doesn’t include all the columns, navigation will be required from a nonclustered index row to the data row in the clustered index or heap to retrieve the value of those columns:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT  p.[Name],
        AVG(sod.LineTotal)
FROM    Sales.SalesOrderDetail AS sod
        JOIN Production.Product p
        ON sod.ProductID = p.ProductID
WHERE   sod.ProductID = 776
GROUP BY sod.CarrierTrackingNumber,
        p.[Name]
HAVING  MAX(sod.OrderQty) &amp;gt; 1
ORDER BY MIN(sod.LineTotal);
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT  *
FROM    Sales.SalesOrderDetail AS sod
WHERE   sod.ProductID = 776 ;

&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;drawbacks-of-bookmark-lookups&#34;&gt;Drawbacks of Bookmark Lookups&lt;/h2&gt;

&lt;p&gt;A lookup requires data page access in addition to index page access. Accessing two sets of pages increases the number of logical reads for the query. Additionally, if the pages are not available in memory, a lookup will probably require a random (or nonsequential) I/O operation on the disk to jump from the index page to the data page as well as requiring the necessary CPU power to marshal this data and perform the necessary operations. This is because, for a large table, the index page and the corresponding data page usually won’t be directly next to each other on the disk.&lt;/p&gt;

&lt;h2 id=&#34;analyzing-the-cause-of-a-bookmark-lookup&#34;&gt;Analyzing the Cause of a Bookmark Lookup&lt;/h2&gt;

&lt;p&gt;Since a lookup can be a costly operation, you should analyze what causes a query plan to choose a lookup step in an execution plan. You may find that you are able to avoid the lookup by including the missing columns in the nonclustered index key or as INCLUDE columns at the index page level and thereby avoid the cost overhead associated with the lookup.&lt;/p&gt;

&lt;p&gt;To learn how to identify the columns not included in the nonclustered index, consider the following query, which pulls information from the HumanResources.Employee table based on NationalIDNumber:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT  NationalIDNumber,
        JobTitle,
        HireDate
FROM    HumanResources.Employee AS e
WHERE   e.NationalIDNumber = &#39;693168613&#39; ;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/boomark-lookup-column-properties.png&#34; alt=&#34;Book Lookup Columns&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The SELECT statement refers to columns NationalIDNumber, JobTitle, and HireDate. The nonclustered index on column NationalIDNumber doesn’t provide values for columns JobTitle and HireDate, so a lookup operation was required to retrieve those columns from the data storage location.&lt;/p&gt;

&lt;p&gt;If you look at the properties on the Key Lookup (Clustered) operation, you can see the output list for the operation. This shows you the columns being output by the lookup. To get the list of output columns quickly and easily and be able to copy them, right-click the operator, which in this case is Key Lookup (Clustered).&lt;/p&gt;

&lt;h2 id=&#34;resolving-bookmark-lookups&#34;&gt;Resolving Bookmark Lookups&lt;/h2&gt;

&lt;p&gt;Since the relative cost of a lookup can be high, you should, wherever possible, try to get rid of lookup operations.&lt;/p&gt;

&lt;h3 id=&#34;using-a-clustered-index&#34;&gt;Using a Clustered Index&lt;/h3&gt;

&lt;p&gt;For a clustered index, the leaf page of the index is the same as the data page of the table. Therefore, when reading the values of the clustered index key columns, the database engine can also read the values of other columns without any navigation from the index row.&lt;/p&gt;

&lt;h3 id=&#34;using-a-covering-index&#34;&gt;Using a Covering Index&lt;/h3&gt;

&lt;p&gt;To understand how you can use a covering index to avoid a lookup, examine the query against the HumanResources.Employee table again.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT  NationalIDNumber,
        JobTitle,
        HireDate
FROM    HumanResources.Employee AS e
WHERE   e.NationalIDNumber = &#39;693168613&#39;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To avoid this bookmark, you can add the columns referred to in the query, JobTitle and HireDate, directly to the nonclustered index key. This will make the nonclustered index a covering index for this query because all columns can be retrieved from the index without having to go to the heap or clustered index.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;CREATE UNIQUE NONCLUSTERED INDEX [AK_Employee_NationalIDNumber] ON
[HumanResources].[Employee]
(NationalIDNumber ASC,
JobTitle ASC,
HireDate ASC )
WITH DROP_EXISTING;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are a couple of caveats to creating a covering index by changing the key, however. If you add too many columns to a nonclustered index, it becomes wider. The index maintenance cost associated with the action queries can increase.&lt;/p&gt;

&lt;p&gt;Another way to arrive at the covering index, without reshaping the index by adding key columns, is to use the INCLUDE columns. Change the index to look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;CREATE UNIQUE NONCLUSTERED INDEX [AK_Employee_NationalIDNumber]
ON [HumanResources].[Employee]
(NationalIDNumber ASC)
INCLUDE  (JobTitle,HireDate)
WITH DROP_EXISTING ;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Because the data is stored at the leaf level of the index, when the index is used to retrieve the key values, the rest of the columns in the INCLUDE statement are available for use, almost like they were part of the key&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Statistics Analysis</title>
      <link>http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/statistics-analysis/</link>
      <pubDate>Sat, 18 Mar 2017 14:53:03 +0200</pubDate>
      
      <guid>http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/statistics-analysis/</guid>
      <description>

&lt;p&gt;The SQL Server optimizer is cost based and its the statistics available to the optimizer that it considers when searching for an optimal query plan. The optimizer must have information about the data that defines an index or a column. That information is referred to as a statistic. Statistics define both the distribution of data and the uniqueness or selectivity of the data. Statistics are maintained both on indexes and on columns within the system&lt;/p&gt;

&lt;h2 id=&#34;the-role-of-statistics-in-query-optimization&#34;&gt;The Role of Statistics in Query Optimization&lt;/h2&gt;

&lt;p&gt;SQL Server’s query optimizer is a cost-based optimizer; it decides on the best data access mechanism and join strategy by identifying the selectivity, how unique the data is, and which columns are used in filtering the data (meaning via the WHERE or JOIN clause). Statistics exist with an index, but they also exist on columns without an index that are used as part of a predicate. As long as you ensure that the default statistical settings for the database are set, the optimizer will be able to do its best to determine effective processing strategies dynamically.&lt;/p&gt;

&lt;h2 id=&#34;statistics-on-an-indexed-column&#34;&gt;Statistics on an Indexed Column&lt;/h2&gt;

&lt;p&gt;The usefulness of an index is largely dependent on the statistics of the indexed columns; without statistics, SQL Server’s cost-based query optimizer can’t decide upon the most effective way of using an index. To meet this requirement, SQL Server automatically creates the statistics of an index key whenever the index is created. It isn’t possible to turn this feature off.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/statistics-database-configuration.png&#34; alt=&#34;Statistics Configuration&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;statistics-on-a-nonindexed-column&#34;&gt;Statistics on a Nonindexed Column&lt;/h2&gt;

&lt;p&gt;Sometimes you may have columns in join or filter criteria without any index. Even for such nonindexed columns, the query optimizer is more likely to make the best choice if it knows the cardinality and data distribution, also known as the statistics, of those columns.&lt;/p&gt;

&lt;p&gt;In addition to statistics on indexes, SQL Server can build statistics on columns with no indexes. The information on data distribution, or the likelihood of a particular value occurring in a nonindexed column, can help the query optimizer determine an optimal processing strategy.&lt;/p&gt;

&lt;h2 id=&#34;analyzing-statistics&#34;&gt;Analyzing Statistics&lt;/h2&gt;

&lt;p&gt;We use the DBCC command to show the statistics of columns and indexes as follows :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;DBCC SHOW_STATISTICS(&#39;Person.Person&#39;, &#39;IX_Person_LastName_FirstName_MiddleName&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;to show the statistics on the index &lt;code&gt;IX_Person_LastName_FirstName_MiddleName&lt;/code&gt; on the &lt;code&gt;Person.Person&lt;/code&gt; table.&lt;/p&gt;

&lt;p&gt;These steps consist of varying size intervals between the 200 values stored. A step provides the following information:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The top value of a given step (RANGE_HI_KEY)&lt;/li&gt;
&lt;li&gt;The number of rows equal to RANGE_HI_KEY (EQ_ROWS)&lt;/li&gt;
&lt;li&gt;The number of rows between the previous top value and the current top value, without counting either of these boundary points (RANGE_ROWS)&lt;/li&gt;
&lt;li&gt;The number of distinct values in the range (DISTINCT_RANGE_ROWS); if all values in the range are unique, then RANGE_ROWS equals DISTINCT_RANGE_ROWS&lt;/li&gt;
&lt;li&gt;The average number of rows equal to any potential key value within a range (AVG_RANGE_ROWS)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For example, when referencing an index, the value of AVG_RANGE_ROWS for a key value within a step in the histogram helps the optimizer decide how (and whether) to use the index when the indexed column is referred to in a WHERE clause. Because the optimizer can perform a SEEK or SCAN operation to retrieve rows from a table, the optimizer can decide which operation to perform based on the number of potential matching rows for the index key value. This can be even more precise when referencing the RANGE_HI_KEY since the optimizer can know that it should find a fairly precise number of rows from that value (assuming the statistics are up-to-date).&lt;/p&gt;

&lt;p&gt;Showing statistics on an a table as follows :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT  s.name,
        s.auto_created,
        s.user_created,
        s.filter_definition,
        sc.column_id,
        c.name AS ColumnName
FROM    sys.stats AS s
        JOIN sys.stats_columns AS sc ON sc.stats_id = s.stats_id
AND sc.object_id = s.object_id
        JOIN sys.columns AS c ON c.column_id = sc.column_id
AND c.object_id = s.object_id
WHERE   s.object_id = OBJECT_ID(&#39;Production.Product&#39;);
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;inspecting-statistics-objects&#34;&gt;Inspecting Statistics Objects&lt;/h2&gt;

&lt;p&gt;Existing statistics for a specific object can be displayed using the sys.stats catalog view, as used in the following query:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- existing statistics for a specific object can be displayed using the sys.stats catalog view, as used in the following query:
SELECT * FROM sys.stats
WHERE object_id = OBJECT_ID(&#39;Sales.SalesOrderDetail&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For example, run the following statement to verify that there are no statistics on the UnitPrice column of the Sales.SalesOrderDetail table:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;DBCC SHOW_STATISTICS (‘Sales.SalesOrderDetail’, UnitPrice)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;By then running the following query, the query optimizer will automatically create statistics on the UnitPrice column, which is used in the query predicate:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- by then running the following query, the query optimizer will automatically create statistics on the UnitPrice column
SELECT * FROM Sales.SalesOrderDetail
WHERE UnitPrice = 35
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/histogram.png&#34; alt=&#34;Statistics Histogram&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Running the previous &lt;strong&gt;DBCC SHOW_STATISTICS&lt;/strong&gt; statement again will now show a statistics object. The output is separated into three result sets called the header, the density vector, and the histogram.&lt;/p&gt;

&lt;h2 id=&#34;density&#34;&gt;Density&lt;/h2&gt;

&lt;p&gt;To better explain the density vector, run the following statement to inspect the statistics of the existing index, IX_SalesOrderDetail_ProductID:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;DBCC SHOW_STATISTICS (’Sales.SalesOrderDetail’, IX_SalesOrderDetail_ProductID)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Density, which is defined as 1 / “number of distinct values,” is listed in the All density field, and it is calculated for each set of columns, forming a prefix for the columns in the statistics object.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/density.png&#34; alt=&#34;Density&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Density information can be used to improve the query optimizer’s estimates for GROUP BY operations, and on equality predicates where a value is unknown, as in the case of a query using local variables.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;--  in this case, we have 1 / 0.003759399, which gives us 266, which is the estimated number of rows shown in the plan
SELECT ProductID FROM Sales.SalesOrderDetail
GROUP BY ProductID
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next is an example of how the density can be used to estimate the cardinality of a query using local variables:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- next is an example of how the density can be used to estimate the cardinality of a query using local variables:
DECLARE @ProductID int
SET @ProductID = 921
SELECT ProductID FROM Sales.SalesOrderDetail
WHERE ProductID = @ProductID
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/density-group-by.png&#34; alt=&#34;Density in Group By Clause&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In this case, the query optimizer does not know the value of the @ProductID local variable at optimization time, so it is not able to use the histogram. The estimated number of rows is obtained using the density multiplied by the number of records in the table, which in our example is 0.003759399 * 121317, or 456.079.&lt;/p&gt;

&lt;p&gt;Actually, because the query optimizer does not know the value of @ProductID at optimization time, the value 921 in the previous listing does not matter; any other value will give exactly the same estimated number of rows and execution plan, this being the average number of rows per value. Finally, run this query with an inequality operator:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- finally, run this query with an inequality operator:
DECLARE @pid int = 897
SELECT * FROM Sales.SalesOrderDetail
WHERE ProductID &amp;lt; @pid
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Just as before, the value 897 does not matter; any other value will give you the same estimated number of rows and execution plan. However, this time, the query optimizer is not able to use the density information and instead is using the standard guess of 30 percent selectivity for inequality comparisons. This means that the estimated number of rows is always 30 percent of the total number of records for an inequality operator; in this case, 30 percent of 121,317 is 36,395.1&lt;/p&gt;

&lt;h2 id=&#34;cardinality-estimation-errors&#34;&gt;Cardinality Estimation Errors&lt;/h2&gt;

&lt;p&gt;Cardinality estimation errors can lead to the query optimizer making poor choices as to how best to execute a query and, therefore, to badly performing execution plans.  In the next query, I show you how to use the SET STATISTICS PROFILE statement with one of our previous examples, where SQL Server is making a blind guess regarding the selectivity of certain columns:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- in the next query, I show you how to use the SET STATISTICS PROFILE statement
SET STATISTICS PROFILE ON
GO
SELECT * FROM Sales.SalesOrderDetail
WHERE OrderQty * UnitPrice &amp;gt; 10000
GO
SET STATISTICS PROFILE OFF
GO
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Using this output, you can easily compare the actual number of rows, shown on the Rows column, against the estimated number of records, shown on the EstimateRows column, for each operator in the plan.&lt;/p&gt;

&lt;p&gt;Introduced with SQL Server 2012, the &lt;strong&gt;inaccurate_cardinality_estimate&lt;/strong&gt; extended event can also be used to detect inaccurate cardinality estimations by identifying which query operators output significantly more rows than those estimated by the query optimizer.&lt;/p&gt;

&lt;h3 id=&#34;recommendations&#34;&gt;Recommendations&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;auto-create and auto-update statistics default configurations&lt;/li&gt;
&lt;li&gt;updating statistics using WITH FULLSCAN&lt;/li&gt;
&lt;li&gt;avoiding local variables in queries&lt;/li&gt;
&lt;li&gt;avoiding non-constant-foldable or complex expressions on predicates using computed columns, and considering multicolumn or filtered statistics&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;statistics-on-computed-columns&#34;&gt;Statistics on Computed Columns&lt;/h2&gt;

&lt;p&gt;SQL Server can automatically create and update statistics on computed columns.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- to see an example, run this query
SELECT * FROM Sales.SalesOrderDetail
WHERE OrderQty * UnitPrice &amp;gt; 10000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The estimated number of rows is 36,395.1, which is 30 percent of the total number of rows (121,317), although the query returns only 772 records. SQL Server is obviously using a selectivity guess because it cannot estimate the selectivity of the expression OrderQty * UnitPrice &amp;gt; 10000.&lt;/p&gt;

&lt;p&gt;Now create a computed column:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- now create a computed column:
ALTER TABLE Sales.SalesOrderDetail
ADD cc AS OrderQty * UnitPrice
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Run the previous SELECT statement again, and note that, this time, the estimated number of rows has changed and is close to the actual number of rows returned by the query.&lt;/p&gt;

&lt;p&gt;Note that creating the computed column does not create statistics; these statistics are created the first time the query is optimized, and you can run the next query to display the information about the statistics objects for the Sales.SalesOrderDetail table:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- note that creating the computed column does not create statistics
SELECT * FROM sys.stats
WHERE object_id = OBJECT_ID(&#39;Sales.SalesOrderDetail&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The newly created statistics object will most likely be at the end of the list. Copy the name of the object, and use the following command to display the details about the statistics object&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- use the following command to display the details about the statistics object
DBCC SHOW_STATISTICS (&#39;Sales.SalesOrderDetail&#39;, _WA_Sys_0000000E_44CA3770)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Unfortunately, for automatic matching to work, the expression must be exactly the same as the computed column definition. So, if I change the query to UnitPrice * OrderQty, instead of OrderQty * UnitPrice, the execution plan will show an estimated number of rows of 30 percent again, as this query will demonstrate:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- unfortunately, for automatic matching to work, the expression must be exactly the same as the computed column definition
SELECT * FROM Sales.SalesOrderDetail
WHERE UnitPrice * OrderQty &amp;gt; 10000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, drop the created computed column:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- finally, drop the created computed column:
ALTER TABLE Sales.SalesOrderDetail
DROP COLUMN cc
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;statistics-maintenance&#34;&gt;Statistics Maintenance&lt;/h2&gt;

&lt;p&gt;SQL Server allows a user to manually override the maintenance of statistics in an individual database. The four main configurations controlling the automatic statistics maintenance behavior of SQL Server are as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;New statistics on columns with no index (auto create statistics)&lt;/li&gt;
&lt;li&gt;Updating existing statistics (auto update statistics)&lt;/li&gt;
&lt;li&gt;The degree of sampling used to generate statistics&lt;/li&gt;
&lt;li&gt;Asynchronous updating of existing statistics (auto update statistics async)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you want to check the status of whether a table has its automatic statistics turned off, you can use this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;EXEC sp_autostats &#39;HumanResources.Department&#39;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Reset the automatic maintenance of the index so that it is on where it has been turned off.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;EXEC sp_autostats
    &#39;HumanResources.Department&#39;,
    &#39;ON&#39;;
EXEC sp_autostats
    &#39;HumanResources.Department&#39;,
    &#39;ON&#39;,
    AK_Department_Name;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;analyzing-the-effectiveness-of-statistics&#34;&gt;Analyzing the Effectiveness of Statistics&lt;/h2&gt;

&lt;p&gt;You can verify the current settings for the autostats feature using the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;sys.databases&lt;/li&gt;
&lt;li&gt;DATABASEPROPERTYEX&lt;/li&gt;
&lt;li&gt;sp_autostats&lt;/li&gt;
&lt;li&gt;Status of Auto Create Statistics&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can verify the current setting for auto create statistics by running a query against the sys.databases system table.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT  is_auto_create_stats_on
FROM    sys.databases
WHERE   [name] = &#39;AdventureWorks2012&#39;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A return value of 1 means enabled, and a value of 0 means disabled.&lt;/p&gt;

&lt;p&gt;You can also verify the status of specific indexes using the sp_autostats system stored procedure, as shown in the following code.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;USE AdventureWorks2012;
EXEC sys.sp_autostats
    &#39;HumanResources.Department&#39;;
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Index Fragmentation Analysis</title>
      <link>http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/index-fragmentation-analysis/</link>
      <pubDate>Sat, 18 Mar 2017 14:53:22 +0200</pubDate>
      
      <guid>http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/index-fragmentation-analysis/</guid>
      <description>

&lt;h2 id=&#34;causes-of-fragmentation&#34;&gt;Causes of Fragmentation&lt;/h2&gt;

&lt;p&gt;Fragmentation occurs when data is modified in a table. When you insert or update data in a table (via INSERT or UPDATE), the table’s corresponding clustered indexes and the affected nonclustered indexes are modified. This can cause an index leaf page split if the modification to an index can’t be accommodated in the same page. A new leaf page will then be added that contains part of the original page and maintains the logical order of the rows in the index key. Although the new leaf page maintains the logical order of the data rows in the original page, this new page usually won’t be physically adjacent to the original page on the disk. Or, put a slightly different way, the logical key order of the index doesn’t match the physical order within the file.&lt;/p&gt;

&lt;p&gt;SQL Server 2014 exposes the leaf and nonleaf pages and other data through a dynamic management view called sys.dm_db_index_physical_stats. It stores both the index size and the fragmentation.&lt;/p&gt;

&lt;h2 id=&#34;fragmentation-overhead&#34;&gt;Fragmentation Overhead&lt;/h2&gt;

&lt;p&gt;Both internal and external fragmentations adversely affect data retrieval performance. External fragmentation causes a noncontiguous sequence of index pages on the disk, with new leaf pages far from the original leaf pages and with their physical ordering different from their logical ordering.&lt;/p&gt;

&lt;p&gt;For better performance, it is preferable to use sequential I/O, since this can read a whole extent (eight 8KB pages together) in a single disk I/O operation. By contrast, a noncontiguous layout of pages requires nonsequential or random I/O operations to retrieve the index pages from the disk, and a random I/O operation can read only 8KB of data in a single disk operation (this may be acceptable, however, if you are retrieving only one row).&lt;/p&gt;

&lt;h2 id=&#34;analyzing-the-amount-of-fragmentation&#34;&gt;Analyzing the Amount of Fragmentation&lt;/h2&gt;

&lt;p&gt;You can analyze the fragmentation ratio of an index by using the &lt;code&gt;sys.dm_db_index_physical_ stats&lt;/code&gt; dynamic management function.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT  ddips.avg_fragmentation_in_percent,
        ddips.fragment_count,
        ddips.page_count,
        ddips.avg_page_space_used_in_percent,
        ddips.record_count,
        ddips.avg_record_size_in_bytes
FROM    sys.dm_db_index_physical_stats(DB_ID(&#39;AdventureWorks2012&#39;),
                                       OBJECT_ID(N&#39;Person.Person&#39;),NULL,
                            NULL,&#39;Sampled&#39;) AS ddips;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;fragmentation-resolutions&#34;&gt;Fragmentation Resolutions&lt;/h2&gt;

&lt;p&gt;You can resolve fragmentation in an index by rearranging the index rows and pages so that their physical and logical orders match. To reduce external fragmentation, you can physically reorder the leaf pages of the index to follow the logical order of the index. You achieve this through the following techniques:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Dropping and re-creating the index&lt;/li&gt;
&lt;li&gt;Re-creating the index with the DROP_EXISTING = ON clause&lt;/li&gt;
&lt;li&gt;Executing the ALTER INDEX REBUILD statement on the index&lt;/li&gt;
&lt;li&gt;Executing the ALTER INDEX REORGANIZE statement on the index&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;significance-of-the-fill-factor&#34;&gt;Significance of the Fill Factor&lt;/h2&gt;

&lt;p&gt;The internal fragmentation of an index is reduced by getting more rows per leaf page in an index. Getting more rows within a leaf page reduces the total number of pages required for the index and in turn decreases disk I/O and the logical reads required to retrieve a range of index rows. On the other hand, if the index key values are highly transactional, then having fully used index pages will cause page splits. Therefore, for a transactional table, a good balance between maximizing the number of rows in a page and avoiding page splits is required&lt;/p&gt;

&lt;p&gt;SQL Server allows you to control the amount of free space within the leaf pages of the index by using the fill factor. If you know that there will be enough INSERT queries on the table or UPDATE queries on the index key columns, then you can pre-add free space to the index leaf page using the fill factor to minimize page splits. If the table is read-only, you can create the index with a high fill factor to reduce the number of index pages.&lt;/p&gt;

&lt;p&gt;The default fill factor is 0, which means the leaf pages are packed to 100 percent, although some free space is left in the branch nodes of the B-tree structure.&lt;/p&gt;

&lt;h2 id=&#34;automatic-maintenance&#34;&gt;Automatic Maintenance&lt;/h2&gt;

&lt;p&gt;In a database with a great deal of transactions, tables and indexes become fragmented over time. Thus, to improve performance, you should check the fragmentation of the tables and indexes regularly, and you should defragment the ones with a high amount of fragmentation. You also may need to take into account the workload and defragment indexes as dictated by the load as well as the fragmentation level of the index. You can do this analysis for a database by following these steps:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Identify all user tables in the current database to analyze fragmentation.&lt;/li&gt;
&lt;li&gt;Determine fragmentation of every user table and index.&lt;/li&gt;
&lt;li&gt;Determine user tables and indexes that require defragmentation by taking into account the following considerations:

&lt;ol&gt;
&lt;li&gt;A high level of fragmentation where avg_fragmentation_in_percent is greater than 20 percent&lt;/li&gt;
&lt;li&gt;Not a very small table/index—that is, pagecount is greater than 8&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;Defragment tables and indexes with high fragmentation.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Execution Plan Cache Analysis</title>
      <link>http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/execution-plan-cache-analysis/</link>
      <pubDate>Sat, 18 Mar 2017 14:54:08 +0200</pubDate>
      
      <guid>http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/execution-plan-cache-analysis/</guid>
      <description>

&lt;h2 id=&#34;execution-plan-generation&#34;&gt;Execution Plan Generation&lt;/h2&gt;

&lt;p&gt;SQL Server uses a cost-based optimization technique to determine the processing strategy of a query. The optimizer considers both the metadata of the database objects, such as unique constraints or index size, and the current distribution statistics of the columns referred to in the query when deciding which index and join strategies should be used.&lt;/p&gt;

&lt;p&gt;The following techniques are performed in order :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Parsing&lt;/li&gt;
&lt;li&gt;Binding&lt;/li&gt;
&lt;li&gt;Query optimization&lt;/li&gt;
&lt;li&gt;Execution plan generation, caching, and hash plan generation&lt;/li&gt;
&lt;li&gt;Query execution&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/query-optimization-process.jpg&#34; alt=&#34;Query Optimization Overview&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In order to arrive at an optimal plan, the query optimizer uses transformation rules and heuristics. The optimizer uses heuristics to reduce the amount of available optimization options. The plans are stored in memory is a component called the &lt;strong&gt;Memo&lt;/strong&gt;. The optimizer adopts different techniques, namely, the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Simplification&lt;/li&gt;
&lt;li&gt;Trivial plan match&lt;/li&gt;
&lt;li&gt;Multiple optimization phases&lt;/li&gt;
&lt;li&gt;Parallel plan optimization&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;view-query-tree&#34;&gt;View Query Tree&lt;/h3&gt;

&lt;p&gt;We can use some undocumented query hints and view the query that sql server optimizaer generates as follows :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT Count(*)
FROM Person.Person
OPTION (RECOMPILE,  QUERYTRACEON 8606, QUERYTRACEON 3604, QUERYTRACEON 8612, QUERYTRACEON 2372);
-- QUERYTRACEON 3604 Output to console
-- QUERYTRACEON 8606 Logical operators trees
-- QUERYTRACEON 8612   -- Add aditional cardinality info
-- QUERYTRACEON 2372,  -- Optimization stage memory info
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;costing-the-plan&#34;&gt;Costing the Plan&lt;/h3&gt;

&lt;p&gt;The query optimizer estimates the cost of each operator in the query tree and selects the least expensive one. SQL Server estimates the number of rows returned, cardinality estimate, by looking at the statistics available. It also considers the resources such as I/O, CPU and memory used by each operator.&lt;/p&gt;

&lt;h2 id=&#34;components-of-the-execution-plan&#34;&gt;Components of the Execution Plan&lt;/h2&gt;

&lt;p&gt;The execution plan generated by the optimizer contains two components.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Query plan&lt;/strong&gt; - This represents the commands that specify all the physical operations required to execute a query.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Execution context&lt;/strong&gt; - This maintains the variable parts of a query within the context of a given user.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;aging-of-the-execution-plan&#34;&gt;Aging of the Execution Plan&lt;/h2&gt;

&lt;p&gt;The procedure cache is part of SQL Server’s buffer cache, which also holds data pages. As new execution plans are added to the procedure cache, the size of the procedure cache keeps growing, affecting the retention of useful data pages in memory. To avoid this, SQL Server dynamically controls the retention of the execution plans in the procedure cache, retaining the frequently used execution plans and discarding plans that are not used for a certain period of time.&lt;/p&gt;

&lt;p&gt;When an execution plan is generated, the age field is populated with the cost of generating the plan. A complex query requiring extensive optimization will have an age field value higher than that for a simpler query.&lt;/p&gt;

&lt;p&gt;The cheaper the execution plan was to generate, the sooner its cost will be reduced to 0. Once an execution plan’s cost reaches 0, the plan becomes a candidate for removal from memory.&lt;/p&gt;

&lt;h2 id=&#34;analyzing-the-execution-plan-cache&#34;&gt;Analyzing the Execution Plan Cache&lt;/h2&gt;

&lt;p&gt;You can obtain a lot of information about the execution plans in the procedure cache by accessing various dynamic management objects. The initial DMO for working with execution plans is sys.dm_exec_cached_plans.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT  *
FROM    sys.dm_exec_cached_plans;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;execution-plan-reuse&#34;&gt;Execution Plan Reuse&lt;/h2&gt;

&lt;p&gt;When a query is submitted, SQL Server checks the procedure cache for a matching execution plan. If one is not found, then SQL Server performs the query compilation and optimization to generate a new execution plan. However, if the plan exists in the procedure cache, it is reused with the private execution context. This saves the CPU cycles that otherwise would have been spent on the plan generation.&lt;/p&gt;

&lt;h2 id=&#34;query-plan-hash-and-query-hash&#34;&gt;Query Plan Hash and Query Hash&lt;/h2&gt;

&lt;p&gt;With SQL Server 2008, new functionality around execution plans and the cache was introduced called the query plan hash and the query hash. These are binary objects using an algorithm against the query or the query plan to generate the binary hash value.&lt;/p&gt;

&lt;p&gt;To see the hash values in action, create two queries.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT  *
FROM    Production.Product AS p
JOIN    Production.ProductSubcategory AS ps
        ON p.ProductSubcategoryID = ps.ProductSubcategoryID
JOIN    Production.ProductCategory AS pc
        ON ps.ProductCategoryID = pc.ProductCategoryID
WHERE   pc.[Name] = &#39;Bikes&#39;
        AND ps.[Name] = &#39;Touring Bikes&#39;;

SELECT  *
FROM    Production.Product AS p
JOIN    Production.ProductSubcategory AS ps
        ON p.ProductSubcategoryID = ps.ProductSubcategoryID
JOIN    Production.ProductCategory AS pc
        ON ps.ProductCategoryID = pc.ProductCategoryID
where   pc.[Name] = &#39;Bikes&#39;
        and ps.[Name] = &#39;Road Bikes&#39;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After you execute each of these queries, you can see the results of these format changes from sys.dm_exec_query_stats :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT  deqs.execution_count,
        deqs.query_hash,
        deqs.query_plan_hash,
        dest.text
FROM    sys.dm_exec_query_stats AS deqs
CROSS APPLY sys.dm_exec_sql_text(deqs.plan_handle) dest
WHERE   dest.text LIKE &#39;SELECT  *
FROM    Production.Product AS p%&#39;;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;query-plan-caching&#34;&gt;Query Plan Caching&lt;/h2&gt;

&lt;p&gt;The execution plan of a query generated by the optimizer is saved in a special part of SQL Server’s memory pool called the plan cache or procedure cache. Saving the plan in a cache allows SQL Server to avoid running through the whole query optimization process again when the same query is resubmitted. SQL Server supports different techniques such as plan cache aging and plan cache types to increase the reusability of the cached plans. It also stores two binary values called the &lt;strong&gt;query hash&lt;/strong&gt; and the &lt;strong&gt;query plan hash&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/plan-retrieved-from-cache.png&#34; alt=&#34;Query Caching&#34; /&gt;&lt;/p&gt;

&lt;p&gt;When performing query tuning, you will need to clear the plan cache. You can use the DBCC command as follows :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;DBCC FREEPROCCACHE
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;view-cached-query-plan&#34;&gt;View Cached Query Plan&lt;/h3&gt;

&lt;p&gt;We can view the cached query plans using the DMV &lt;code&gt;sys.dm_exec_cached_plans&lt;/code&gt; as follows :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT usecounts, cacheobjtype, plan_handle
FROM sys.dm_exec_cached_plans
WHERE cacheobjtype = &#39;Compiled Plan&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;view-graphical-execution-plan-from-cache&#34;&gt;View Graphical Execution Plan from Cache&lt;/h3&gt;

&lt;p&gt;We can use the stored binary plan handle to retrieve the XML and graphical query plans using the DMF &lt;code&gt;sys.dm_exec_query_plan&lt;/code&gt; and passing the plan handle as follows :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT query_plan 
FROM sys.dm_exec_query_plan(&amp;lt;query plan handle here&amp;gt;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;view-memory-used-by-cached-plans&#34;&gt;View Memory Used by Cached Plans&lt;/h3&gt;

&lt;p&gt;We can use the &lt;code&gt;sys.dm_os_memory_cache_entries&lt;/code&gt; to see the memory used by cached query plans for both Adhoc queries, stored procedures and bound tree with the following :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT name, in_use_count, original_cost, pages_kb
FROM sys.dm_os_memory_cache_entries
WHERE type in (&#39;CACHESTORE_OBJCP&#39;, &#39;CACHESTORE_SQLCP&#39;, &#39;CACHESTORE_PHDR&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;reading-query-plans&#34;&gt;Reading Query Plans&lt;/h2&gt;

&lt;p&gt;SQL Server supports displaying the query plans as :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Graphical&lt;/li&gt;
&lt;li&gt;Text&lt;/li&gt;
&lt;li&gt;XML&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;display-graphical-query-plan&#34;&gt;Display Graphical Query Plan&lt;/h3&gt;

&lt;p&gt;Using SQL Server Management Studio we can display two types of the execution plan :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Estimated Execution Plan&lt;/li&gt;
&lt;li&gt;Actual Execution Plan&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Running the following query on my machine :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;USE AdventureWorks2012
GO
SELECT DISTINCT(City)
FROM Person.Address
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;i get the following execution plan.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/graphical-execution-plan.png&#34; alt=&#34;Graphical Execution Plans&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;characterstics&#34;&gt;Characterstics&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Each node represents an operation implemented by an &lt;strong&gt;iterator&lt;/strong&gt;, also called an operator. The logical operation is displayed in brackets. If both logical and physical are the same no brackets are displayed.&lt;/li&gt;
&lt;li&gt;Each node is related to its parent, and data flow is represented with arrows&lt;/li&gt;
&lt;li&gt;Data flows from left to right&lt;/li&gt;
&lt;li&gt;The arrow width represents the relative number of rows returned&lt;/li&gt;
&lt;li&gt;You can hover on the operator for more properties&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To perform their job, physical operators implement at least the following three methods:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Open()&lt;/strong&gt; - Causes an operator to be initialized, and may include setting up any required data structures&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GetRow()&lt;/strong&gt; - Requests a row from the operator&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Close()&lt;/strong&gt; - Performs some cleanup operations and shuts down the operator once it has performed its role&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;display-text-plan&#34;&gt;Display Text Plan&lt;/h3&gt;

&lt;p&gt;The text plans will be deprecated in future, instead you should use the XML and graphical execution plan. The two options allow you to display estimated execution plans as text. We can also display query plan as text using the SET option &lt;code&gt;SHOWPLAN_TEXT&lt;/code&gt; and setting to &lt;strong&gt;ON&lt;/strong&gt; and &lt;strong&gt;OFF&lt;/strong&gt;  as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;USE AdventureWorks2012
GO
SET SHOWPLAN_TEXT ON
GO
SELECT DISTINCT(City) 
FROM Person.Address
GO
SET SHOWPLAN_TEXT OFF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and we will get the follwing output :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;StmtText
----------------------------------------------
SELECT DISTINCT(City) 
FROM Person.Address

(1 row(s) affected)

StmtText
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  |--Hash Match(Aggregate, HASH:([AdventureWorks2012].[Person].[Address].[City]), RESIDUAL:([AdventureWorks2012].[Person].[Address].[City] = [AdventureWorks2012].[Person].[Address].[City]))
       |--Index Scan(OBJECT:([AdventureWorks2012].[Person].[Address].[IX_Address_AddressLine1_AddressLine2_City_StateProvinceID_PostalCode]))

(2 row(s) affected)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To display additional information, you can use the &lt;code&gt;SHOWPLAN_ALL&lt;/code&gt; SET option as follows :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;USE AdventureWorks2012
GO
SET SHOWPLAN_ALL ON
GO
SELECT DISTINCT(City) 
FROM Person.Address
GO
SET SHOWPLAN_ALL OFF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;STATISTICS PROFILE&lt;/code&gt; SET options  actually runs the query  and displays the text plan with more detailed results as follows :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;Use AdventureWorks2012
GO
SET STATISTICS PROFILE ON
GO
SELECT DISTINCT(City)
FROM Person.Address
GO
SET STATISTICS PROFILE OFF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/statistics-profile-on.png&#34; alt=&#34;Statistics Profile&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;missing-indexes-from-graphical-execution&#34;&gt;Missing Indexes from Graphical Execution&lt;/h3&gt;

&lt;p&gt;The graphical query execution plan can show when a query can benefit from missing indexes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/missing-index-from-execution-plan.png&#34; alt=&#34;Missing Indexes from Graphical Execution Plan&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;additional-plan-properties&#34;&gt;Additional Plan Properties&lt;/h3&gt;

&lt;p&gt;The query plan properties also display the reason for an early termination in the &lt;strong&gt;Optimization Level&lt;/strong&gt; property, e.g when we run the following query :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT * FROM Sales.SalesOrderHeader
WHERE SalesOrderID = 43666
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;you will notice that the level is displayed as a trivial as follows :&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/trivial-plan.png&#34; alt=&#34;Trivial Plan&#34; /&gt;&lt;/p&gt;

&lt;p&gt;but we can use a query hint to disable the trivial plan instead as follows :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SELECT * FROM Sales.SalesOrderHeader
WHERE SalesOrderID = 43666
OPTION (QUERYTRACEON 8757)
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&#34;admonition note&#34;&gt;
&lt;p class=&#34;admonition-title&#34;&gt;Trace Flags&lt;/p&gt;
&lt;p&gt;The QUERYTRACEON query hint is used to apply a trace flag at the query level.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;The &lt;strong&gt;CardinalityEstimationModelVersion&lt;/strong&gt; attribute refers to the version of the cardinality estimation model used by the query optimizer.  SQL Server 2014 introduces a new cardinality estimator, but you still have the choice of using the old one by changing the database compatibility level or using trace flags 2312 and 9481.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;NonParallelPlanReason&lt;/strong&gt; optimal gives a reason why a parallel plan was not chosen. E.g if we use the option hint to use only one processor by setting the &lt;strong&gt;MAXDOP&lt;/strong&gt; to 1 as follows :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT * FROM Sales.SalesOrderHeader
WHERE SalesOrderID = 43666
OPTION (MAXDOP 1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/nonparallel-reason.png&#34; alt=&#34;Non Paralled Reason&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;warnings-on-execution-plans&#34;&gt;Warnings on Execution Plans&lt;/h3&gt;

&lt;p&gt;The query plan also shows warnings and these should be carefully reviewed. The following warnings are displayed :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ColumnsWithNoStatistics&lt;/li&gt;
&lt;li&gt;NoJoinPredicate&lt;/li&gt;
&lt;li&gt;SpillToTempDb&lt;/li&gt;
&lt;li&gt;TypeConversion&lt;/li&gt;
&lt;li&gt;Wait&lt;/li&gt;
&lt;li&gt;PlanAffectingConvert&lt;/li&gt;
&lt;li&gt;SpatialGuess&lt;/li&gt;
&lt;li&gt;UnmatchedIndexes&lt;/li&gt;
&lt;li&gt;FullUpdateForOnlineIndexBuild&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;columnswithnostatistics-warning&#34;&gt;ColumnsWithNoStatistics Warning&lt;/h3&gt;

&lt;p&gt;They were no statistics available to the query optimizer for the column.&lt;/p&gt;

&lt;p&gt;Run the following statement to drop the existing statistics for the VacationHours column, if available:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;DROP STATISTICS HumanResources.Employee._WA_Sys_0000000C_49C3F6B7
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can view the statistics on the table with the following query :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT s.name, c.name, auto_created 
FROM sys.stats s
JOIN sys.columns c
ON s.object_id = c.object_id and s.stats_id = c.column_id
WHERE s.object_id = object_id(&#39;HumanResources.Employee&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, temporarily disable automatic creation of statistics at the database level:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;ALTER DATABASE AdventureWorks2012
SET AUTO_CREATE_STATISTICS OFF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then run this query:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT * FROM HumanResources.Employee
WHERE VacationHours = 48
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and you will get a query plan as follows :&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/missing-column-statistics.png&#34; alt=&#34;Query Plan Missing Statistics&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Enable the auto creating of statistics with the following :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;ALTER DATABASE AdventureWorks2012 SET AUTO_CREATE_STATISTICS ON
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and re-run the query. You will notice the query plan no longer have warnings as the statistics have been auto created.&lt;/p&gt;

&lt;h3 id=&#34;nojoinpredicate-warning&#34;&gt;NoJoinPredicate Warning&lt;/h3&gt;

&lt;p&gt;A possible problem while using the old-style ANSI SQL-89 join syntax is accidentally missing the join predicate and getting a NoJoinPredicate warning. Let’s suppose you intend to run the following query but forgot to include the WHERE clause:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT * FROM Sales.SalesOrderHeader soh, Sales.SalesOrderDetail sod WHERE soh.SalesOrderID = sod.SalesOrderID
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The first indication of a problem could be that the query takes way too long to execute, even for small tables. Later, you will see that the query also returns a huge result set.&lt;/p&gt;

&lt;h3 id=&#34;planaffectingconvert-warning&#34;&gt;PlanAffectingConvert Warning&lt;/h3&gt;

&lt;p&gt;This warning shows that type conversions were performed that may impact the performance of the resulting execution plan. Run the following example, which declares a variable as nvarchar and then uses it in a query to compare against a varchar column, CreditCardApprovalCode:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;DECLARE @code NVARCHAR(15)
SET @code = &#39;95555Vi4081&#39;
SELECT * 
FROM Sales.SalesOrderHeader
WHERE CreditCardApprovalCode = @code
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/plan-convert.png&#34; alt=&#34;Plan Convert Issue&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;spilltotempdb-warning&#34;&gt;SpillToTempDb Warning&lt;/h3&gt;

&lt;p&gt;This warning shows than an operation didn’t have enough memory and had to spill data to disk during execution, which can be a performance problem because of the extra I/O overhead. To simulate this problem, run the following example:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT * FROM Sales.SalesOrderDetail ORDER BY UnitPrice
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is a very simple query, and depending on the memory available on your system, you may not get the warning in your test environment, so you may need to try with a larger table instead.&lt;/p&gt;

&lt;h3 id=&#34;unmatchedindexes&#34;&gt;UnmatchedIndexes&lt;/h3&gt;

&lt;p&gt;Finally, the UnmatchedIndexes element can show that the query optimizer was not able to match a filtered index for a particular query (for example, when it is not able to see the value of a parameter). Suppose you create the following filtered index:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;CREATE INDEX IX_Color ON Production.Product(Name, ProductNumber)
WHERE Color = ‘White’
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and then run the query :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;DECLARE @color nvarchar(15)
SET @color = &#39;White&#39;
SELECT Name, ProductNumber FROM Production.Product
WHERE Color = @color
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You will notice we get an unmatched index, however the following will use an index :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT Name, ProductNumber FROM Production.Product
WHERE Color = &#39;White&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and remove the index :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;DROP INDEX Production.Product.IX_Color
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;getting-plans-from-the-plan-cache&#34;&gt;Getting Plans from the Plan Cache&lt;/h2&gt;

&lt;p&gt;Most of the times you will not run the queries manually but you will need to invest the queries that have already been ran. We can can retrieve the query plan in cache using the &lt;code&gt;sys.sm_exec_cached_plans&lt;/code&gt; or from currently executing queries using &lt;code&gt;sys.dm_exec_requests&lt;/code&gt; and from query statistics using &lt;code&gt;sys.dm_exec_query_stats&lt;/code&gt; :&lt;/p&gt;

&lt;h3 id=&#34;retrive-from-current-executing-queries&#34;&gt;Retrive from current executing queries&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;-- A plan_handle is a hash value that represents a specific execution plan
SELECT * FROM sys.dm_exec_requests
CROSS APPLY
sys.dm_exec_query_plan(plan_handle)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;from-query-stats&#34;&gt;From query stats&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- The sys.dm_exec_query_stats DMV contains one row per query statement within the cached plan
SELECT * FROM sys.dm_exec_query_stats
CROSS APPLY
sys.dm_exec_query_plan(plan_handle)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;retrieve-top-10-queries-by-usage&#34;&gt;Retrieve top 10 queries by usage&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- You can run the following query to get this information
SELECT TOP 10 total_worker_time/execution_count AS avg_cpu_time,
plan_handle, query_plan
FROM sys.dm_exec_query_stats
CROSS APPLY sys.dm_exec_query_plan(plan_handle)
ORDER BY avg_cpu_time DESC
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;removing-plans-from-the-plan-cache&#34;&gt;Removing Plans from the Plan Cache&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;The &lt;strong&gt;DBCC FREEPROCCACHE&lt;/strong&gt; statement can be used to remove all the entries from the plan cache.&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;DBCC FREESYSTEMCACHE&lt;/strong&gt; statement can be used to remove all the elements from the plan cache or only the elements associated with a Resource Governor pool name.&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;DBCC FLUSHPROCINDB&lt;/strong&gt; can be used to remove all the cached plans for a particular database.&lt;/li&gt;
&lt;li&gt;Not related,  the &lt;strong&gt;DBCC DROPCLEANBUFFERS&lt;/strong&gt; statement can be used to remove all the buffers from the buffer pool.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;set-statistics-time-io&#34;&gt;SET STATISTICS TIME / IO&lt;/h2&gt;

&lt;p&gt;You can use SET STATISTICS TIME to see the number of milliseconds required to parse, compile, and execute each statement. For example, run&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SET STATISTICS TIME ON
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and then run the following query:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT DISTINCT(CustomerID)
FROM Sales.SalesOrderHeader
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and turn it off :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SET STATISTICS TIME OFF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;SET STATISTICS IO&lt;/strong&gt; displays the amount of disk activity generated by a query. To enable it, run the following statement:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SET STATISTICS IO ON
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Run this next statement to clean all the buffers from the buffer pool to make sure that no pages for this table are loaded in memory:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;DBCC DROPCLEANBUFFERS
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then run the following query:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT * FROM Sales.SalesOrderDetail WHERE ProductID = 870
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It will show an output similar to the following:&lt;/p&gt;

&lt;p&gt;Here are the definitions of these items, which all use 8K pages:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Logical reads&lt;/strong&gt; Number of pages read from the buffer pool.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Physical reads&lt;/strong&gt; Number of pages read from disk.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Read-ahead reads&lt;/strong&gt; Read-ahead is a performance optimization mechanism that anticipates the needed data pages and reads them from disk. It can read up to 64 contiguous pages from one data file.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lob logical reads&lt;/strong&gt; Number of large object (LOB) pages read from the buffer pool.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lob physical reads&lt;/strong&gt; Number of large object (LOB) pages read from disk.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lob read-ahead reads&lt;/strong&gt; Number of large object (LOB) pages read from disk using the read-ahead mechanism, as explained earlier.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now, if you run the same query again, you will no longer get physical and read-ahead reads.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Scan count&lt;/strong&gt; is defined as the number of seeks or scans started after reaching the leaf level (that is, the bottom level of an index). The only case when scan count will return 0 is when you’re seeking for only one value on a unique index, like in the following example:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT * FROM Sales.SalesOrderHeader WHERE SalesOrderID = 51119
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you try the following query, in which SalesOrderID is defined in a nonunique index and can return more than one record, you can see that scan count now returns 1:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT * FROM Sales.SalesOrderDetail WHERE SalesOrderID = 51119
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, in the following example, scan count is 4 because SQL Server has to perform four seeks:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT * FROM Sales.SalesOrderHeader WHERE SalesOrderID IN (51119, 43664, 63371, 75119)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;execution-plan-cache-recommendations&#34;&gt;Execution Plan Cache Recommendations&lt;/h2&gt;

&lt;p&gt;To ensure efficient use of the plan cache, follow these recommendations:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Explicitly parameterize variable parts of a query.&lt;/li&gt;
&lt;li&gt;Use stored procedures to implement business functionality.&lt;/li&gt;
&lt;li&gt;Use sp_executesql to avoid stored procedure maintenance.&lt;/li&gt;
&lt;li&gt;Use the prepare/execute model to avoid resending a query string.&lt;/li&gt;
&lt;li&gt;Avoid ad hoc queries.&lt;/li&gt;
&lt;li&gt;Use sp_executesql over EXECUTE for dynamic queries.&lt;/li&gt;
&lt;li&gt;Parameterize variable parts of queries with care.&lt;/li&gt;
&lt;li&gt;Avoid modifying environment settings between connections.&lt;/li&gt;
&lt;li&gt;Avoid the implicit resolution of objects in queries.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;query-optimizer-in-depth&#34;&gt;Query Optimizer in-depth&lt;/h2&gt;

&lt;p&gt;In this part we will now go in-depth and look at the query optimization process. Here is the major steps of the query optimization process.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/query-optimization-in-depth.png&#34; alt=&#34;Query Optimization in Depth&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;dmvs-for-query-optimization-sys-dm-exec-query-optimizer-info&#34;&gt;DMVs for Query Optimization sys.dm_exec_query_optimizer_info&lt;/h3&gt;

&lt;p&gt;Returns detailed statistics about the operation of the SQL Server query optimizer. You can use this view when tuning a workload to identify query optimization problems or improvements. For example, you can use the total number of optimizations, the elapsed time value, and the final cost value to compare the query optimizations of the current workload and any changes observed during the tuning process. Some counters provide data that is relevant only for SQL Server internal diagnostic use.&lt;/p&gt;

&lt;h3 id=&#34;viewing-statistics-on-optimizer-execution&#34;&gt;Viewing statistics on optimizer execution&lt;/h3&gt;

&lt;p&gt;What are the current optimizer execution statistics for this instance of SQL Server?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT * FROM sys.dm_exec_query_optimizer_info;
&lt;/code&gt;&lt;/pre&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;th&gt;Data type&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;counter&lt;/td&gt;
&lt;td&gt;nvarchar(4000)&lt;/td&gt;
&lt;td&gt;Name of optimizer statistics event.&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;occurrence&lt;/td&gt;
&lt;td&gt;bigint&lt;/td&gt;
&lt;td&gt;Number of occurrences of optimization event for this counter.&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;value&lt;/td&gt;
&lt;td&gt;float&lt;/td&gt;
&lt;td&gt;Average property value per event occurrence.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;viewing-the-total-number-of-optimizations&#34;&gt;Viewing the total number of optimizations&lt;/h3&gt;

&lt;p&gt;How many optimizations are performed?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT occurrence AS Optimizations FROM sys.dm_exec_query_optimizer_info
WHERE counter = &#39;optimizations&#39;;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;average-elapsed-time-per-optimization&#34;&gt;Average elapsed time per optimization&lt;/h3&gt;

&lt;p&gt;What is the average elapsed time per optimization?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT ISNULL(value,0.0) AS ElapsedTimePerOptimization  
FROM sys.dm_exec_query_optimizer_info WHERE counter = &#39;elapsed time&#39;;  
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;fraction-of-optimizations-that-involve-subqueries&#34;&gt;Fraction of optimizations that involve subqueries&lt;/h3&gt;

&lt;p&gt;What fraction of optimized queries contained a subquery?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT (SELECT CAST (occurrence AS float) 
FROM sys.dm_exec_query_optimizer_info WHERE counter = &#39;contains subquery&#39;) /  
(SELECT CAST (occurrence AS float) FROM sys.dm_exec_query_optimizer_info WHERE counter = &#39;optimizations&#39;)  
AS ContainsSubqueryFraction;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and as a percentage of the total optimizations :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT (SELECT CAST (occurrence AS float) 
FROM sys.dm_exec_query_optimizer_info WHERE counter = &#39;contains subquery&#39;) /  
(SELECT CAST (occurrence AS float) FROM sys.dm_exec_query_optimizer_info WHERE counter = &#39;optimizations&#39;) * 100 
AS ContainsSubqueryFraction ;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;view-percentage-of-optimization-on-query-with-hints&#34;&gt;View percentage of optimization on query with hints&lt;/h3&gt;

&lt;p&gt;This will be a good indicator on the flexibility of the application. The application should use hints sparingly.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- for example, the next query displays the percentage of optimizations in the instance that include hints
SELECT (SELECT occurrence FROM sys.dm_exec_query_optimizer_info WHERE counter = &#39;hints&#39; ) * 100.0 / 
(SELECT occurrence FROM sys.dm_exec_query_optimizer_info WHERE counter = &#39;optimizations&#39; )
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;check-optimizations-for-a-specific-workload&#34;&gt;Check optimizations for a specific workload&lt;/h3&gt;

&lt;p&gt;We can use the DMV to check for the optimization applied to a specific workload but they are several issues. There is no easy way but we can get a snapshot of the before optimization and after optimization and subtract the value to have an idea of the optimizations applied.&lt;/p&gt;

&lt;p&gt;Here is a query that can do that&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- optimize these queries now
-- so they do not skew the collected results
GO
SELECT *
INTO after_query_optimizer_info
FROM sys.dm_exec_query_optimizer_info
GO
SELECT *
INTO before_query_optimizer_info
FROM sys.dm_exec_query_optimizer_info
GO
DROP TABLE before_query_optimizer_info
DROP TABLE after_query_optimizer_info
GO
-- real execution starts
GO
SELECT *
INTO before_query_optimizer_info
FROM sys.dm_exec_query_optimizer_info
GO
-- insert your query here
SELECT *
FROM Person.Address
-- keep this to force a new optimization
OPTION (RECOMPILE)
GO
SELECT *
INTO after_query_optimizer_info
FROM sys.dm_exec_query_optimizer_info
GO
SELECT a.counter,
(a.occurrence - b.occurrence) AS occurrence,
(a.occurrence * a.value - b.occurrence *
b.value) AS value
FROM before_query_optimizer_info b
JOIN after_query_optimizer_info a
ON b.counter = a.counter
WHERE b.occurrence &amp;lt;&amp;gt; a.occurrence
DROP TABLE before_query_optimizer_info
DROP TABLE after_query_optimizer_info
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We list some queries twi so that the query optimizer can optimize them first so that when we run again no new optimizations are added.&lt;/p&gt;

&lt;h3 id=&#34;parsing-and-binding&#34;&gt;Parsing and Binding&lt;/h3&gt;

&lt;p&gt;The Algebrizer parse and binds the SQL queries. It validates the syntax and uses the query information to build a relation query tree. Parsing only checks for valid SQL syntax without accessing the table names or columns or checking for their existence.&lt;/p&gt;

&lt;p&gt;We can test the following in SMSS by setting the options to &lt;strong&gt;PARSE_ONLY&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT lname, fname FROM authors
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/parse-only.png&#34; alt=&#34;Parse Only in SSMS&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The above will pass the syntax since that&amp;rsquo;s valid SQL statement.&lt;/p&gt;

&lt;p&gt;The next stage is the binding, which validates the existence of the tables and their columns against the system catalogs and the permissions. The output is a algebrizer tree that is sent to the optimizer for optimization.&lt;/p&gt;

&lt;p&gt;The tree contains logical operations that are closely related to the corresponding SQL statement. There is no documentation on this but there&amp;rsquo;s a DMV that have some information on the mappings of the logical operations :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT * FROM sys.dm_xe_map_values 
WHERE name = &#39;query_optimizer_tree_id&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are also several undocumented trace flags that can allow you to see these different logical trees. For example, you can use the following query with the undocumented trace flag 8605. But first enable trace flag 3604, as shown next:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;DBCC TRACEON(3604)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and check in the message tab for the logical query tree :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT * FROM Sales.SalesOrderDetail
WHERE SalesOrderID = 43659
OPTION (RECOMPILE, QUERYTRACEON 8605, QUERYTRACEON 3604)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The tree gets verbose quickly even for a small query. Unfortunately, there is no documented information to help understand these output trees and their operations.&lt;/p&gt;

&lt;h3 id=&#34;simplification&#34;&gt;Simplification&lt;/h3&gt;

&lt;p&gt;At this stage of the optimization process the query tree is simplified into a simpler form to enable quick optimization. Here are some of the things that happens during the simplification stage :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Contradictions are detected and removed and SQL server will not even execute the query. The optimizer can detect that no row will be returned by using a certain set of predicate combination.&lt;/li&gt;
&lt;li&gt;Filters in the WHERE clause are pushed down the tree so that they are evaluated early and reduce the amount of data access and also enable using indexes to retrieve the data. This is called &lt;em&gt;predicate pushdown&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Subqueries are converted into joins if possible.&lt;/li&gt;
&lt;li&gt;Redudant inner and outer joins are removed, e.g in when a table is join with another table that have a foregin key relationship, the other table might not be needed to retrieve data in what is called a Foreign Key Join elimination.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;contradiction-detection&#34;&gt;Contradiction Detection&lt;/h3&gt;

&lt;p&gt;Let&amp;rsquo;s use a table with a check constraint and see how the optimizer does contradicition detection. There&amp;rsquo;s a check constraint on the &lt;strong&gt;HumanResources.Employee&lt;/strong&gt; called &lt;strong&gt;CK_Employee_VacationHours&lt;/strong&gt;. This constraint uses the following rule &lt;code&gt;([VacationHours]&amp;gt;=(-40) AND [VacationHours]&amp;lt;=(240))&lt;/code&gt;. The value needs to be between -40 and 240.&lt;/p&gt;

&lt;p&gt;Let run the following query and look at the query plan :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT * FROM HumanResources.Employee
WHERE VacationHours &amp;gt; 80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;SQL Server will use a clustered index scan to retrieve the results, now lets change the value to 300 as follows :&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/contradiction-detection.png&#34; alt=&#34;Contradiction Detection&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT * FROM HumanResources.Employee
WHERE VacationHours &amp;gt; 300
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The query plan changes and SQL server now uses a constant scan and the table is not even accessed at all. Because there is no need to access the table at all, SQL Server saves resources such as I/O, locks, memory, and CPU, thus making the query execute faster.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/constant-scan.png&#34; alt=&#34;Constant Scan&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Disable the constraint and run again and the index will now be scanned :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;ALTER TABLE HumanResources.Employee NOCHECK CONSTRAINT CK_Employee_VacationHours
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Enable the index back again :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ALTER TABLE HumanResources.Employee WITH CHECK CHECK CONSTRAINT CK_Employee_VacationHours
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Another contradiction is when the predicate does contain the contradiction as follows :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT * FROM HumanResources.Employee 
WHERE VacationHours &amp;gt; 10 AND VacationHours &amp;lt; 5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A trivial plan might also be used instead of a constant scan operator.&lt;/p&gt;

&lt;h3 id=&#34;view-the-simplified-tree&#34;&gt;View the Simplified Tree&lt;/h3&gt;

&lt;p&gt;Using the undocumented query hint from before, we can examine the generated simplified query tree by running following query and looking at the messages tab :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT * FROM HumanResources.Employee
WHERE VacationHours &amp;gt; 300
OPTION(RECOMPILE, QUERYTRACEON 8606,QUERYTRACEON 3604)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here is the output truncated :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;*** Input Tree: ***
        LogOp_Project QCOL: [AdventureWorks2012].[HumanResources].[Employee].BusinessEntityID QCOL: [AdventureWorks2012].[HumanResources].
            LogOp_Select
*** Simplified Tree: ***
LogOp_ConstTableGet (0) COL: IsBaseRow1000  QCOL: [AdventureWorks2012].[HumanResources].[Employee].BusinessEntityID QCOL:
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;foreign-key-join-elimination&#34;&gt;Foreign Key Join Elimination&lt;/h3&gt;

&lt;p&gt;For the demonstration run the following query and observe the execution plan :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT soh.CustomerID, c.AccountNumber
FROM Sales.SalesOrderHeader soh
JOIN Sales.Customer c
on soh.CustomerID = c.CustomerID
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here is the generated execution plan :&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/join-elimination-1.png&#34; alt=&#34;Join Elimination&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You will notice a merge join was used and both tables are read.&lt;/p&gt;

&lt;p&gt;Now comment out the &lt;strong&gt;AccountNumber&lt;/strong&gt; column and run the query again :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT soh.CustomerID --, c.AccountNumber
FROM Sales.SalesOrderHeader soh
JOIN Sales.Customer c
on soh.CustomerID = c.CustomerID
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and here is the generated execution plan :&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/join-elimination-2.png&#34; alt=&#34;Join Elimination&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Notice that only &lt;strong&gt;Sales.SalesOrderHeader&lt;/strong&gt; table is being read. The optimizer simplified the query using the foreign key constraint and in this case we do not need any rows from the &lt;strong&gt;Customers&lt;/strong&gt; table so there is no need to access the table at all, and the foreign key guarantees the data returned is correct.&lt;/p&gt;

&lt;p&gt;We can inspect the logical query tree using the trace flags :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT soh.CustomerID --, c.AccountNumber
FROM Sales.SalesOrderHeader soh
JOIN Sales.Customer c
on soh.CustomerID = c.CustomerID
OPTION(RECOMPILE, QUERYTRACEON 8606,QUERYTRACEON 3604)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and the generated query tree truncated looks as follows :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;*** Simplified Tree: ***
        LogOp_Join

            LogOp_Get TBL: Sales.SalesOrderHeader(alias TBL: soh) Sales.SalesOrderHeader

            LogOp_Get TBL: Sales.Customer(alias TBL: c) Sales.Customer

*******************
*** Join-collapsed Tree: ***
        LogOp_Get TBL: Sales.SalesOrderHeader(alias TBL: soh) Sales.SalesOrderHeader
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Notice the simplified query tree still retrieves from both tables but the join collapsed tree only retrieves the data from one table.&lt;/p&gt;

&lt;p&gt;Try disabling the constraint and run the query, you will notice the generated plan have to use the join as there is no other way to retrieve the data.&lt;/p&gt;

&lt;h3 id=&#34;trivial-plan&#34;&gt;TRIVIAL PLAN&lt;/h3&gt;

&lt;p&gt;The optimizer chooses a trivial plan when there&amp;rsquo;s no other way to run the query. SQL Server uses it to avoid the expensive operation of query optimization for queries that does not involve any cost estimation, e.g the following query will use a trivial plan :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;-- for example, testing with a very simple AdventureWorks2012 query, such as
SELECT * FROM Sales.SalesOrderDetail
WHERE SalesOrderID = 43659
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The reason is being the there&amp;rsquo;s a unique index &lt;strong&gt;PK_SalesOrderDetail_SalesOrderID_SalesOrderDetailID&lt;/strong&gt; that can only contain 0 or 1 row.
We can use a trace flag 8757 to disable the use a trivial plan as follows :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- let’s again add the undocumented trace flag 8757 to avoid a trivial plan
SELECT * FROM Sales.SalesOrderDetail
WHERE SalesOrderID = 43659
OPTION (RECOMPILE, QUERYTRACEON 8757)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and you notice from the properties in &lt;strong&gt;SSMS&lt;/strong&gt; that we get a full optimization. Use the &lt;code&gt;sys.dm_exec_query_optimizer_info&lt;/code&gt; and check the optimization information. You will notice instead of a trivial optimization, we have a hint and the optimization goes to search 1.&lt;/p&gt;

&lt;p&gt;Now use a different column and you will get full optimization :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT * FROM Sales.SalesOrderDetail
WHERE ProductID = 870
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The reason is because they are several plans available and the query optimizer need to find the optimal plan to execute the query.&lt;/p&gt;

&lt;h3 id=&#34;statistics&#34;&gt;STATISTICS&lt;/h3&gt;

&lt;p&gt;The optimizer maintains statistics on columns and indexes that it uses during costing in the optimization process.&lt;/p&gt;

&lt;h2 id=&#34;query-operators&#34;&gt;Query Operators&lt;/h2&gt;

&lt;p&gt;The operators or iterators are used by the execution engine to perform the operation of retrieving data based on the execution plan. The operators have different characteristics, some are memory consuming, and some have to build the input before they start executing. In this section we will look at the common operators using the execution engine.&lt;/p&gt;

&lt;h2 id=&#34;data-access-operators&#34;&gt;Data Access Operators&lt;/h2&gt;

&lt;p&gt;These are the most common operators as they are used to retrieve data :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;img src=&#34;https://i-msdn.sec.s-msft.com/dynimg/IC121534.gif&#34; alt=&#34;Table Scan&#34; /&gt; Heap Table Scan&lt;/li&gt;
&lt;li&gt;&lt;img src=&#34;https://i-msdn.sec.s-msft.com/dynimg/IC72069.gif&#34; alt=&#34;Index Scan&#34; /&gt; Clustered Index Scan&lt;/li&gt;
&lt;li&gt;&lt;img src=&#34;https://i-msdn.sec.s-msft.com/dynimg/IC322.gif&#34; alt=&#34;Clustered Index Seek&#34; /&gt; Clustered Index Seek&lt;/li&gt;
&lt;li&gt;&lt;img src=&#34;https://i-msdn.sec.s-msft.com/dynimg/IC72069.gif&#34; alt=&#34;Index Scan&#34; /&gt; Non-clustered Index Scan&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;img src=&#34;https://i-msdn.sec.s-msft.com/dynimg/IC322.gif&#34; alt=&#34;Clustered Index Seek&#34; /&gt; Non-Clustered Index Seek&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A &lt;strong&gt;scan&lt;/strong&gt; reads an entire structure, which could be a heap, a clustered index, or a nonclustered index.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A seek, on the other hand, does not scan an entire structure, but instead efficiently retrieves rows by navigating an index. Can only be performed on a clustered on nonclustered index.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;scans&#34;&gt;Scans&lt;/h3&gt;

&lt;p&gt;Let’s start with the simplest example, by scanning a heap, which is performed by the Table Scan operator&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- the following query on the AdventureWorks2012 database will use a Table Scan
SELECT * FROM DatabaseLog
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Similarly, the following query will show a Clustered Index Scan operator :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT * FROM Person.Address
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The scan operator does not guarantee returning sorted results. The storage engine uses efficient methods to return the data without sorting. Some features are only available in the Enterprise Edition. You can check the properties of the query plan to see if the results were return sorted by looking at the &lt;strong&gt;Ordered&lt;/strong&gt; property.&lt;/p&gt;

&lt;p&gt;Run the following and check the Ordered property :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- if you want to know whether your data has been sorted, the Ordered property can show if the data 
SELECT * FROM Person.Address
ORDER BY AddressID
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and you will notice that its now true.&lt;/p&gt;

&lt;h3 id=&#34;non-clustered-index-scan&#34;&gt;Non-clustered index scan&lt;/h3&gt;

&lt;p&gt;The following query will produce a non-clustered index scan. The data will be returned without querying the base table :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT AddressID, City, StateProvinceID
FROM Person.Address
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Each non-clustered index contains a clustering key, so in this case we didn&amp;rsquo;t have to query the base table because the index also contains the clustering key.&lt;/p&gt;

&lt;h3 id=&#34;seeks&#34;&gt;Seeks&lt;/h3&gt;

&lt;p&gt;An Index Seek does not scan the entire index, but instead navigates the B-tree index structure to quickly find one or more records. These can be performed by both the Clustered Index Seek and the Index Seek operators.&lt;/p&gt;

&lt;p&gt;The following query produces a clustered index seek :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt; -- now let’s look at Index Seeks
SELECT AddressID, City, StateProvinceID FROM Person.Address
WHERE AddressID = 12037
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and the following produces a non-clustered index seek :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- the next query and Figure 4-6 both illustrate a nonclustered Index Seek operator
SELECT AddressID, StateProvinceID FROM Person.Address
WHERE StateProvinceID = 32
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;a clustered index seek can also return multiple rows, e,g in the following query :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- the previous query just returned one row, but you can change it to a new parameter like in the following example:
SELECT AddressID, StateProvinceID FROM Person.Address
WHERE StateProvinceID = 9
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The query have been auto-parameterized, so any query will use the same plan, we can try to drop the cached query plans but you will notice the same query plan will be used.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;DBCC FREEPROCCACHE
GO
Use AdventureWorks2012
GO
-- the previous query just returned one row, but you can change it to a new parameter like in the following example:
SELECT AddressID, StateProvinceID FROM Person.Address
WHERE StateProvinceID = 9
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The query is using a partial ordered scan. It first finds the initial row and continues scanning the index for all matching rows which are logically together in the same leaf pages on the index. In this case, the data was retrieved without ever touching the base table.&lt;/p&gt;

&lt;p&gt;A more complicated example of partial ordered scans involves using a nonequality operator or a BETWEEN clause, like in the following example:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- a more complicated example of partial ordered scans involves using a nonequality operator or a BETWEEN clause
SELECT AddressID, City, StateProvinceID FROM Person.Address
WHERE AddressID BETWEEN 10000 and 20000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A Clustered Index Seek operation will be used to find the first row that qualifies to the filter predicate and will continue scanning the index, row by row, until the last row that qualifies is found. More accurately, the scan will stop on the first row that does not qualify.&lt;/p&gt;

&lt;h3 id=&#34;bookmark-lookup&#34;&gt;Bookmark Lookup&lt;/h3&gt;

&lt;p&gt;The bookmark operator is used when a nonclustered index is useful to find one or more rows but does not cover all the columns in the query. Because the nonclustered index contains the clustering key, the base table is accessed using the clustering key to retrieve the columns not covered by the index.&lt;/p&gt;

&lt;p&gt;For example, in our previous query, an existing nonclustered index covers both AddressID and StateProvinceID columns. What if we also request the City and ModifiedDate columns on the same query? This is shown in the next query, which returns one record :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT AddressID, City, StateProvinceID, ModifiedDate
FROM Person.Address
WHERE StateProvinceID = 32
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The query optimizer is choosing the index &lt;strong&gt;IX_Address_StateProvinceID&lt;/strong&gt; to find the records quickly. However, because the index does not cover the additional columns, it also needs to use the base table (in this case, the clustered index) to get that additional information. This operation is called a &lt;strong&gt;bookmark lookup&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Text and XML plans can show whether a Clustered Index Seek operator is performing a bookmark lookup by looking at the LOOKUP keyword and Lookup attributes :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SET SHOWPLAN_TEXT ON
GO
SELECT AddressID, City, StateProvinceID, ModifiedDate
FROM Person.Address
WHERE StateProvinceID = 32
GO
SET SHOWPLAN_TEXT OFF
GO
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;output :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;StmtText
----------------------------------------------------------------------------------------------------------------------------------------
  |--Nested Loops(Inner Join, OUTER REFERENCES:([AdventureWorks2012].[Person].[Address].[AddressID]))
       ....
       |--Clustered Index Seek(OBJECT:.. SEEK:([Person].[Address].[AddressID]) LOOKUP ORDERED FORWARD)

(3 row(s) affected)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now run the same query, but this time, request StateProvinceID equal to 20. Now this will produce an index scan :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT AddressID, City, StateProvinceID, ModifiedDate
FROM Person.Address
WHERE StateProvinceID = 20
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So the query optimizer is producing two different execution plans for the same query, with the only difference being the value of the StateProvinceID parameter. In this case, the query optimizer uses the value of the query’s &lt;strong&gt;StateProvinceID&lt;/strong&gt; parameter to estimate the cardinality of the predicate as it tries to produce an efficient plan for that parameter.&lt;/p&gt;

&lt;p&gt;The bookmark lookup performs random I/O so it is an expensive operation when more records need to be returned, so the optimizer instead chose to scan the index. The optimizer is cost based, so it used the statistics to find an optimal plan.&lt;/p&gt;

&lt;h2 id=&#34;aggregations&#34;&gt;Aggregations&lt;/h2&gt;

&lt;p&gt;SQL Server have two physical operators for implementing aggregations :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Stream Aggregate&lt;/li&gt;
&lt;li&gt;Hash Aggregate&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;sorting-and-hashing&#34;&gt;Sorting and Hashing&lt;/h3&gt;

&lt;p&gt;Lets discuss sorting and hashing before continuing with the rest of the operators as this plays an important roles.  The optimizer employs different methods to provide sorted data :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Use an index&lt;/li&gt;
&lt;li&gt;use the Sort operator&lt;/li&gt;
&lt;li&gt;Use hashing algorithms&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;stream-aggregate&#34;&gt;Stream Aggregate&lt;/h3&gt;

&lt;p&gt;The Stream Aggregate is used for scalar aggregates, aggregates that return only a single value, e.g the SUM, COUNT, AVG, ect.c functions.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT AVG(ListPrice) FROM Production.Product
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and produces the following plan :&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/scalar-aggregate.png&#34; alt=&#34;Scalar Aggregate&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Use the ext plan to reveal more information :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SET SHOWPLAN_TEXT ON
GO
SELECT AVG(ListPrice) FROM Production.Product
GO
SET SHOWPLAN_TEXT OFF
GO
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;In order to implement the &lt;strong&gt;AVG&lt;/strong&gt; aggregation function, the &lt;strong&gt;Stream Aggregate&lt;/strong&gt; is computing both a &lt;strong&gt;COUNT&lt;/strong&gt; and a &lt;strong&gt;SUM&lt;/strong&gt; aggregate, the results of which will be stored in the computed expressions &lt;strong&gt;Expr1003&lt;/strong&gt; and &lt;strong&gt;Expr1004&lt;/strong&gt;, respectively.&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;Compute Scalar&lt;/strong&gt; verifies that there is no division by zero by using a &lt;strong&gt;CASE&lt;/strong&gt; expression.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;group-by-clause&#34;&gt;Group By Clause&lt;/h3&gt;

&lt;p&gt;When using a GROUP BY clause, the input needs to be sorted first, if not an operator is introduced to sort the results first :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- now let’s see an example of a query using the GROUP BY clause
SELECT ProductLine, COUNT(*) FROM Production.Product
GROUP BY ProductLine
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/group-by-aggregate.png&#34; alt=&#34;Group By&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The first query did not use a sort operator because without a &lt;strong&gt;GROUP BY&lt;/strong&gt; clause, the input is considered a single group.&lt;/p&gt;

&lt;h3 id=&#34;sort-with-an-index&#34;&gt;Sort with an Index&lt;/h3&gt;

&lt;p&gt;The Stream Aggregate can also use an index to provide sorted results :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- a Stream Aggregate can also use an index to have its input sorted, as in the following query
SELECT SalesOrderID, SUM(LineTotal)FROM Sales.SalesOrderDetail
GROUP BY SalesOrderID
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;output :&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/index-sort-aggregate.png&#34; alt=&#34;Index Sort&#34; /&gt;&lt;/p&gt;

&lt;p&gt;No Sort operator is needed in this plan because the Clustered Index Scan provides the data already sorted by &lt;strong&gt;SalesOrderID&lt;/strong&gt;, which is part of the clustering key of the &lt;strong&gt;SalesOrderDetail&lt;/strong&gt; table&lt;/p&gt;

&lt;h3 id=&#34;hash-aggregate&#34;&gt;Hash Aggregate&lt;/h3&gt;

&lt;p&gt;Its implemented with the &lt;strong&gt;Hash Match&lt;/strong&gt; physical operator.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Works on unsorted data for big tables&lt;/li&gt;
&lt;li&gt;Cardinality estimates should produce few groups&lt;/li&gt;
&lt;li&gt;Builds a hash table in memory&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For example, the &lt;strong&gt;SalesOrderHeader&lt;/strong&gt; table has no index on the &lt;strong&gt;TerritoryID&lt;/strong&gt; column, so the following query will use a &lt;strong&gt;Hash Aggregate&lt;/strong&gt; operator.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- for example, the SalesOrderHeader table has no index on the TerritoryID column, 
-- so the following query will use a Hash Aggregate operator
SELECT TerritoryID, COUNT(*)
FROM Sales.SalesOrderHeader
GROUP BY TerritoryID
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and produces the following output :&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/hash-aggregate.png&#34; alt=&#34;Hash Aggregate&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The algorithm for the Hash Aggregate operator is similar to the Stream Aggregate, with the exceptions that, in this case, the input data does not have to be sorted, a hash table is created in memory, and a hash value is calculated for each row processed.&lt;/p&gt;

&lt;p&gt;When data is sorted the optimizer might choose a stream aggregate instead, try the following :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- run the following statement to create an index
CREATE INDEX IX_TerritoryID ON Sales.SalesOrderHeader(TerritoryID)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and re-run the query. Notice the stream aggregate is now used, as the index is providing sorted input.&lt;/p&gt;

&lt;p&gt;Drop the index when done :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- to clean up, drop the index using the following DROP INDEX statement:
DROP INDEX Sales.SalesOrderHeader.IX_TerritoryID
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If the input is not sorted and order is explicitly requested in a query, the query optimizer may introduce a Sort operator and a Stream Aggregate, as shown previously, or it may decide to use a Hash Aggregate and then sort the results as in the following query :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- the query optimizer will estimate which operation is the least expensive
SELECT TerritoryID, COUNT(*)
FROM Sales.SalesOrderHeader
GROUP BY TerritoryID
ORDER BY TerritoryID
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and the plan produced :&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/sort-with-hash-aggregate.png&#34; alt=&#34;Sort with Hash Aggregate&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;distinct-sort&#34;&gt;Distinct Sort&lt;/h3&gt;

&lt;p&gt;A query using the &lt;strong&gt;DISTINCT&lt;/strong&gt; keyword can be implemented by a :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Stream Aggregate&lt;/li&gt;
&lt;li&gt;Hash Aggregate&lt;/li&gt;
&lt;li&gt;Distinct Sort operator&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The Distinct Sort operator is used to both remove duplicates and sort its input. In fact, a query using DISTINCT can be rewritten as a GROUP BY query, and both can generate the same execution plan.&lt;/p&gt;

&lt;p&gt;The following two queries return the same data and produce the same execution plan :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- the following two queries return the same data and produce the same execution plan
SELECT DISTINCT(JobTitle)
FROM HumanResources.Employee
GO
SELECT JobTitle
FROM HumanResources.Employee
GROUP BY JobTitle
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/distinct-sort-operator.png&#34; alt=&#34;Distinct Sort&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Note that the plan is using a Distinct Sort operator. This operator will sort the rows and eliminate duplicates.&lt;/p&gt;

&lt;p&gt;If we create an index, the query optimizer may instead use a Stream Aggregate operator because the plan can take advantage of the fact that the data is already sorted. To test it, run this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;CREATE INDEX IX_JobTitle ON HumanResources.Employee(JobTitle)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then run the previous two queries again. Both queries will now produce a plan showing a Stream Aggregate operator. Drop the index before continuing by using this statement:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;DROP INDEX HumanResources.Employee.IX_JobTitle
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, for a bigger table without an index to provide order, a Hash Aggregate may be used, as in the two following examples:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- finally, for a bigger table without an index to provide order, a Hash Aggregate may be used, as in the two following examples:
SELECT DISTINCT(TerritoryID)
FROM Sales.SalesOrderHeader
GO
SELECT TerritoryID
FROM Sales.SalesOrderHeader
GROUP BY TerritoryID
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;join-operators&#34;&gt;Join Operators&lt;/h2&gt;

&lt;p&gt;SQL Server uses three physical join operators to implement joins :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Nested Loops Join&lt;/li&gt;
&lt;li&gt;Merge Join&lt;/li&gt;
&lt;li&gt;Hash Join&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;nested-loops-join&#34;&gt;Nested Loops Join&lt;/h3&gt;

&lt;p&gt;The optimizer uses the Nested Join for workloads with a small resultset. Lets us the following query and see the query plan :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT e.BusinessEntityID, TerritoryID
FROM HumanResources.Employee AS e
JOIN Sales.SalesPerson AS s ON e.BusinessEntityID = s.BusinessEntityID;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;notes-of-the-nested-loop&#34;&gt;Notes of the Nested Loop&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Smaller results&lt;/li&gt;
&lt;li&gt;The operator used to access the outer input is executed only once&lt;/li&gt;
&lt;li&gt;The operator used to access the inner input is executed once for every record that qualifies on the outer input&lt;/li&gt;
&lt;li&gt;The outer table is shown on top in the graphical query plan&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/nested-loop-join.png&#34; alt=&#34;Nested Loop&#34; /&gt;_&lt;/p&gt;

&lt;p&gt;This query produces a plan similar to the one shown previously using the SalesPerson as the outer input and a Clustered Index Seek on the Employee table as the inner input. The filter on the SalesPerson table is asking for TerritoryID equal to 1, and only three records qualify this time. As a result, the Clustered Index Seek, which is the operator on the inner input, is executed only three times. You can verify this information by looking at the properties of each operator, as we did for the previous query.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SET STATISTICS PROFILE ON
GO
-- let’s change the query to add a filter by TerritoryID:
SELECT e.BusinessEntityID, HireDate
FROM HumanResources.Employee AS e
JOIN Sales.SalesPerson AS s ON e.BusinessEntityID = s.BusinessEntityID
WHERE TerritoryID = 1
GO
SET STATISTICS PROFILE OFF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/nested-loop-executions.png&#34; alt=&#34;Nested Loop Execution&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The query optimizer is more likely to choose a Nested Loops Join when the outer input is small and the inner input has an index on the join key. This join type can be especially effective when the inner input is potentially large, as only a few rows, indicated by the outer input, will be searched.&lt;/p&gt;

&lt;h3 id=&#34;merge-join&#34;&gt;Merge Join&lt;/h3&gt;

&lt;p&gt;the Mere Join have the following characteristics :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The optimizer chooses for medium workloads&lt;/li&gt;
&lt;li&gt;Requires sorted inputs&lt;/li&gt;
&lt;li&gt;Requires an equality operator&lt;/li&gt;
&lt;li&gt;Can use an index to provide sorted inputs&lt;/li&gt;
&lt;li&gt;Reads the inputs only once&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Lets look at an example :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT h.SalesOrderID, s.SalesOrderDetailID, OrderDate
FROM Sales.SalesOrderHeader h
JOIN Sales.SalesOrderDetail s ON h.SalesOrderID = s.SalesOrderID
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and produces the following plan :&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/merge-join.png&#34; alt=&#34;Merge Join&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Taking benefit from the fact that both of its inputs are sorted on the join predicate, a Merge Join simultaneously reads a row from each input and compares them. If the rows match, they are returned. If the rows do not match, the smaller value can be discarded—because both inputs are sorted, the discarded row will not match any other row on the other input table.&lt;/p&gt;

&lt;p&gt;If the inputs are not already sorted, the query optimizer is not likely to choose a Merge Join. However, it might decide to sort one or even both inputs if it deems the cost is cheaper than the alternatives. Let’s follow an exercise to see what happens if we force a Merge Join on, for example, a Nested Loops join plan.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- if you run the following query, you will notice that it uses a Nested Loops Join
SELECT * FROM Sales.SalesOrderDetail s
JOIN Production.Product p ON s.ProductID = p.ProductID
WHERE SalesOrderID = 43659
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and produces the following plan :&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/nested-loop-merge-join.png&#34; alt=&#34;Nested loop&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In this case, a good plan is created using efficient Clustered Index Seek operators. If we force a Merge Join using a hint, as in the following query, the query optimizer has to introduce sorted sources such as Clustered Index Scan and Sort operators, both of which can be seen on the plan&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- obviously, these additional operations are more expensive than a Clustered Index Seek
SELECT * FROM Sales.SalesOrderdetail s
JOIN Production.Product p ON s.ProductID = p.ProductID
WHERE SalesOrderID = 43659
OPTION (MERGE JOIN)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;produces the following plan :&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/merge-join-forced.png&#34; alt=&#34;Merge Join Forced&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In summary, given the nature of the Merge Join, the query optimizer is more likely to choose this algorithm when faced with medium to large inputs, where there is an equality operator on the join predicate, and the inputs are sorted.&lt;/p&gt;

&lt;h3 id=&#34;hash-join&#34;&gt;Hash Join&lt;/h3&gt;

&lt;p&gt;The query optimizer uses hash joins to process large, unsorted, non-indexed inputs efficiently. Lets run an example :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT h.SalesOrderID, s.SalesOrderDetailID FROM Sales.SalesOrderHeader h
JOIN Sales.SalesOrderDetail s ON h.SalesOrderID = s.SalesOrderID
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;which produces the following plan :&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/images/hash-match.png&#34; alt=&#34;Hash Match&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;characteristics&#34;&gt;Characteristics&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;The Hash Join requires an equality operator on the join predicate,&lt;/li&gt;
&lt;li&gt;Unlike the Merge Join, it does not require its inputs to be sorted&lt;/li&gt;
&lt;li&gt;In addition, the operations in both inputs are executed only once&lt;/li&gt;
&lt;li&gt;Hash Join works by creating a hash table in memory&lt;/li&gt;
&lt;li&gt;The query optimizer will use cardinality estimation to detect the smaller of the two inputs, called the &lt;strong&gt;build input&lt;/strong&gt;, and will use it to build a hash table in memory&lt;/li&gt;
&lt;li&gt;If there is not enough memory to host the hash table, SQL Server can use a &lt;strong&gt;workfile&lt;/strong&gt; in &lt;strong&gt;tempdb&lt;/strong&gt;, which can impact the performance of the query.&lt;/li&gt;
&lt;li&gt;A Hash Join is a blocking operation, but only during the time the build input is hashed&lt;/li&gt;
&lt;li&gt;After the build input is hashed, the second table, called the probe input, will be read and compared to the hash table&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In summary, the query optimizer can choose a Hash Join for large inputs where there is an equality operator on the join predicate. Because both tables are scanned, the cost of a Hash Join is the sum of both inputs.&lt;/p&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://msdn.microsoft.com/en-us/library/ms191158(v=sql.110).aspx&#34;&gt;https://msdn.microsoft.com/en-us/library/ms191158(v=sql.110).aspx&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/en-us/sql/relational-databases/system-dynamic-management-views/sys-dm-exec-query-optimizer-info-transact-sql&#34;&gt;https://docs.microsoft.com/en-us/sql/relational-databases/system-dynamic-management-views/sys-dm-exec-query-optimizer-info-transact-sql&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Query Recompilation</title>
      <link>http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/query-recompilation/</link>
      <pubDate>Sat, 18 Mar 2017 14:54:45 +0200</pubDate>
      
      <guid>http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/query-recompilation/</guid>
      <description>

&lt;h2 id=&#34;benefits-and-drawbacks-of-recompilation&#34;&gt;Benefits and Drawbacks of Recompilation&lt;/h2&gt;

&lt;p&gt;The recompilation of queries can be both beneficial and harmful. Sometimes, it may be beneficial to consider a new processing strategy for a query instead of reusing the existing plan, especially if the data distribution in the table (or the corresponding statistics) has changed or new indexes are added to the table.&lt;/p&gt;

&lt;p&gt;Recompiles in SQL Server 2014 are at the statement level. This increases the overall number of recompiles that can occur within a procedure, but it reduces the effects and overhead of recompiles in general. Statement-level recompiles reduce overhead because they recompile only an individual statement rather than all the statements within a procedure&lt;/p&gt;

&lt;h2 id=&#34;identifying-the-statement-causing-recompilation&#34;&gt;Identifying the Statement Causing Recompilation&lt;/h2&gt;

&lt;p&gt;SQL Server can recompile individual statements within a procedure or the entire procedure. Thus, to find the cause of recompilation, it’s important to identify the SQL statement that can’t reuse the existing plan.&lt;/p&gt;

&lt;h2 id=&#34;analyzing-causes-of-recompilation&#34;&gt;Analyzing Causes of Recompilation&lt;/h2&gt;

&lt;p&gt;Statement recompilation occurs for the following reasons:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The schema of regular tables, temporary tables, or views referred to in the stored procedure statement have changed. Schema changes include changes to the metadata of the table or the indexes on the table.&lt;/li&gt;
&lt;li&gt;Bindings (such as defaults) to the columns of regular or temporary tables have changed.&lt;/li&gt;
&lt;li&gt;Statistics on the table indexes or columns have changed, either automatically or manually.&lt;/li&gt;
&lt;li&gt;An object did not exist when the stored procedure was compiled, but it was created during execution. This is called deferred object resolution, which is the cause of the preceding recompilation.&lt;/li&gt;
&lt;li&gt;SET options have changed.&lt;/li&gt;
&lt;li&gt;The execution plan was aged and deallocated.&lt;/li&gt;
&lt;li&gt;An explicit call was made to the sp_recompile system stored procedure.&lt;/li&gt;
&lt;li&gt;There was an explicit use of the RECOMPILE hint.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Recompiling explicitly :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;EXEC dbo.CustomerList
    @CustomerId = 1
    WITH RECOMPILE;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;avoiding-recompilations&#34;&gt;Avoiding Recompilations&lt;/h2&gt;

&lt;p&gt;You can avoid it by following these implementation practices:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Don’t interleave DDL and DML statements.&lt;/li&gt;
&lt;li&gt;Avoid recompilation caused by statistics changes.&lt;/li&gt;
&lt;li&gt;Use the KEEPFIXED PLAN option.&lt;/li&gt;
&lt;li&gt;Disable the auto update statistics feature on the table.&lt;/li&gt;
&lt;li&gt;Use table variables.&lt;/li&gt;
&lt;li&gt;Avoid changing SET options within the stored procedure.&lt;/li&gt;
&lt;li&gt;Use the OPTIMIZE FOR query hint.&lt;/li&gt;
&lt;li&gt;Use plan guides.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Query Design Analysis</title>
      <link>http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/query-design-analysis/</link>
      <pubDate>Sat, 18 Mar 2017 14:54:56 +0200</pubDate>
      
      <guid>http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/query-design-analysis/</guid>
      <description>

&lt;p&gt;A database schema may include a number of performance-enhancement features such as indexes, statistics, and stored procedures. But none of these features guarantees good performance if your queries are written badly in the first place.&lt;/p&gt;

&lt;h2 id=&#34;query-design-recommendations&#34;&gt;Query Design Recommendations&lt;/h2&gt;

&lt;p&gt;When you need to run a query, you can often use many different approaches to get the same data. In many cases, the optimizer generates the same plan, irrespective of the structure of the query. However, in some situations the query structure won’t allow the optimizer to select the best possible processing strategy. It is important that you are aware that this can happen and, should it occur, what you can do to avoid it.&lt;/p&gt;

&lt;p&gt;In general, keep the following recommendations in mind to ensure the best performance:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Operate on small result sets.&lt;/li&gt;
&lt;li&gt;Use indexes effectively.&lt;/li&gt;
&lt;li&gt;Avoid optimizer hints.&lt;/li&gt;
&lt;li&gt;Use domain and referential integrity.&lt;/li&gt;
&lt;li&gt;Avoid resource-intensive queries.&lt;/li&gt;
&lt;li&gt;Reduce the number of network round-trips.&lt;/li&gt;
&lt;li&gt;Reduce the transaction cost.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;operating-on-small-result-sets&#34;&gt;Operating on Small Result Sets&lt;/h2&gt;

&lt;p&gt;To improve the performance of a query, limit the amount of data it operates on, including both columns and rows. Operating on a small result set reduces the amount of resources consumed by a query and increases the effectiveness of indexes. Two of the rules you should follow to limit the data set’s size are as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Limit the number of columns in the select list.&lt;/li&gt;
&lt;li&gt;Use highly selective WHERE clauses to limit the rows returned.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;using-indexes-effectively&#34;&gt;Using Indexes Effectively&lt;/h2&gt;

&lt;p&gt;It is extremely important to have effective indexes on database tables to improve performance. However, it is equally important to ensure that the queries are designed properly to use these indexes effectively. These are some of the query design rules you should follow to improve the use of indexes:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Avoid nonsargable search conditions.&lt;/li&gt;
&lt;li&gt;Avoid arithmetic operators on the WHERE clause column.&lt;/li&gt;
&lt;li&gt;Avoid functions on the WHERE clause column.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;avoiding-optimizer-hints&#34;&gt;Avoiding Optimizer Hints&lt;/h2&gt;

&lt;p&gt;Since it is usually difficult to outsmart the optimizer, the usual recommendation is to avoid optimizer hints. Generally, it is beneficial to let the optimizer determine a cost-effective processing strategy based on the data distribution statistics, indexes, and other factors.&lt;/p&gt;

&lt;h2 id=&#34;using-domain-and-referential-integrity&#34;&gt;Using Domain and Referential Integrity&lt;/h2&gt;

&lt;p&gt;Domain and referential integrity help define and enforce valid values for a column, maintaining the integrity of the database. This is done through column/table constraints.&lt;/p&gt;

&lt;p&gt;Domain and referential integrity help the SQL Server 2014 optimizer analyze valid data values without physically accessing the data, which reduces query time.&lt;/p&gt;

&lt;h2 id=&#34;avoiding-resource-intensive-queries&#34;&gt;Avoiding Resource-Intensive Queries&lt;/h2&gt;

&lt;p&gt;These are a few techniques you can use to reduce the footprint of a query:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Avoid data type conversion.&lt;/li&gt;
&lt;li&gt;Use EXISTS over COUNT(*) to verify data existence.&lt;/li&gt;
&lt;li&gt;Use UNION ALL over UNION.&lt;/li&gt;
&lt;li&gt;Use indexes for aggregate and sort operations.&lt;/li&gt;
&lt;li&gt;Avoid local variables in a batch query.&lt;/li&gt;
&lt;li&gt;Be careful when naming stored procedures.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;reducing-the-number-of-network-round-trips&#34;&gt;Reducing the Number of Network Round-Trips&lt;/h2&gt;

&lt;p&gt;To reduce the overhead of multiple network round-trips, consider the following techniques:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Execute multiple queries together.&lt;/li&gt;
&lt;li&gt;Use SET NOCOUNT.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;reducing-the-transaction-cost&#34;&gt;Reducing the Transaction Cost&lt;/h2&gt;

&lt;p&gt;Based on the characteristics of a transaction, here are two broad recommendations to reduce the cost of the transaction:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Reduce logging overhead.&lt;/li&gt;
&lt;li&gt;Reduce lock overhead.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Blocking Analysis</title>
      <link>http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/blocking-analysis/</link>
      <pubDate>Sat, 18 Mar 2017 14:55:06 +0200</pubDate>
      
      <guid>http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/blocking-analysis/</guid>
      <description>

&lt;h2 id=&#34;blocking-fundamentals&#34;&gt;Blocking Fundamentals&lt;/h2&gt;

&lt;p&gt;In an ideal world, every SQL query would be able to execute concurrently, without any blocking by other queries. However, in the real world, queries do block each other. In SQL Server, this traffic management takes the form of the lock manager, which controls concurrent access to a database resource to maintain data consistency.&lt;/p&gt;

&lt;p&gt;Terms :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;locking&lt;/li&gt;
&lt;li&gt;blocking&lt;/li&gt;
&lt;li&gt;deadlocking&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Locking&lt;/strong&gt;  - is an integral part of the process of SQL Server managing multiple sessions. When a session needs access to a piece of data, a lock of some type is placed on it.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Blocking&lt;/strong&gt; - is when one session, or thread, needs access to a piece of data and has to wait for another session’s lock to clear&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Deadlocking&lt;/strong&gt; is when two sessions, or threads, form what is sometimes referred to as a deadly embrace. They are each waiting on the other for a lock to clear. Deadlocking could also be referred to as a permanent blocking situation&lt;/p&gt;

&lt;p&gt;So, locks can lead to blocks, and both locks and blocks play a part in deadlocks, but these are three very distinct concepts.&lt;/p&gt;

&lt;h2 id=&#34;understanding-blocking&#34;&gt;Understanding Blocking&lt;/h2&gt;

&lt;p&gt;In SQL Server, a database query can execute as a logical unit of work in itself, or it can participate in a bigger logical unit of work. A bigger logical unit of work can be defined using the BEGIN TRANSACTION statement along with COMMIT and/or ROLLBACK statements. Every logical unit of work must conform to a set of four properties called ACID properties:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Atomicity&lt;/li&gt;
&lt;li&gt;Consistency&lt;/li&gt;
&lt;li&gt;Isolation&lt;/li&gt;
&lt;li&gt;Durability&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;atomicity&#34;&gt;Atomicity&lt;/h3&gt;

&lt;p&gt;The atomicity of a user-defined transaction can be ensured in the following two ways:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;SET XACT_ABORT ON&lt;/li&gt;
&lt;li&gt;Explicit rollback&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The SET XACT_ABORT statement specifies whether SQL Server should automatically roll back and abort an entire transaction when a statement within the transaction fails.&lt;/p&gt;

&lt;p&gt;You can also manage the atomicity of a user-defined transaction by using the TRY/CATCH error-trapping mechanism within SQL Server. If a statement within the TRY block of code generates an error, then the CATCH block of code will handle the error. The ROLLBACK statement rolls back all the actions performed in the transaction until that point.&lt;/p&gt;

&lt;h2 id=&#34;locks&#34;&gt;Locks&lt;/h2&gt;

&lt;p&gt;When a session executes a query, SQL Server determines the database resources that need to be accessed; and, if required, the lock manager grants different types of locks to the session. The query is blocked if another session has already been granted the locks; however, to provide both transaction isolation and concurrency, SQL Server uses different lock granularities.&lt;/p&gt;

&lt;h3 id=&#34;lock-granularity&#34;&gt;Lock Granularity&lt;/h3&gt;

&lt;p&gt;To improve concurrency, SQL Server implements lock granularities at the following resource levels and in this order:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Row (RID)&lt;/li&gt;
&lt;li&gt;Key (KEY)&lt;/li&gt;
&lt;li&gt;Page (PAG)&lt;/li&gt;
&lt;li&gt;Extent (EXT)&lt;/li&gt;
&lt;li&gt;Heap or B-tree (HoBT)&lt;/li&gt;
&lt;li&gt;Table (TAB)&lt;/li&gt;
&lt;li&gt;File (FIL)&lt;/li&gt;
&lt;li&gt;Application (APP)&lt;/li&gt;
&lt;li&gt;MetaData (MDT)&lt;/li&gt;
&lt;li&gt;Allocation Unit (AU)&lt;/li&gt;
&lt;li&gt;Database (DB)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;row-level-lock&#34;&gt;Row-Level Lock&lt;/h3&gt;

&lt;p&gt;This lock is maintained on a single row within a table and is the lowest level of lock on a database table. When a query modifies a row in a table, an RID lock is granted to the query on the row. For example, consider the transaction on the following test table:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;--Create a test table
IF (SELECT  OBJECT_ID(&#39;dbo.Test1&#39;)
   ) IS NOT NULL
    DROP TABLE dbo.Test1;
GO
CREATE TABLE dbo.Test1 (C1 INT);
INSERT  INTO dbo.Test1
VALUES  (1);
GO

BEGIN TRAN
DELETE  dbo.Test1
WHERE   C1 = 1;

SELECT  dtl.request_session_id,
        dtl.resource_database_id,
        dtl.resource_associated_entity_id,
        dtl.resource_type,
        dtl.resource_description,
        dtl.request_mode,
        dtl.request_status
FROM    sys.dm_tran_locks AS dtl
WHERE   dtl.request_session_id = @@SPID;
ROLLBACK
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The dynamic management view, sys.dm_tran_locks, can be used to display the lock status. Granting an RID lock to the DELETE statement prevents other transactions from accessing the row.&lt;/p&gt;

&lt;p&gt;The resource locked by the RID lock can be represented in the following format from the resource_description column:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;DatabaseID:FileID:PageID:Slot(row)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;he resource_description column value for the RID type represents the remaining part of the RID resource as 1:23819:0. In this case, a FileID of 1 is the primary data file, a PageID of 23819 is a page belonging to the dbo.Test1 table identified by the C1 column, and a Slot (row) of 0 represents the row position within the page. You can obtain the table name and the database name by executing the following SQL statements:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT  OBJECT_NAME(1668200993), DB_NAME(5);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The row-level lock provides very high concurrency since blocking is restricted to the row under effect&lt;/p&gt;

&lt;h3 id=&#34;key-level-lock&#34;&gt;Key-Level Lock&lt;/h3&gt;

&lt;p&gt;This is a row lock within an index, and it is identified as a KEY lock. As you know, for a table with a clustered index, the data pages of the table and the leaf pages of the clustered index are the same. Since both of the rows are the same for a table with a clustered index, only a KEY lock is acquired on the clustered index row, or limited range of rows, while accessing the rows from the table (or the clustered index). For example, consider having a clustered index on the Test1 table.&lt;/p&gt;

&lt;p&gt;CREATE CLUSTERED INDEX TestIndex ON dbo.Test1(C1);
Next, rerun the following code:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;BEGIN TRAN
DELETE  dbo.Test1
WHERE   C1 = 1 ;

SELECT  dtl.request_session_id,
        dtl.resource_database_id,
        dtl.resource_associated_entity_id,
        dtl.resource_type,
        dtl.resource_description,
        dtl.request_mode,
        dtl.request_status
FROM    sys.dm_tran_locks AS dtl
WHERE   dtl.request_session_id = @@SPID ;
ROLLBACK
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The corresponding output from sys.dm_tran_locks shows a KEY lock instead of the RID lock&lt;/p&gt;

&lt;p&gt;When you are querying sys.dm_tran_locks, you will be able to retrieve the database identifier, resource_database_id. You can also get information about what is being locked from resource_associated_entity&lt;em&gt;id; however, to get to the particular resource (in this case, the page on the key), you have to go to the resource&lt;/em&gt; description column for the value, which is 1:24117. In this case, the Index ID of 1 is the clustered index on the dbo.Test1 table. You also see the types of requests that are made: S, Sch-S, X, and so on&lt;/p&gt;

&lt;p&gt;Like the row-level lock, the key-level lock provides very high concurrency.&lt;/p&gt;

&lt;h3 id=&#34;page-level-lock&#34;&gt;Page-Level Lock&lt;/h3&gt;

&lt;p&gt;A page-level lock is maintained on a single page within a table or an index, and it is identified as a PAG lock. When a query requests multiple rows within a page, the consistency of all the requested rows can be maintained by acquiring either RID/KEY locks on the individual rows or a PAG lock on the entire page.&lt;/p&gt;

&lt;p&gt;The resource locked by the PAG lock may be represented in the following format in the resource_description column of sys.dm_tran_locks:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;DatabaseID:FileID:PageID&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The page-level lock can increase the performance of an individual query by reducing its locking overhead, but it hurts the concurrency of the database by blocking access to all the rows in the page.&lt;/p&gt;

&lt;p&gt;The resource locked by the TAB lock will be represented in resource_description in the following format:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;DatabaseID:ObjectID&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;A table-level lock requires the least overhead compared to the other locks and thus improves the performance of the individual query. On the other hand, since the table-level lock blocks all write requests on the entire table (including indexes), it can significantly hurt database concurrency.&lt;/p&gt;

&lt;p&gt;In such cases, a query developer may override the lock manager’s lock level selection for a table referred to in the query by using locking hints.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT * FROM &amp;lt;TableName&amp;gt; WITH(TABLOCK)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;database-level-lock&#34;&gt;Database-Level Lock&lt;/h3&gt;

&lt;p&gt;A database-level lock is maintained on a database and is identified as a DB lock. When an application makes a database connection, the lock manager assigns a database-level shared lock to the corresponding session_id. This prevents a user from accidentally dropping or restoring the database while other users are connected to it.&lt;/p&gt;

&lt;h3 id=&#34;lock-modes&#34;&gt;Lock Modes&lt;/h3&gt;

&lt;p&gt;Depending on the type of access requested, SQL Server uses different lock modes while locking resources:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Shared (S)&lt;/li&gt;
&lt;li&gt;Update (U)&lt;/li&gt;
&lt;li&gt;Exclusive (X)&lt;/li&gt;
&lt;li&gt;Intent:

&lt;ul&gt;
&lt;li&gt;Intent Shared (IS)&lt;/li&gt;
&lt;li&gt;Intent Exclusive (IX)&lt;/li&gt;
&lt;li&gt;Shared with Intent Exclusive (SIX)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Schema:

&lt;ul&gt;
&lt;li&gt;Schema Modification (Sch-M)&lt;/li&gt;
&lt;li&gt;Schema Stability (Sch-S)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Bulk Update (BU)&lt;/li&gt;
&lt;li&gt;Key-Range&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;shared-s-mode&#34;&gt;Shared (S) Mode&lt;/h3&gt;

&lt;p&gt;Shared mode is used for read-only queries, such as a SELECT statement. It doesn’t prevent other read-only queries from accessing the data simultaneously because the integrity of the data isn’t compromised by the concurrent reads. By default, the (S) lock acquired by a SELECT statement is released immediately after the data is read. For example, consider the following transaction:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;BEGIN TRAN
SELECT  *
FROM    Production.Product AS p
WHERE   p.ProductID = 1;
--Other queries
COMMIT
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The (S) lock acquired by the SELECT statement is not held until the end of the transaction; instead, it is released immediately after the data is read by the SELECT statement under read_ committed, the default isolation level. This behavior of the (S) lock can be altered by using a higher isolation level or a lock hint.&lt;/p&gt;

&lt;h3 id=&#34;update-u-mode&#34;&gt;Update (U) Mode&lt;/h3&gt;

&lt;p&gt;Update mode may be considered similar to the (S) lock, but it also includes an objective to modify the data as part of the same query. Unlike the (S) lock, the (U) lock indicates that the data is read for modification. Since the data is read with an objective to modify it, SQL Server does not allow more than one (U) lock on the data simultaneously. This rule helps maintain data integrity. Note that concurrent (S) locks on the data are allowed.&lt;/p&gt;

&lt;p&gt;Different lock modes are used in the two intermediate steps to maximize concurrency. Instead of acquiring an exclusive right while reading the data, the first step acquires a (U) lock on the data. In the second step, the (U) lock is converted to an exclusive lock for modification. If no modification is required, then the (U) lock is released; in other words, it’s not held until the end of the transaction. Consider the following example, which demonstrates the locking behavior of the UPDATE statement:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;BEGIN TRANSACTION LockTran1
UPDATE  Sales.Currency
SET     Name = &#39;Euro&#39;
WHERE   CurrencyCode = &#39;EUR&#39;;
COMMIT
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Block the second step of the UPDATE statement by first executing a transaction from a second connection.
&amp;ndash;Execute from a second connection&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;BEGIN TRANSACTION LockTran2
--Retain an  (S) lock on the resource
SELECT  *
FROM    Sales.Currency AS c WITH (REPEATABLEREAD)
WHERE   c.CurrencyCode = &#39;EUR&#39; ;
--Allow DMVs to be executed before second step of
-- UPDATE statement is executed by tran
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;exclusive-x-mode&#34;&gt;Exclusive (X) Mode&lt;/h3&gt;

&lt;p&gt;Exclusive mode provides an exclusive right on a database resource for modification by data manipulation queries such as INSERT, UPDATE, and DELETE. It prevents other concurrent transactions from accessing the resource under modification.&lt;/p&gt;

&lt;p&gt;The (X) lock serves two purposes.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;It prevents other transactions from accessing the resource under modification so that they see a value either before or after the modification, not a value undergoing modification.&lt;/li&gt;
&lt;li&gt;It allows the transaction modifying the resource to safely roll back to the original value before modification, if needed, since no other transaction is allowed to modify the resource simultaneously.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;BEGIN TRAN
DELETE  Sales.Currency
WHERE   CurrencyCode = &#39;ALL&#39;;

SELECT  tl.request_session_id,
        tl.resource_database_id,
        tl.resource_associated_entity_id,
        tl.resource_type,
        tl.resource_description,
        tl.request_mode,
        tl.request_status
FROM    sys.dm_tran_locks tl;

ROLLBACK TRAN
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;isolation-levels-effect-of-indexes-on-locking&#34;&gt;Isolation Levels Effect of Indexes on Locking&lt;/h2&gt;

&lt;p&gt;SQL Server provides isolation to a transaction by preventing other transactions from accessing the same resource in an incompatible way.&lt;/p&gt;

&lt;p&gt;SQL Server implements six isolation levels, four of them as defined by ISO:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Read Uncommitted&lt;/li&gt;
&lt;li&gt;Read Committed&lt;/li&gt;
&lt;li&gt;Repeatable Read&lt;/li&gt;
&lt;li&gt;Serializable&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can use the following SET statement to configure the isolation level of a database connection to the Read Uncommitted isolation level:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can also achieve this degree of isolation on a query basis using the NOLOCK locking hint.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT  *
FROM    Production.Product AS p WITH (NOLOCK);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The effect of the locking hint remains applicable for the query and doesn’t change the isolation level of the connection.&lt;/p&gt;

&lt;p&gt;The Read Uncommitted isolation level avoids the blocking caused by a SELECT statement, but you should not use it if the transaction depends on the accuracy of the data read by the SELECT statement or if the transaction cannot withstand a concurrent change of data by another transaction.&lt;/p&gt;

&lt;h2 id=&#34;capturing-blocking-information&#34;&gt;Capturing Blocking Information&lt;/h2&gt;

&lt;p&gt;In a blocking scenario, you need the following information to have a clear understanding of the cause of the blocking:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The connection information of the blocking and blocked sessions: You can obtain this information from the sys.dm_os_waiting_tasks dynamic management view or the sp_who2 system stored procedure.&lt;/li&gt;
&lt;li&gt;The lock information of the blocking and blocked sessions: You can obtain this information from the sys.dm_tran_locks DMO.&lt;/li&gt;
&lt;li&gt;The SQL statements last executed by the blocking and blocked sessions: You can use the sys.dm_exec_requests DMV combined with sys.dm_exec_sql_text and sys.dm_exec_queryplan or Extended Events to obtain this information.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT  dtl.request_session_id AS WaitingSessionID,
        der.blocking_session_id AS BlockingSessionID,
        dowt.resource_description,
        der.wait_type,
        dowt.wait_duration_ms,
        DB_NAME(dtl.resource_database_id) AS DatabaseName,
        dtl.resource_associated_entity_id AS WaitingAssociatedEntity,
        dtl.resource_type AS WaitingResourceType,
        dtl.request_type AS WaitingRequestType,
        dest.[text] AS WaitingTSql,
        dtlbl.request_type BlockingRequestType,
        destbl.[text] AS BlockingTsql
FROM    sys.dm_tran_locks AS dtl
JOIN    sys.dm_os_waiting_tasks AS dowt
        ON dtl.lock_owner_address = dowt.resource_address
JOIN    sys.dm_exec_requests AS der
        ON der.session_id = dtl.request_session_id
CROSS APPLY sys.dm_exec_sql_text(der.sql_handle) AS dest
LEFT JOIN sys.dm_exec_requests derbl
        ON derbl.session_id = dowt.blocking_session_id
OUTER APPLY sys.dm_exec_sql_text(derbl.sql_handle) AS destbl
LEFT JOIN sys.dm_tran_locks AS dtlbl
        ON derbl.session_id = dtlbl.request_session_id;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Extended Events provide an event called blocked_process_report. This event works off the blocked process threshold that you need to provide to the system configuration. This script sets the threshold to five seconds:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;EXEC sp_configure &#39;show advanced option&#39;, &#39;1&#39;;
RECONFIGURE;
EXEC sp_configure
    &#39;blocked process threshold&#39;,
    5;
RECONFIGURE;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This would normally be a very low value in most systems. If you have an established performance service level agreement (SLA), you could use that as the threshold.&lt;/p&gt;

&lt;h2 id=&#34;blocking-resolutions&#34;&gt;Blocking Resolutions&lt;/h2&gt;

&lt;p&gt;Once you’ve analyzed the cause of a block, the next step is to determine any possible resolutions. Here are a few techniques you can use to do this:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Optimize the queries executed by blocking and blocked SPIDs.&lt;/li&gt;
&lt;li&gt;Decrease the isolation level.&lt;/li&gt;
&lt;li&gt;Partition the contended data.&lt;/li&gt;
&lt;li&gt;Use a covering index on the contended data.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;recommendations-to-reduce-blocking&#34;&gt;Recommendations to Reduce Blocking&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Keep transactions short.&lt;/li&gt;
&lt;li&gt;Optimize queries.&lt;/li&gt;
&lt;li&gt;Use query timeouts or a resource governor to control runaway queries. For more on the resource governor, consult Books Online: &lt;a href=&#34;http://bit.ly/1jiPhfS&#34;&gt;http://bit.ly/1jiPhfS&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Use the lowest isolation level required.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;automation-to-detect-and-collect-blocking-information&#34;&gt;Automation to Detect and Collect Blocking Information&lt;/h2&gt;

&lt;p&gt;In addition to capturing information using extended events, you can automate the process of detecting a blocking condition and collecting the relevant information using SQL Server Agent.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deadlock Analysis</title>
      <link>http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/deadlock-analysis/</link>
      <pubDate>Sat, 18 Mar 2017 14:55:24 +0200</pubDate>
      
      <guid>http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/deadlock-analysis/</guid>
      <description>

&lt;h2 id=&#34;deadlock-fundamentals&#34;&gt;Deadlock Fundamentals&lt;/h2&gt;

&lt;p&gt;A deadlock is a special blocking scenario in which two processes get blocked by each other. Each process, while holding its own resources, attempts to access a resource that is locked by the other process. This will lead to a blocking scenario known as a deadly embrace&lt;/p&gt;

&lt;p&gt;Choosing the Deadlock Victim&lt;/p&gt;

&lt;p&gt;SQL Server determines the session to be a deadlock victim by evaluating the cost of undoing the transaction of the participating sessions, and it selects the one with the least estimated cost. You can exercise some control over the session to be chosen as a victim by setting the deadlock priority of its connection to LOW.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SET DEADLOCK_PRIORITY LOW;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This steers SQL Server toward choosing this particular session as a victim in the event of a deadlock. You can reset the deadlock priority of the connection to its normal value by executing the following SET statement:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SET DEADLOCK_PRIORITY NORMAL;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The SET statement allows you to mark a session as a HIGH deadlock priority, too. This won’t prevent deadlocks on a given session, but it will reduce the likelihood of a given session being picked as the victim. You can even set the priority level to a number value from –10 for the lowest priority or to 10 for the highest.&lt;/p&gt;

&lt;h2 id=&#34;using-error-handling-to-catch-a-deadlock&#34;&gt;Using Error Handling to Catch a Deadlock&lt;/h2&gt;

&lt;p&gt;When SQL Server chooses a session as a victim, it raises an error with the error number. You can use the TRY/CATCH construct within T-SQL to handle the error. SQL Server ensures the consistency of the database by automatically rolling back the transaction of the victim session. The rollback ensures that the session is returned to the same state it was in before the start of its transaction.&lt;/p&gt;

&lt;p&gt;Take the following T-SQL statement as an example of one method for handling a deadlock error:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;DECLARE @retry AS TINYINT = 1,
    @retrymax AS TINYINT = 2,
    @retrycount AS TINYINT = 0;
WHILE @retry = 1
    AND @retrycount &amp;lt;= @retrymax
    BEGIN
        SET @retry = 0;

        BEGIN TRY
            UPDATE  HumanResources.Employee
            SET     LoginID = &#39;54321&#39;
            WHERE   BusinessEntityID = 100;
        END TRY
        BEGIN CATCH
            IF (ERROR_NUMBER() = 1205)
                BEGIN
                    SET @retrycount = @retrycount + 1;
                    SET @retry = 1;
                END
        END CATCH
    END
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;deadlock-analysis&#34;&gt;Deadlock Analysis&lt;/h2&gt;

&lt;p&gt;You can sometimes prevent a deadlock from happening by analyzing the causes. You need the following information to do this:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The sessions participating in the deadlock&lt;/li&gt;
&lt;li&gt;The resources involved in the deadlock&lt;/li&gt;
&lt;li&gt;The queries executed by the sessions&lt;/li&gt;
&lt;li&gt;Collecting Deadlock Information&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You have four ways to collect the deadlock information.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Use Extended Events&lt;/li&gt;
&lt;li&gt;Set trace flag 1222&lt;/li&gt;
&lt;li&gt;Set trace flag 1204&lt;/li&gt;
&lt;li&gt;Use trace events&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Extended Events provides several ways to gather the deadlock information. This is probably the best method you can apply to your server for collecting deadlock information. You can use these options:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Lock_deadlock:&lt;/strong&gt; Displays basic information about a deadlock occurrence&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lock_deadlock_chain:&lt;/strong&gt; Captures information from each participant in a deadlock&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Xml_deadlock_report:&lt;/strong&gt; Displays an XML deadlock graph with the cause of the deadlock&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;avoiding-deadlocks&#34;&gt;Avoiding Deadlocks&lt;/h2&gt;

&lt;p&gt;The methods for avoiding a deadlock scenario depend upon the nature of the deadlock. The following are some of the techniques you can use to avoid a deadlock:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Access resources in the same physical order.&lt;/li&gt;
&lt;li&gt;Decrease the number of resources accessed.&lt;/li&gt;
&lt;li&gt;Minimize lock contention.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Cursor Cost Analysis</title>
      <link>http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/cursor-cost-analysis/</link>
      <pubDate>Sat, 18 Mar 2017 14:55:45 +0200</pubDate>
      
      <guid>http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/cursor-cost-analysis/</guid>
      <description>

&lt;h2 id=&#34;cursor-fundamentals&#34;&gt;Cursor Fundamentals&lt;/h2&gt;

&lt;p&gt;When a query is executed by an application, SQL Server returns a set of data consisting of rows. Generally, applications can’t process multiple rows together; instead, they process one row at a time by walking through the result set returned by SQL Server. This functionality is provided by a cursor, which is a mechanism to work with one row at a time out of a multirow result set.&lt;/p&gt;

&lt;p&gt;T-SQL cursor processing usually involves the following steps:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Declare the cursor to associate it with a SELECT statement and define the characteristics of the cursor.&lt;/li&gt;
&lt;li&gt;Open the cursor to access the result set returned by the SELECT statement.&lt;/li&gt;
&lt;li&gt;Retrieve a row from the cursor. Optionally, modify the row through the cursor.&lt;/li&gt;
&lt;li&gt;Once all the rows in the result set are processed, close the cursor and release the resources assigned to the cursor.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The following is an example of a server cursor processing of query results from a table:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;--Associate a SELECT statement to a cursor and define the
--cursor&#39;s characteristics
USE AdventureWorks2012;
GO
SET NOCOUNT ON
DECLARE MyCursor CURSOR /*&amp;lt;cursor characteristics&amp;gt;*/
FOR
SELECT  adt.AddressTypeID,
        adt.Name,
        adt.ModifiedDate
FROM    Person.AddressType adt;

--Open the cursor to access the result set returned by the
--SELECT statement
OPEN MyCursor;

--Retrieve one row at a time from the result set returned by
--the SELECT statement
DECLARE @AddressTypeId INT,
    @Name VARCHAR(50),
    @ModifiedDate DATETIME;

FETCH NEXT FROM MyCursor INTO @AddressTypeId,@Name,@ModifiedDate;

WHILE @@FETCH_STATUS = 0
    BEGIN
        PRINT &#39;NAME =   &#39; + @Name;

--Optionally, modify the row through the cursor
        UPDATE  Person.AddressType
        SET     Name = Name + &#39;z&#39;
        WHERE CURRENT OF MyCursor;

        FETCH NEXT FROM MyCursor
            INTO @AddressTypeId,@Name,@ModifiedDate;
    END

--Close the cursor and release all resources assigned to the
--cursor
CLOSE MyCursor;
DEALLOCATE MyCursor;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;cursor-cost-comparison&#34;&gt;Cursor Cost Comparison&lt;/h2&gt;

&lt;h2 id=&#34;default-result-set&#34;&gt;Default Result Set&lt;/h2&gt;

&lt;h2 id=&#34;analyzing-sql-server-overhead-with-cursors&#34;&gt;Analyzing SQL Server Overhead with Cursors&lt;/h2&gt;

&lt;p&gt;he T-SQL cursors implemented using T-SQL statements are always executed on SQL Server because they need the SQL Server engine to process their T-SQL statements.&lt;/p&gt;

&lt;p&gt;In most cases, you can avoid cursor operations by rewriting the functionality using SQL queries, concentrating on set-based methods of accessing the data. For example, you can rewrite the preceding stored procedure using SQL queries (instead of the cursor operations) as follows (nocursor.sql in the download):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;IF (SELECT  OBJECT_ID(&#39;dbo.TotalLoss&#39;)
   ) IS NOT NULL
    DROP PROC dbo.TotalLoss;
GO
CREATE PROC dbo.TotalLoss
AS
SELECT  CASE  --Determine status based on following computation
             WHEN SUM(MoneyLostPerProduct) &amp;gt; 5000 THEN &#39;We are bankrupt!&#39;
             ELSE &#39;We are safe!&#39;
        END AS Status
FROM    (--Calculate total money lost for all discarded products
         SELECT SUM(wo.ScrappedQty * p.ListPrice) AS MoneyLostPerProduct
         FROM   Production.WorkOrder AS wo
         JOIN   Production.ScrapReason AS sr
                ON wo.ScrapReasonID = sr.ScrapReasonID
         JOIN   Production.Product AS p
                ON wo.ProductID = p.ProductID
         GROUP BY p.ProductID
        ) DiscardedProducts;
GO
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;cursor-recommendations&#34;&gt;Cursor Recommendations&lt;/h2&gt;

&lt;p&gt;An ineffective use of cursors can degrade the application performance by introducing extra network round-trips and load on server resources. To keep the cursor cost low, try to follow these recommendations:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Use set-based SQL statements over T-SQL cursors, since SQL Server is designed to work with sets of data.&lt;/li&gt;
&lt;li&gt;Use the least expensive cursor.&lt;/li&gt;
&lt;li&gt;When using SQL Server cursors, use the FAST FORWARD cursor type.&lt;/li&gt;
&lt;li&gt;When using the API cursors implemented by ADO, OLEDB, or ODBC, use the default cursor type, which is generally referred to as the default result set.&lt;/li&gt;
&lt;li&gt;When using ADO.NET, use the DataReader object.&lt;/li&gt;
&lt;li&gt;Minimize impact on server resources.&lt;/li&gt;
&lt;li&gt;Use a client-side cursor for API cursors.&lt;/li&gt;
&lt;li&gt;Do not perform actions on the underlying tables through the cursor.&lt;/li&gt;
&lt;li&gt;Always deallocate the cursor as soon as possible. This helps free resources, especially in tempdb.&lt;/li&gt;
&lt;li&gt;Redesign the cursor’s SELECT statement (or the application) to return the minimum set of rows and columns.&lt;/li&gt;
&lt;li&gt;Avoid T-SQL cursors entirely by rewriting the logic of the cursor as set-based statements, which are generally more efficient than cursors.&lt;/li&gt;
&lt;li&gt;Use a ROWVERSION column for dynamic cursors to benefit from the efficient, version-based concurrency control instead of relying upon the value-based technique.&lt;/li&gt;
&lt;li&gt;Minimize impact on tempdb.&lt;/li&gt;
&lt;li&gt;Minimize resource contention in tempdb by avoiding the static and keyset-driven cursor types.&lt;/li&gt;
&lt;li&gt;Static and key-set cursors put additional load on tempdb, so take that into account if you must use them, or avoid them if your tempdb is under stress.&lt;/li&gt;
&lt;li&gt;Minimize blocking.&lt;/li&gt;
&lt;li&gt;Use the default result set, fast-forward-only cursor, or static cursor.&lt;/li&gt;
&lt;li&gt;Process all cursor rows as quickly as possible.&lt;/li&gt;
&lt;li&gt;Avoid scroll locks or pessimistic locking.&lt;/li&gt;
&lt;li&gt;Minimize network round-trips while using API cursors.&lt;/li&gt;
&lt;li&gt;Use the CacheSize property of ADO to fetch multiple rows in one round-trip.&lt;/li&gt;
&lt;li&gt;Use client-side cursors.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Database Performance Stress Testing</title>
      <link>http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/database-performance-stress-testing/</link>
      <pubDate>Sat, 18 Mar 2017 14:56:03 +0200</pubDate>
      
      <guid>http://guides.peruzal.com/sql-server-performance-tuning-and-optimization/database-performance-stress-testing/</guid>
      <description>

&lt;h2 id=&#34;database-stressing-with-jmeter&#34;&gt;Database Stressing with JMeter&lt;/h2&gt;

&lt;h2 id=&#34;replaying-sql-scripts-with-jmeter&#34;&gt;Replaying SQL Scripts with JMeter&lt;/h2&gt;

&lt;h2 id=&#34;performance-testing-overview&#34;&gt;Performance Testing Overview&lt;/h2&gt;

&lt;h2 id=&#34;capturing-data-with-the-server-side-trace-distributed-replay-for-database-testing&#34;&gt;Capturing Data with the Server Side Trace Distributed Replay for Database Testing&lt;/h2&gt;

&lt;h2 id=&#34;summary-and-sql-server-optimization-checklist&#34;&gt;Summary and SQL Server Optimization Checklist&lt;/h2&gt;
</description>
    </item>
    
  </channel>
</rss>